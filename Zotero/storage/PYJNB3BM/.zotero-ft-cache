Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space

Anh Nguyen
University of Wyoming‚Ä†
anh.ng8@gmail.com

Jeff Clune
Uber AI Labs‚Ä†, University of Wyoming
jeffclune@uwyo.edu

Yoshua Bengio
Montreal Institute for Learning Algorithms yoshua.umontreal@gmail.com

Alexey Dosovitskiy
University of Freiburg dosovits@cs.uni-freiburg.de

Jason Yosinski
Uber AI Labs‚Ä†
yosinski@uber.com

Abstract

arXiv:1612.00005v2 [cs.CV] 12 Apr 2017

Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [37] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classiÔ¨Åer network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 √ó 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a uniÔ¨Åed probabilistic interpretation of related activation maximization methods and call the general class of models ‚ÄúPlug and Play Generative Networks.‚Äù PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable ‚Äúcondition‚Äù network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classiÔ¨Åcation network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [40], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.

Figure 1: Images synthetically generated by Plug and Play Generative Networks at high-resolution (227x227) for four ImageNet classes. Not only are many images nearly photorealistic, but samples within a class are diverse.
1. Introduction
Recent years have seen generative models that are increasingly capable of synthesizing diverse, realistic images that capture both the Ô¨Åne-grained details and global coherence of natural images [54, 27, 9, 15, 43, 24]. However, many important open challenges remain, including (1) pro-
‚Ä†This work was mostly performed at Geometric Intelligence, which Uber acquired to create Uber AI Labs.

1

(a) Real: top 9

(b) DGN-AM [37]

(c) Real: random 9

(d) PPGN (this)

Figure 2: For the ‚Äúcardoon‚Äù class neuron in a pre-trained ImageNet classiÔ¨Åer, we show: a) the 9 real training set images that most highly activate that neuron; b) images synthesized by DGN-AM [37], which are of similar type and diversity to the real top-9 images; c) random real training set images in the cardoon class; and d) images synthesized by PPGN, which better represent the diversity of random images from the class. Fig. S10 shows the same four groups for other classes.

ducing photo-realistic images at high resolutions [30], (2) training generators that can produce a wide variety of images (e.g. all 1000 ImageNet classes) instead of only one or a few types (e.g. faces or bedrooms [43]), and (3) producing a diversity of samples that match the diversity in the dataset instead of modeling only a subset of the data distribution [14, 53]. Current image generative models often work well at low resolutions (e.g. 32 √ó 32), but struggle to generate high-resolution (e.g. 128 √ó 128 or higher), globally coherent images (especially for datasets such as ImageNet [7] that have a large variability [41, 47, 14]) due to many challenges including difÔ¨Åculty in training [47, 41] and computationally expensive sampling procedures [54, 55].
Nguyen et al. [37] recently introduced a technique that produces high quality images at a high resolution. Their Deep Generator Network-based Activation Maximization1 (DGN-AM) involves training a generator G to create realistic images from compressed features extracted from a pretrained classiÔ¨Åer network E (Fig. 3f). To generate images conditioned on a class, an optimization process is launched to Ô¨Ånd a hidden code h that G maps to an image that highly activates a neuron in another classiÔ¨Åer C (not necessarily the same as E). Not only does DGN-AM produce realistic images at a high resolution (Figs. 2b & S10b), but, without having to re-train G, it can also produce interesting new types of images that G never saw during training. For example, a G trained on ImageNet can produce ballrooms, jail cells, and picnic areas if C is trained on the MIT Places dataset (Fig. S17, top).
A major limitation with DGN-AM, however, is the lack of diversity in the generated samples. While samples may vary slightly (e.g. ‚Äúcardoons‚Äù with two or three Ô¨Çowers viewed from slightly different angles; see Fig. 2b), the whole image tends to have the same composition (e.g. a
1 Activation maximization is a technique of searching via optimization for the synthetic image that maximally activates a target neuron in order to understand which features that neuron has learned to detect [11].

closeup of a single cardoon plant with a green background). It is noteworthy that the images produced by DGN-AM closely match the images from that class that most highly activate the class neuron (Fig. 2a). Optimization often converges to the same mode even with different random initializations, a phenomenon common with activation maximization [11, 40, 59]. In contrast, real images within a class tend to show more diversity (Fig. 2c). In this paper, we improve the diversity and quality of samples produced via DGN-AM by adding a prior on the latent code that keeps optimization along the manifold of realistic-looking images (Fig. 2d).
We do this by providing a probabilistic framework in which to unify and interpret activation maximization approaches [48, 64, 40, 37] as a type of energy-based model [4, 29] where the energy function is a sum of multiple constraint terms: (a) priors (e.g. biasing images to look realistic) and (b) conditions, typically given as a category of a separately trained classiÔ¨Åcation model (e.g. encouraging images to look like ‚Äúpianos‚Äù or both ‚Äúpianos‚Äù and ‚Äúcandles‚Äù). We then show how to sample iteratively from such models using an approximate Metropolis-adjusted Langevin sampling algorithm.
We call this general class of models Plug and Play Generative Networks (PPGN). The name reÔ¨Çects an important, attractive property of the method: one is free to design an energy function, and ‚Äúplug and play‚Äù with different priors and conditions to form a new generative model. This property has recently been shown to be useful in multiple image generation projects that use the DGN-AM generator network prior and swap in different condition networks [66, 13]. In addition to generating images conditioned on a class, PPGNs can generate images conditioned on text, forming a text-to-image generative model that allows one to describe an image with words and have it synthesized. We accomplish this by attaching a recurrent, image-captioning network (instead of an image classiÔ¨Åcation network) to the output of the generator, and performing similar iterative

2

sampling. Note that, while this paper discusses only the image generation domain, the approach should generalize to many other data types. We publish our code and the trained networks at http://EvolvingAI.org/ppgn.

2. Probabilistic interpretation of iterative image generation methods

Beginning with the Metropolis-adjusted Langevin algorithm [46, 45] (MALA), it is possible to deÔ¨Åne a Markov chain Monte Carlo (MCMC) sampler whose stationary distribution approximates a given distribution p(x). We refer to our variant of MALA as MALA-approx, which uses the following transition operator:2

xt+1 = xt + 12‚àá log p(xt) + N (0, 23)

(1)

A full derivation and discussion is given in Sec. S6. Using this sampler we Ô¨Årst derive a probabilistically interpretable formulation for activation maximization methods (Sec. 2.1) and then interpret other activation maximization algorithms in this framework (Sec. 2.2, Sec. S7).

2.1. Probabilistic framework for Activation Maximization

Assume we wish to sample from a joint model p(x, y), which can be decomposed into an image model and a classiÔ¨Åcation model:

p(x, y) = p(x)p(y|x)

(2)

This equation can be interpreted as a ‚Äúproduct of experts‚Äù [19] in which each expert determines whether a soft constraint is satisÔ¨Åed. First, a p(y|x) expert determines a condition for image generation (e.g. images have to be classiÔ¨Åed as ‚Äúcardoon‚Äù). Also, in a high-dimensional image space, a good p(x) expert is needed to ensure the search stays in the manifold of image distribution that we try to model (e.g. images of faces [6, 63], shoes [67] or natural images [37]), otherwise we might encounter ‚Äúfooling‚Äù examples that are unrecognizable but have high p(y|x) [38, 51]. Thus, p(x) and p(y|x) together impose a complicated high-dimensional constraint on image generation.
We could write a sampler for the full joint p(x, y), but because y variables are categorical, suppose for now that we Ô¨Åx y to be a particular chosen class yc, with yc either sampled or chosen outside the inner sampling loop.3 This

2 We abuse notation slightly in the interest of space and denote as

N (0,

2 3

)

a

sample

from

that

distribution.

The

Ô¨Årst

step

size

is

given

as

12

in anticipation of later splitting into separate 1 and 2 terms.

3 One could resample y in the loop as well, but resampling y via the

Langevin family under consideration is not a natural Ô¨Åt: because y values

from the data set are one-hot ‚Äì and from the model hopefully nearly so ‚Äì

there will be a wide small- or zero-likelihood region between (x, y) pairs

coming from different classes. Thus making local jumps will not be a good

sampling scheme for the y components.

leaves us with the conditional p(x|y):

p(x|y = yc) = p(x)p(y = yc|x)/p(y = yc)

‚àù p(x)p(y = yc|x)

(3)

We can construct a MALA-approx sampler for this model, which produces the following update step:

xt+1 = xt + 12‚àá log p(xt|y = yc) + N (0, 23)
= xt + 12‚àá log p(xt)+ 12‚àá log p(y = yc|xt)+N (0, 23) (4)

Expanding the ‚àá into explicit partial derivatives and decoupling 12 into explicit 1 and 2 multipliers, we arrive at the following form of the update rule:

xt+1 = xt+

1

‚àÇ

log p(xt) ‚àÇxt

+

2

‚àÇ

log

p(y = ‚àÇxt

yc|xt)

+N

(0,

23)

(5)

We empirically found that decoupling the 1 and 2 mul-

tipliers works better. An intuitive interpretation of the ac-

tions of these three terms is as follows:

‚Ä¢ 1 term: take a step from the current image xt toward one that looks more like a generic image (an image from any class).

‚Ä¢ 2 term: take a step from the current image xt toward an image that causes the classiÔ¨Åer to output higher conÔ¨Ådence in the chosen class. The p(y = yc|xt) term is typically modeled by the softmax output units of a modern convnet, e.g. AlexNet [26] or VGG [49].

‚Ä¢ 3 term: add a small amount of noise to jump around the search space to encourage a diversity of images.

2.2. Interpretation of previous models
Aside from the errors introduced by not including a reject step, the stationary distribution of the sampler in Eq. 5 will converge to the appropriate distribution if the terms are chosen appropriately [61]. Thus, we can use this framework to interpret previously proposed iterative methods for generating samples, evaluating whether each method faithfully computes and employs each term.
There are many previous approaches that iteratively sample from a trained model to generate images [48, 64, 40, 37, 60, 2, 11, 63, 67, 6, 39, 38, 34], with methods designed for different purposes such as activation maximization [48, 64, 40, 37, 60, 11, 38, 34] or generating realisticlooking images by sampling in the latent space of a generator network [63, 37, 67, 6, 2, 17]. However, most of them are gradient-based, and can be interpreted as a variant of MCMC sampling from a graphical model [25].
While an analysis of the full spectrum of approaches is outside this paper‚Äôs scope, we do examine a few representative approaches under this framework in Sec. S7. In

3

PPGN	 ¬†with	 ¬†different	 ¬†learned	 ¬†prior	 ¬†networks	 ¬†(i.e.	 ¬†different	 D¬† AEs)

a PPGN-¬≠‚ÄêÌ†µÌ±• Image	 ¬†classifier

b DGN-¬≠‚ÄêAM

Image	 ¬†classifier

c PPGN-¬≠‚Äê‚Ñé

Image	 ¬†classifier

Ì†µÌ±• + Ì†µÌºÇ DAE

C classes

d Joint	 ¬†PPGN-¬≠‚Äê‚Ñé

‚Ñé

G

Ì†µÌ±•

C

‚Ñé + Ì†µÌºÇ

G

(no	 ¬†learned	 ¬†p(h)	 ¬†prior)

classes

DAE

Image	 ¬†classifier

e Noiseless	 ¬†joint	 ¬†PPGN-¬≠‚Äê‚Ñé

Ì†µÌ±•

C

classes

Image	 ¬†classifier

Pre-¬≠‚Äêtrained	 ¬†convnet for	 ¬†image	 ¬†classification

f

Ì†µÌ±• E1 ‚Ñé$ E2

‚Ñé

image

pool5

fc6

1000 labels

Encoder	 ¬†network	 E¬†

Image-¬≠‚Äêcaptioning	 ¬†network a red car END
g
features

‚Ñé + Ì†µÌºÇ

G

Ì†µÌ±• + Ì†µÌºÇ

E2 ‚Ñé$ + Ì†µÌºÇ E1

C classes

‚Ñé

G

Ì†µÌ±•

E2 ‚Ñé$ E1

Sampling	 ¬†conditioning	 ¬†on	 ¬†classes

C classes

‚Ñé

G

Ì†µÌ±•

C

E2 ‚Ñé$ E1

START a red car

Sampling	 ¬†conditioning	 ¬†on	 ¬†captions

Figure 3: Different variants of PPGN models we tested. The Noiseless Joint PPGN-h (e), which we found empirically produces the best images, generated the results shown in Figs. 1 & 2 & Sections 3.5 & 4. In all variants, we perform iterative sampling following the gradients of two terms: the condition (red arrows) and the prior (black arrows). (a) PPGN-x (Sec. 3.1): To avoid fooling examples [38] when sampling in the high-dimensional image space, we incorporate a p(x) prior modeled via a denoising autoencoder (DAE) for images, and sample images conditioned on the output classes of a condition network C (or, to visualize hidden neurons, conditioned upon the activation of a hidden neuron in C). (b) DGN-AM (Sec. 3.2): Instead of sampling in the image space (i.e. in the space of individual pixels), Nguyen et al. [37] sample in the abstract, high-level feature space h of a generator G trained to reconstruct images x from compressed features h extracted from a pre-trained encoder E (f). Because the generator network was trained to produce realistic images, it serves as a prior on p(x) since it ideally can only generate real images. However, this model has no learned prior on p(h) (save for a simple Gaussian assumption). (c) PPGN-h (Sec. 3.3): We attempt to improve the mixing speed and image quality by incorporating a learned p(h) prior modeled via a multi-layer perceptron DAE for h. (d) Joint PPGN-h (Sec. 3.4): To improve upon the poor data modeling of the DAE in PPGN-h, we experiment with treating G + E1 + E2 as a DAE that models h via x. In addition, to possibly improve the robustness of G, we also add a small amount of noise to h1 and x during training and sampling, treating the entire system as being composed of 4 interleaved models that share parameters: a GAN and 3 interleaved DAEs for x, h1 and h, respectively. This model mixes substantially faster and produces better image quality than DGN-AM and PPGN-h (Fig. S14). (e) Noiseless Joint PPGN-h (Sec. 3.5): We perform an ablation study on the Joint PPGN-h, sweeping across noise levels or loss combinations, and found a Noiseless Joint PPGN-h variant trained with one less loss (Sec. S9.4) to produce the best image quality. (f) A pre-trained image classiÔ¨Åcation network (here, AlexNet trained on ImageNet) serves as the encoder network E component of our model by mapping an image x to a useful, abstract, high-level feature space h (here, AlexNet‚Äôs fc6 layer). (g) Instead of conditioning on classes, we can generate images conditioned on a caption by attaching a recurrent, image-captioning network to the output layer of G, and performing similar iterative sampling.

particular, we interpret the models that lack a p(x) image prior, yielding adversarial or fooling examples [51, 38] as setting ( 1, 2, 3) = (0, 1, 0); and methods that use L2 decay during sampling as using a Gaussian p(x) prior with ( 1, 2, 3) = (Œª, 1, 0). Both lack a noise term and thus sacriÔ¨Åce sample diversity.
3. Plug and Play Generative Networks
Previous models are often limited in that they use handengineered priors when sampling in either image space or the latent space of a generator network (see Sec. S7). In this paper, we experiment with 4 different explicitly learned priors modeled by a denoising autoencoder (DAE) [57].
We choose a DAE because, although it does not allow evaluation of p(x) directly, it does allow approximation of the gradient of the log probability when trained with Gaus-

sian noise with variance œÉ2 [1]; with sufÔ¨Åcient capacity and training time, the approximation is perfect in the limit as œÉ ‚Üí 0:

‚àÇ log p(x) ‚àÇx

‚âà

Rx(x) ‚àí x œÉ2

(6)

where Rx is the reconstruction function in x-space representing the DAE, i.e. Rx(x) is a ‚Äúdenoised‚Äù output of the autoencoder Rx (an encoder followed by a decoder) when the encoder is fed input x. This term approximates exactly
the 1 term required by our sampler, so we can use it to deÔ¨Åne the steps of a sampler for an image x from class c. Pulling the œÉ2 term into 1, the update is:

4

xt+1 = xt+

1

Rx(xt)‚àíxt

+

2

‚àÇ

log

p(y = ‚àÇxt

yc|xt)

+N

(0,

23)

(7)

3.1. PPGN-x: DAE model of p(x)

First, we tested using a DAE to model p(x) directly (Fig. 3a) and sampling from the entire model via Eq. 7. However, we found that PPGN-x exhibits two expected problems: (1) it models the data distribution poorly; and (2) the chain mixes slowly. More details are in Sec. S11.

3.2. DGN-AM: sampling without a learned prior

Poor mixing in the high-dimensional pixel space of PPGN-x is consistent with previous observations that mixing on higher layers can result in faster exploration of the space [5, 33]. Thus, to ameliorate the problem of slow mixing, we may reparameterize p(x) as h p(h)p(x|h)dh for some latent h, and perform sampling in this lowerdimensional h-space.
While several recent works had success with this approach [37, 6, 63], they often hand-design the p(h) prior. Among these, the DGN-AM method [37] searches in the latent space of a generator network G to Ô¨Ånd a code h such that the image G(h) highly activates a given neuron in a target DNN. We start by reproducing their results for comparison. G is trained following the methodology in Dosovitskiy & Brox [9] with an L2 image reconstruction loss, a Generative Adversarial Networks (GAN) loss [14], and an L2 loss in a feature space h1 of an encoder E (Fig. 3f). The last loss encourages generated images to match the real images in a high-level feature space and is referred to as ‚Äúfeature matching‚Äù [47] in this paper, but is also known as ‚Äúperceptual similarity‚Äù [28, 9] or a form of ‚Äúmoment matching‚Äù [31]. Note that in the GAN training for G, we simultaneously train a discriminator D to tell apart real images x vs. generated images G(h). More training details are in Sec. S9.4.
The directed graphical model interpretation of DGN-AM is h ‚Üí x ‚Üí y (see Fig. 3b) and the joint p(h, x, y) can be decomposed into:

p(h, x, y) = p(h)p(x|h)p(y|x)

(8)

where h in this case represents features extracted from the Ô¨Årst fully connected layer (called fc6) of a pre-trained AlexNet [26] 1000-class ImageNet [7] classiÔ¨Åcation network (see Fig. 3f). p(x|h) is modeled by G, an upconvolutional (also ‚Äúdeconvolutional‚Äù) network [10] with 9 upconvolutional and 3 fully connected layers. p(y|x) is modeled by C, which in this case is also the AlexNet classiÔ¨Åer. The model for p(h) was an implicit unimodal Gaussian implemented via L2 decay in h-space [37].
Since x is a deterministic variable, the model simpliÔ¨Åes to:

p(h, y) = p(h)p(y|h)

(9)

From Eq. 5, if we deÔ¨Åne a Gaussian p(h) centered at 0 and set ( 1, 2, 3) = (Œª, 1, 0), pulling Gaussian constants into Œª, we obtain the following noiseless update rule in Nguyen et al. [37] to sample h from class yc:

ht+1 = (1 ‚àí Œª)ht +

‚àÇ log p(y = yc|ht)

2

‚àÇht

= (1 ‚àí Œª)ht +

‚àÇ log Cc(G(ht)) ‚àÇG(ht)

2 ‚àÇG(ht)

‚àÇht

(10)

where Cc(¬∑) represents the output unit associated with class yc. As before, all terms are computable in a single forwardbackward pass. More concretely, to compute the 2 term, we push a code h through the generator G and condition network C to the output class c that we want to condition on (Fig. 3b, red arrows), and back-propagate the gradient via the same path to h. The Ô¨Ånal h is pushed through G to produce an image sample.
Under this newly proposed framework, we have successfully reproduced the original DGN-AM results and their issue of converging to the same mode when starting from different random initializations (Fig. 2b). We also found that DGN-AM mixes somewhat poorly, yielding the same image after many sampling steps (Figs. S13b & S14b).

3.3. PPGN-h: Generator and DAE model of p(h)

We attempt to address the poor mixing speed of DGNAM by incorporating a proper p(h) prior learned via a DAE into the sampling procedure described in Sec. 3.2. SpeciÔ¨Åcally, we train Rh, a 7-layer, fully-connected DAE on h (as before, h is a fc6 feature vector). The size of the hidden layers are respectively: 4096 ‚àí 2048 ‚àí 1024 ‚àí 500 ‚àí 1024 ‚àí 2048 ‚àí 4096. Full training details are provided in S9.3.
The update rule to sample h from this model is similar to Eq. 10 except for the inclusion of all three terms:

ht+1

= ht +

1 (Rh (ht ) ‚àí ht ) +

‚àÇ log Cc(G(ht)) ‚àÇG(ht)

2 ‚àÇG(ht)

‚àÇht

+N (0, 23)

(11)

Concretely, to compute Rh(ht) we push ht through the

learned DAE, encoding and decoding it (Fig. 3c, black ar-

rows). The 2 term is computed via a forward and backward

pass through both G and C networks as before (Fig. 3c, red

arrows). Lastly, we add the same amount of noise N (0,

2 3

)

used during DAE training to h. Equivalently, noise can also

be added before the encode-decode step. We sample4 using ( 1, 2, 3) = (10‚àí5, 1, 10‚àí5) and

show results in Figs. S13c & S14c. As expected, the chain

4 If faster mixing or more stable samples are desired, then 1 and 3 can be scaled up or down together. Here we scale both down to 10‚àí5.

5

mixes faster than PPGN-x, with subsequent samples appearing more qualitatively different from their predecessors. However, the samples for PPGN-h are qualitatively similar to those from DGN-AM (Figs. S13b & S14b). Samples still lack quality and diversity, which we hypothesize is due to the poor p(h) model learned by the DAE.
3.4. Joint PPGN-h: joint Generator and DAE
The previous result suggests that the simple multi-layer perceptron DAE poorly modeled the distribution of fc6 features. This could occur because the DAE faces the generally difÔ¨Åcult unconstrained density estimation problem. To combat this issue, we experiment with modeling h via x with a DAE: h ‚Üí x ‚Üí h. Intuitively, to help the DAE better model h, we force it to generate realistic-looking images x from features h and then decode them back to h. One can train this DAE from scratch separately from G (as done for PPGN-h). However, in the DGN-AM formulation, G models the h ‚Üí x (Fig. 3b) and E models the x ‚Üí h (Fig. 3f). Thus, the composition G(E(.)) can be considered an AE h ‚Üí x ‚Üí h (Fig. 3d).
Note that G(E(.)) is theoretically not a formal h-DAE because its two components were trained with neither noise added to h nor an L2 reconstruction loss for h [37] (more details in Sec. S9.4) as is required for regular DAE training [57]. To make G(E(.)) a more theoretically justiÔ¨Åable h-DAE, we add noise to h and train G with an additional reconstruction loss for h (Fig. S9c). We do the same for x and h1 (pool5 features), hypothesizing that a little noise added to x and h1 might encourage G to be more robust [57]. In other words, with the same existing network structures from DGN-AM [37], we train G differently by treating the entire model as being composed of 3 interleaved DAEs that share parameters: one each for h, h1, and x (see Fig. S9c). Note that E remains frozen, and G is trained with 4 losses in total i.e. three L2 reconstruction losses for x, h, and h1 and a GAN loss for x. See Sec. S9.5 for full training details. We call this the Joint PPGN-h model.
We sample from this model following the update rule in Eq. 11 with ( 1, 2) = (10‚àí5, 1), and with noise added to all three variables: h, h1 and x instead of only to h (Fig. 3d vs e). The noise amounts added at each layer are the same as those used during training. As hypothesized, we observe that the sampling chain from this model mixes substantially faster and produces samples with better quality than all previous PPGN treatments (Figs. S13d & S14d) including PPGN-h, which has a multi-layer perceptron h-DAE.
3.5. Ablation study with Noiseless Joint PPGN-h
While the Joint PPGN-h outperforms all previous treatments in sample quality and diversity (as the chain mixes faster), the model is trained with a combination of four losses and noise added to all variables. This complex train-

ing process can be difÔ¨Åcult to understand, making further improvements non-intuitive. To shed more light into how the Joint PPGN-h works, we perform ablation experiments which later reveal a better-performing variant.
Noise sweeps. To understand the effects of adding noise to each variable, we train variants of the Joint PPGN-h (1) with different noise levels, (2) using noise on only a single variable, and (3) using noise on multiple variables simultaneously. We did not Ô¨Ånd these variants to produce qualitatively better reconstruction results than the Joint PPGN-h. Interestingly, in a PPGN variant trained with no noise at all, the h-autoencoder given by G(E(.)) still appears to be contractive, i.e. robust to a large amount of noise (Fig. S16). This is beneÔ¨Åcial during sampling; if ‚Äúunrealistic‚Äù codes appear, G could map them back to realistic-looking images. We believe this property might emerge for multiple reasons: (1) G and E are not trained jointly; (2) h features encode global, high-level rather than local, low-level information; (3) the presence of the adversarial cost when training G could make the h ‚Üí x mapping more ‚Äúmany-to-one‚Äù by pushing x towards modes of the image distribution.
Combinations of losses. To understand the effects of each loss component, we repeat the Joint PPGN-h training (Sec. 3.4), but without noise added to the variables. Specifically, we test different combinations of losses and compare the quality of images G(h) produced by pushing the codes h of real images through G (without MCMC sampling).
First, we found that removing the adversarial loss from the 4-loss combination yields blurrier images (Fig. S8c). Second, we compare 3 different feature matching losses: fc6, pool5, and both fc6 and pool5 combined, and found that pool5 feature matching loss leads to the best image quality (Sec. S8). Our result is consistent with Dosovitskiy & Brox [9]. Thus, the model that we found empirically to produce the best image quality is trained without noise and with three losses: a pool5 feature matching loss, an adversarial loss, and an image reconstruction loss. We call this variant ‚ÄúNoiseless Joint PPGN-h‚Äù: it produced the results in Figs. 1 & 2 and Sections 3.5 & 4.
Noiseless Joint PPGN-h. We sample from this model with ( 1, 2, 3) = (10‚àí5, 1, 10‚àí17) following the same update rule in Eq. 11 (we need noise to make it a proper sampling procedure, but found that inÔ¨Ånitesimally small noise produces better and more diverse images, which is to be expected given that the DAE in this variant was trained without noise). Interestingly, the chain mixes substantially faster than DGN-AM (Figs. S13e & S13b) although the only difference between two treatments is the existence of the learned p(h) prior. Overall, the Noiseless Joint PPGNh produces a large amount of sample diversity (Fig. 2). Compared to the Joint PPGN-h, the Noiseless Joint PPGNh produces better image quality, but mixes slightly slower (Figs. S13 & S14). Sweeping across the noise levels dur-

6

ing sampling, we noted that larger noise amounts often results in worse image quality, but not necessarily faster mixing speed (Fig. S15). Also, as expected, a small 1 multiplier makes the chain mix faster, and a large one pulls the samples towards being generic instead of class-speciÔ¨Åc (Fig. S23).
Evaluations. Evaluating image generative models is challenging, and there is not yet a commonly accepted quantitative performance measure [53]. We qualitatively evaluate sample diversity of the Noiseless Joint PPGN-h variant by running 10 sampling chains, each for 200 steps, to produce 2000 samples, and Ô¨Åltering out samples with class probability of less than 0.97. From the remaining, we randomly pick 400 samples and plot them in a grid t-SNE [56] (Figs. S12 & S11). More examples for the reader‚Äôs evaluation of sample quality and diversity are provided in Figs. S21, S22 & S25. To better observe the mixing speed, we show videos of sampling chains (with one sample per frame; no samples Ô¨Åltered out) from within classes and between 10 different classes at https://goo.gl/ 36S0Dy. In addition, Table S3 provides quantitative comparisons between PPGN, auxiliary classiÔ¨Åer GAN [41] and real ImageNet images in image quality (via Inception score [47] & Inception accuracy [41]) and diversity (via MSSSIM metric [41]).
While future work is required to fully understand why the Noiseless Joint PPGN-h produces high-quality images at a high resolution for 1000-class ImageNet more successfully than other existing latent variable models [41, 47, 43], we discuss possible explanations in Sec. S12.
4. Additional results
In this section, we take the Noiseless Joint PPGN-h model and show its capabilities on several different tasks.
4.1. Generating images with different condition networks
A compelling property that makes PPGN different from other existing generative models is that one can ‚Äúplug and play‚Äù with different prior and condition components (as shown in Eq. 2) and ask the model to perform new tasks, including challenging the generator to produce images that it has never seen before. Here, we demonstrate this feature by replacing the p(y|x) component with different networks.
Generating images conditioned on classes Above we showed that PPGN could generate a diversity
of high quality samples for ImageNet classes (Figs. 1 & 2 & Sec. 3.5). Here, we test whether the generator G within the PPGN can generalize to new types of images that it has never seen before. SpeciÔ¨Åcally, we sample with a different p(y|x) model: an AlexNet DNN [26] trained to classify 205 categories of scene images from the MIT Places

Figure 4: Images synthesized conditioned on MIT Places [65] classes instead of ImageNet classes.
dataset [65]. Similar to DGN-AM [37], the PPGN generates realistic-looking images for classes that the generator was never trained on, such as ‚Äúalley‚Äù or ‚Äúhotel room‚Äù (Fig. 4). A side-by-side comparison between DGN-AM and PPGN are in Fig. S17. Generating images conditioned on captions
Instead of conditioning on classes, we can also condition the image generation on a caption (Fig. 3g). Here, we swap in an image-captioning recurrent network (called LRCN) from [8] that was trained on the MS COCO dataset [32] to predict a caption y given an image x. SpeciÔ¨Åcally, LRCN is a two-layer LSTM network that generates captions conditioned on features extracted from the output softmax layer of AlexNet [26].
Figure 5: Images synthesized to match a text description. A PPGN containing the image captioning model from [8] can generate reasonable images that differ based on userprovided captions (e.g. red car vs. blue car, oranges vs. a pile of oranges). For each caption, we show 3 images synthesized starting from random codes (more in Fig. S18).
We found that PPGN can generate reasonable images in many cases (Figs. 5 & S18), although the image quality is lower than when conditioning on classes. In other cases, it also fails to generate high-quality images for certain types of images such as ‚Äúpeople‚Äù or ‚Äúgiraffe‚Äù, which are not cate-

7

gories in the generator‚Äôs training set (Fig. S18). We also observe ‚Äúfooling‚Äù images [38]‚Äîthose that look unrecognizable to humans, but produce high-scoring captions. More results are in Fig. S18. The challenges for this task could be: (1) the sampling is conditioned on many (10 ‚àí 15) words at the same time, and the gradients backpropagated from different words could conÔ¨Çict with each other; (2) the LRCN captioning model itself is easily fooled, thus additional priors on the conversion from image features to natural language could improve the result further; (3) the depth of the entire model (AlexNet and LRCN) impairs gradient propagation during sampling. In the future, it would be interesting to experiment with other state-of-the-art image captioning models [12, 58]. Overall, we have demonstrated that PPGNs can be Ô¨Çexibly turned into a text-to-image model by combining the prior with an image captioning network, and this process does not even require additional training.
Generating images conditioned on hidden neurons PPGNs can perform a more challenging form of acti-
vation maximization called Multifaceted Feature Visualization (MFV) [40], which involves generating the set of inputs that activate a given neuron. Instead of conditioning on a class output neuron, here we condition on a hidden neuron, revealing many facets that a neuron has learned to detect (Fig. 6).

Figure 7: We perform class-conditional image sampling to Ô¨Åll in missing pixels (see Sec. 4.2). In addition to conditioning on a speciÔ¨Åc class (PPGN), PPGN-context also constrains the code h to produce an image that matches the context region. PPGN-context (c) matches the pixels surrounding the masked region better than PPGN (b), and semantically Ô¨Ålls in better than the Context-Aware Fill feature in Photoshop (d) in many cases. The result shows that the class-conditional PPGN does understand the semantics of images. More PPGN-context results are in Fig. S24.

Figure 6: Images synthesized to activate a hidden neuron (number 196) previously identiÔ¨Åed as a ‚Äúface detector neuron‚Äù [64] in the Ô¨Åfth convolutional layer of the AlexNet DNN trained on ImageNet. The PPGN uncovers a large diversity of types of inputs that activate this neuron, thus performing Multifaceted Feature Visualization [40], which sheds light into what the neuron has learned to detect. The different facets include different types of human faces (top row), dog faces (bottom row), and objects that only barely resemble faces (e.g. the windows of a house, or something resembling green hair above a Ô¨Çesh-colored patch). More examples and details are shown in Figs. S19 & S20.
4.2. Inpainting
Because PPGNs can be interpreted probabilistically, we can also sample from them conditioned on part of an image (in addition to the class condition) to perform inpainting‚Äî Ô¨Ålling in missing pixels given the observed context regions [42, 3, 63, 54]. The model must understand the entire image

to be able to reasonably Ô¨Åll in a large masked out region that is positioned randomly. Overall, we found that PPGNs are able to perform inpainting suggesting that the models do ‚Äúunderstand‚Äù the semantics of concepts such as junco or bell pepper (Fig. 7) rather than merely memorizing the images. More details and results are in Sec. S10.
5. Conclusion
The most useful property of PPGN is the capability of ‚Äúplug and play‚Äù‚Äîallowing one to drop in a replaceable condition network and generate images according to a condition speciÔ¨Åed at test time. Beyond the applications we demonstrated here, one could use PPGNs to synthesize images for videos or create arts with one or even multiple condition networks at the same time [13]. Note that DGN-AM [37]‚Äîthe predecessor of PPGNs‚Äîhas previously enabled both scientists and amateurs without substantial resources to take a pre-trained condition network and generate art [13] and scientiÔ¨Åc visualizations [66]. An explanation for why this is possible is that the fc6 features that the generator was trained to invert are relatively general and cover the set of natural images. Thus, there is great value in producing Ô¨Çexible, powerful generators that can be combined with pretrained condition networks in a plug and play fashion.

8

Acknowledgments
We thank Theo Karaletsos and Noah Goodman for helpful discussions, and Jeff Donahue for providing a trained image captioning model [8] for our experiments. We also thank Joost Huizinga, Christopher Stanton, Rosanne Liu, Tyler Jaszkowiak, Richard Yang, and Jon Berliner for invaluable suggestions on preliminary drafts.
References
[1] G. Alain and Y. Bengio. What regularized auto-encoders learn from the data-generating distribution. The Journal of Machine Learning Research, 15(1):3563‚Äì3593, 2014. 4, 17, 19
[2] K. Arulkumaran, A. Creswell, and A. A. Bharath. Improving sampling from generative autoencoders with markov chains. arXiv preprint arXiv:1610.09296, 2016. 3, 13
[3] C. Barnes, E. Shechtman, A. Finkelstein, and D. Goldman. Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Transactions on GraphicsTOG, 28(3):24, 2009. 8, 17
[4] I. G. Y. Bengio and A. Courville. Deep learning. Book in preparation for MIT Press, 2016. 2, 12
[5] Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai. Better mixing via deep representations. In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 552‚Äì560, 2013. 5
[6] A. Brock, T. Lim, J. Ritchie, and N. Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016. 3, 5, 13
[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248‚Äì255. IEEE, 2009. 2, 5, 14, 16, 29
[8] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Computer Vision and Pattern Recognition, 2015. 7, 9, 29
[9] A. Dosovitskiy and T. Brox. Generating Images with Perceptual Similarity Metrics based on Deep Networks. In Advances in Neural Information Processing Systems, 2016. 1, 5, 6, 14, 15, 16, 18, 27
[10] A. Dosovitskiy, J. Tobias Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1538‚Äì1546, 2015. 5, 16
[11] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network. Technical report, Technical report, University of Montreal, 2009. 2, 3, 13, 14
[12] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A. Ranzato, and T. Mikolov. Devise: A deep visual-semantic embedding model. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2121‚Äì 2129. Curran Associates, Inc., 2013. 8

[13] G. Goh. Image synthesis from yahoo open nsfw. https: //opennsfw.gitlab.io, 2016. 2, 8
[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672‚Äì2680, 2014. 2, 5, 16, 18, 19, 27
[15] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image generation. In ICML, 2015. 1
[16] A. Gretton, K. M. Borgwardt, M. Rasch, B. Scho¬®lkopf, and A. J. Smola. A kernel method for the two-sample-problem. In Advances in neural information processing systems, pages 513‚Äì520, 2006. 15
[17] T. Han, Y. Lu, S.-C. Zhu, and Y. N. Wu. Alternating backpropagation for generator network. In AAAI, 2017. 3, 13
[18] W. K. Hastings. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57(1):97‚Äì 109, 1970. 12
[19] G. E. Hinton. Products of experts. In ArtiÔ¨Åcial Neural Networks, 1999. ICANN 99. Ninth International Conference on (Conf. Publ. No. 470), volume 1, pages 1‚Äì6. IET, 1999. 3
[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015. 16
[21] Y. Jia. Caffe: An open source convolutional architecture for fast feature embedding. http://caffe. berkeleyvision.org/, 2013. 16
[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. arXiv preprint arXiv:1603.08155, 2016. 14
[23] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 16
[24] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. Dec. 2014. 1, 19
[25] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009. 3, 12
[26] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiÔ¨Åcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106‚Äì1114, 2012. 3, 5, 7, 14, 16, 18, 27
[27] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. Journal of Machine Learning Research, 15:29‚Äì37, 2011. 1
[28] A. B. L. Larsen, S. K. S√∏nderby, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. CoRR, abs/1512.09300, 2015. 5, 14
[29] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning. Predicting structured data, 1:0, 2006. 2
[30] C. Ledig, L. Theis, F. Husza¬¥r, J. Caballero, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802, 2016. 2

9

[31] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International Conference on Machine Learning, pages 1718‚Äì1727, 2015. 5
[32] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla¬¥r, and C. L. Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740‚Äì755. Springer, 2014. 7
[33] H. Luo, P. L. Carrier, A. C. Courville, and Y. Bengio. Texture modeling with convolutional spike-and-slab rbms and deep extensions. In AISTATS, pages 415‚Äì423, 2013. 5
[34] A. Mahendran and A. Vedaldi. Visualizing deep convolutional neural networks using natural pre-images. International Journal of Computer Vision, pages 1‚Äì23, 2016. 3, 13, 14
[35] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087‚Äì1092, 1953. 12
[36] A. Mordvintsev, C. Olah, and M. Tyka. Inceptionism: Going deeper into neural networks. Google Research Blog. Retrieved June, 20, 2015. 14
[37] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In Advances in Neural Information Processing Systems, 2016. 1, 2, 3, 4, 5, 6, 7, 8, 13, 14, 16, 17, 21, 24, 25, 28, 30
[38] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High conÔ¨Ådence predictions for unrecognizable images. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 3, 4, 8, 13
[39] A. Nguyen, J. Yosinski, and J. Clune. Innovation engines: Automated creativity and improved stochastic optimization via deep learning. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), 2015. 3, 13
[40] A. Nguyen, J. Yosinski, and J. Clune. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. In Visualization for Deep Learning Workshop, ICML conference, 2016. 1, 2, 3, 8, 13, 14, 30
[41] A. Odena, C. Olah, and J. Shlens. Conditional Image Synthesis With Auxiliary ClassiÔ¨Åer GANs. ArXiv e-prints, Oct. 2016. 2, 7, 18, 20
[42] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. arXiv preprint arXiv:1604.07379, 2016. 8, 17
[43] A. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. Nov. 2015. 1, 2, 7, 16, 18
[44] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 833‚Äì840, 2011. 27
[45] G. O. Roberts and J. S. Rosenthal. Optimal scaling of discrete approximations to langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1):255‚Äì268, 1998. 3, 12

[46] G. O. Roberts and R. L. Tweedie. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli, pages 341‚Äì363, 1996. 3, 12
[47] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. CoRR, abs/1606.03498, 2016. 2, 5, 7, 16, 18, 19, 20
[48] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classiÔ¨Åcation models and saliency maps. arXiv preprint arXiv:1312.6034, presented at ICLR Workshop 2014, 2013. 2, 3, 13, 14
[49] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 3
[50] C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261, 2016. 20
[51] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. 3, 4, 13
[52] Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and Ô¨Çuctuations for stochastic gradient langevin dynamics. 2014. 12
[53] L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. Nov 2016. International Conference on Learning Representations. 2, 7, 19
[54] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel Recurrent Neural Networks. ArXiv e-prints, Jan. 2016. 1, 2, 8
[55] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu. Conditional image generation with pixelcnn decoders. CoRR, abs/1606.05328, 2016. 2
[56] L. Van der Maaten and G. Hinton. Visualizing data using tsne. Journal of Machine Learning Research, 9(11), 2008. 7, 22, 23, 30
[57] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096‚Äì1103. ACM, 2008. 4, 6, 17, 19, 27
[58] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555, 2014. 8
[59] D. Wei, B. Zhou, A. Torrabla, and W. Freeman. Understanding intra-class knowledge inside cnn. arXiv preprint arXiv:1507.02379, 2015. 2, 14
[60] D. Wei, B. Zhou, A. Torrabla, and W. Freeman. Understanding intra-class knowledge inside cnn. arXiv preprint arXiv:1507.02379, 2015. 3, 13
[61] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681‚Äì688, 2011. 3, 12
[62] J. Xie, Y. Lu, S.-C. Zhu, and Y. N. Wu. Cooperative training of descriptor and generator networks. arXiv preprint arXiv:1609.09408, 2016. 17

10

[63] R. Yeh, C. Chen, T. Y. Lim, M. Hasegawa-Johnson, and M. N. Do. Semantic image inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539, 2016. 3, 5, 8, 13
[64] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding neural networks through deep visualization. In Deep Learning Workshop, International Conference on Machine Learning (ICML), 2015. 2, 3, 8, 13, 14, 17, 30, 31
[65] B. Zhou, A. Khosla, A` . Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. In International Conference on Learning Representations (ICLR), volume abs/1412.6856, 2014. 7, 28
[66] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva. Places: An image database for deep scene understanding. arXiv preprint arXiv:1610.02055, 2016. 2, 8
[67] J.-Y. Zhu, P. Kra¬®henbu¬®hl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pages 597‚Äì613. Springer, 2016. 3, 13
11

Supplementary materials for: Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space

S6. Markov chain Monte Carlo methods and derivation of MALA-approx
Assume a distribution p(x) that we wish to produce samples from. For certain distributions with amenable structure it may be possible to write down directly an independent and identically distributed (IID) sampler, but in general this can be difÔ¨Åcult. In such cases where IID samplers are not readily available, we may instead resort to Markov Chain Monte Carlo (MCMC) methods for sampling. Complete discussions of this topic Ô¨Åll books [25, 4]. Here we brieÔ¨Çy review the background that led to the sampler we propose.
In cases where evaluation of p(x) is possible, we can write down the Metropolis-Hastings (hereafter: MH) sampler for p(x) [35, 18]. It requires a choice of proposal distribution q(x |x); for simplicity we consider (and later use) a simple Gaussian proposal distribution. Starting with an x0 from some initial distribution, the sampler takes steps according to a transition operator deÔ¨Åned by the below routine, with N (0, œÉ2) shorthand for a sample from that Gaussian proposal distribution:
1. xt+1 = xt + N (0, œÉ2)
2. Œ± = p(xt+1)/p(xt)
3. if Œ± < 1, reject sample xt+1 with probability 1 ‚àí Œ± by setting xt+1 = xt, else keep xt+1
In theory, sufÔ¨Åciently many steps of this simple sampling rule produce samples for any computable p(x), but in practice it has two problems: it mixes slowly, because steps are small and uncorrelated in time, and it requires us to be able to compute p(x) to calculate Œ±, which is often not possible. A Metropolis-adjusted Langevin algorithm (hereafter: MALA) [46, 45] addresses the Ô¨Årst problem. This sampler follows a slightly modiÔ¨Åed procedure:
1. xt+1 = xt + œÉ2/2‚àá log p(xt) + N (0, œÉ2)
2. Œ± = f (xt, xt+1, p(xt+1), p(xt))
3. if Œ± < 1, reject sample xt+1 with probability 1 ‚àí Œ± by setting xt+1 = xt, else keep xt+1

where f (¬∑) is the slightly more complex calculation of Œ±, with the notable property that as the step size goes to 0, f (¬∑) ‚Üí 1. This sampler preferentially steps in the direction of higher probability, which allows it to spend less time rejecting low probability proposals, but it still requires computation of p(x) to calculate Œ±.
The stochastic gradient Langevin dynamics (SGLD) method [61, 52] was proposed to sidestep this troublesome requirement by generating probability proposals that are based on a small subset of the data only: by using stochastic gradient descent plus noise, by skipping the accept-reject step, and by using decreasing step sizes. Inspired by SGLD, we deÔ¨Åne an approximate sampler by assuming small step size and doing away with the reject step (by accepting every sample). The idea is that the stochasticity of SGD itself introduces an implicit noise: although the resulting update does not produce asymptotically unbiased samples, it does if we also anneal the step size (or, equivalently, gradually increase the minibatch size).
While an accept ratio of 1 is only approached in the limit as the step size goes to zero, in practice we empirically observe that this approximation produces reasonable samples even for moderate step sizes. This approximation leads to a sampler deÔ¨Åned by the simple update rule:
xt+1 = xt + œÉ2/2‚àá log p(xt) + N (0, œÉ2) (12)
As explained below, we propose to decouple the two step sizes for each of the above two terms after xt, with two independent scaling factors to allow independently tuning each ( 12 and 3 in Eq. 13). This variant makes sense when we consider that the stochasticity of SGD itself introduces more noise, breaking the direct link between the amount of noise injected and the step size under Langevin dynamics.
We note that p(x) ‚àù exp(‚àíEnergy(x)), ‚àá log p(xt) is just the gradient of the energy (because the partition function does not depend on x) and that the scaling factor (œÉ2/2 in the above equation) can be partially absorbed when changing the temperature associated with energy, since temperature is just a multiplicative scaling factor in the energy. Changing that link between the two terms is thus equivalent to changing temperature because the incorrect scale factor can be absorbed in the energy as a change

12

MH MALA MALA-approx

mixes slowly
ok ok

uses accept/ reject step and requires p(x)
yes yes no

update rule (not including accept/reject step)

xt+1 = xt + N (0, œÉ2)

xt+1 = xt + 1/2œÉ‚àá log p(xt) + N (0, œÉ2)

xt+1 = xt +

12‚àá log p(xt) + N (0,

2 3

)

Table S1: Samplers properties assuming Gaussian proposal distributions. Samples are drawn via MALA-approx in this paper.

in the temperature. Since we do not control directly the amount of noise (some of which is now produced by the stochasticity of SGD itself), it is better to ‚Äúmanually‚Äù control the trade-off by introducing an extra hyperparameter. Doing so also may help to compensate for the fact that the SGD noise is not perfectly normal, which introduces a bias in the Markov chain. By manually controlling both the step size and the normal noise, we can thus Ô¨Ånd a good tradeoff between variance (or temperature level, which would blur the distribution) and bias (which makes us sample from a slightly different distribution). In our experience, such decoupling has helped Ô¨Ånd better tradeoffs between sample diversity and quality, perhaps compensating for idiosyncrasies of sampling without a reject step. We call this sampler MALA-approx:

xt+1 = xt + 12‚àá log p(xt) + N (0, 23)

(13)

Table S1 summarizes the samplers and their properties.

S7. Probabilistic interpretation of previous models (continued)
In this paper, we consider four main representative approaches in light of the framework:

1. Activation maximization with no priors [38, 51, 11]
2. Activation maximization with a Gaussian prior [48, 64]
3. Activation maximization with hand-designed priors [48, 64, 40, 60, 39, 38, 34]
4. Sampling in the latent space of a generator network [2, 63, 67, 6, 37, 17]

Here we discuss the Ô¨Årst three and refer readers to the main text (Sec. 2.2) for the fourth approach. Activation maximization with no priors. From Eq. 5, if we set ( 1, 2, 3) = (0, 1, 0) , we obtain a sampler that follows the class gradient directly without contributions from a p(x) term or the addition of noise. In a high-dimensional space, this results in adversarial or fooling images [51, 38]. We can also interpret the sampling procedure in [51, 38]

as a sampler with non-zero 1 but with a p(x) such that

‚àÇ log p(x) ‚àÇx

=

0; in other words, a uniform p(x) where all

images are equally likely.

Activation maximization with a Gaussian prior. To com-

bat the fooling problem [38], several works have used L2 decay, which can be thought of as a simple Gaussian prior

over images [48, 64, 60]. From Eq. 5, if we deÔ¨Åne a Gaus-

sian p(x) centered at the origin (assume the mean image

has been subtracted) and set ( 1, 2, 3) = (Œª, 1, 0), pulling Gaussian constants into Œª, we obtain the following noiseless

update rule:

xt+1

=

(1

‚àí

Œª)xt

+

‚àÇ

log

p(y = ‚àÇxt

yc|xt)

(14)

The Ô¨Årst term decays the current image slightly toward

the origin, as appropriate under a Gaussian image prior, and

the second term pulls the image toward higher probability

regions for the chosen class. Here, the second term is com-

puted as the derivative of the log of a softmax unit in the

output layer of the classiÔ¨Åcation network, which is trained

to model p(y|x). If we let li be the logit outputs of a classiÔ¨Åcation network, where i indexes over the classes, then the

softmax outputs are given by si = exp(li)/ j exp(lj), and the value p(y = yc|xt) is modeled by the softmax unit
sc.

Note that the second term is similar, but not identical,

to the gradient of logit term used by [48, 64, 34]. There

are three variants of computing this class gradient term: 1)

derivative of logit; 2) derivative of softmax; and 3) deriva-

tive of log of softmax. Previously mentioned papers empir-

ically reported that derivative of the logit unit li produces

better visualizations than the derivative of the softmax unit

si (Table S2a vs. b), but this observation had not been fully justiÔ¨Åed [48]. In light of our probablistic interpretation (dis-

cussed in Sec. 2.1), we consider activation maximization as

performing noisy gradient descent to minimize the energy

function E(x, y):

E(x, y) = ‚àílog(p(x, y)) = ‚àílog(p(x)p(y|x)) = ‚àí(log(p(x)) + log(p(y|x))) (15)

To sample from the joint model p(x, y), we follow the energy gradient:

13

a. Derivative of logit. Has worked well in practice [37, 11] but not quite the right term to maximize under the sampler framework set out in this paper.
b. Derivative of softmax. Previously avoided due to poor performance [48, 64], but poor performance may have been due to ill-conditioned optimization rather than the inclusion of logits from other classes. In particular, the term goes to 0 as si goes zero.

‚àÇli ‚àÇx

Ô£´

Ô£∂

‚àÇsi ‚àÇx

=

si

Ô£≠

‚àÇli ‚àÇx

‚àí

sj

‚àÇlj ‚àÇx

Ô£∏

j

c. Derivative of log of softmax. Correct term under the sampler framework set out in this paper. Well-behaved under optimization, perhaps due to the ‚àÇli/‚àÇx term untouched by the si multiplier.

‚àÇ log si = ‚àÇ log p(y = yi|xt)

‚àÇx

‚àÇx

= ‚àÇli ‚àí ‚àÇ log ‚àÇx ‚àÇx

exp(lj )

j

Table S2: A comparison of derivatives for use in activation maximization experiments. The Ô¨Årst has most commonly been used, the second has worked in the past but with some difÔ¨Åculty, but the third is correct under the sampler framework set out in this paper. We perform experiments in this paper with the third variant.

‚àÇE(x, y)

‚àÇlog(p(x)) ‚àÇlog(p(y|x))

=‚àí

+

‚àÇx

‚àÇx

‚àÇx

(16)

which derives the class gradient term that matches that in our framework (Eq. 14, second term). In addition, recall that the classiÔ¨Åcation network is trained to model p(y|x) via softmax, thus the class gradient variant (the derivative of log of softmax) is the most theoretically justiÔ¨Åable in light of our interpretation. We summarize all three variants in Table S2. In overall, we found the proposed class gradient term a) theoretically justiÔ¨Åable under the probabilistic interpretation, and b) working well empirically, and thus suggest it for future activation maximization studies. Activation maximization with hand-designed priors. In an effort to outdo the simple Gaussian prior, many works have proposed more creative, hand-designed image priors such as Gaussian blur [64], total variation [34], jitter [36], and data-driven patch priors [59]. These priors effectively serve as a simple p(x) component. Those that cannot be explicitly expressed in the mathematical p(x) form (e.g. jitter [36] and center-biased regularization [40]) can be written as a general regularization function r(.) as in [64], in which case the noiseless update becomes:

xt+1

=

r(xt)

+

‚àÇ

log

p(y = ‚àÇxt

yc|xt)

(17)

Note that all methods considered in this section are noiseless and therefore produce samples showing diversity

only by starting the optimization process at different initial conditions. The effect is that samples tend to converge to a single mode or a small number of modes [11, 40].
S8. Comparing feature matching losses
The addition of feature matching losses (i.e. the differences between a real image and a generated image not in pixel space, but in a feature space, such as a high-level code in a deep neural network) to the training cost has been shown to substantially improve the quality of samples produced by generator networks, e.g. by producing sharper and more realistic images [9, 28, 22].
Dosovitskiy & Brox [9] used the feature matching loss measured in the pool5 layer code space of AlexNet deep neural network (DNN) [26] trained to classify 1000-class ImageNet images [7]. Here, we empirically compare several feature matching losses computed in different layers of the AlexNet DNN. SpeciÔ¨Åcally, we follow the training procedure in Dosovitskiy & Brox [9], and train 3 generator networks, each with a different feature matching loss computed in different layers from the pretrained AlexNet DNN: a) pool5, b) fc6 and c) both pool5 and fc6 losses. We empirically found that matching the pool5 features leads to the best image quality (Fig. S8), and chose the generator with this loss for the main experiments in the paper.

14

(a) Real images
(b) Joint PPGN-h (Limg + Lh1 + Lh + LGAN )
(c) LGAN removed (Limg + Lh1 + Lh)
(d) Lh1 removed: Limg + Lh + LGAN
(e) Lh removed: Limg + Lh1 + LGAN Figure S8: A comparison of images produced by different generators G, each trained with a different loss combination (below each image). Limg, Lh1 , and Lh are L2 reconstruction losses respectively in the pixel (x), pool5 feature (h1) and fc6 feature (h) space. G is trained to map h ‚Üí x, i.e. reconstructing images from fc6 features. In the Joint PPGN-h treatment (Sec. 3.4), G is trained with a combination of 4 losses (panel b). Here, we perform an ablation study on this loss combination to understand the effect of each loss, and Ô¨Ånd a combination that produces the best image quality. We found that removing the GAN loss yields blurry results (panel c). The Noiseless Joint PPGN-h variant (Sec. 3.5) is trained with the loss combination that produces the best image quality (panel e). Compared to pool5, fc6 feature matching loss often produce the worse image quality because it is effectively encouraging generated images to match the high-level abstract statistics of real images instead of low-level statistics [16]. Our result is in consistent with Dosovitskiy & Brox [9].
15

S9. Training details
S9.1. Common training framework
We use the Caffe framework [21] to train the networks. All networks are trained with the Adam optimizer [23] with momentum Œ≤1 = 0.9, Œ≤2 = 0.999, and Œ≥ = 0.5, and an initial learning rate of 0.0002 following [9]. The batch size is 64. To stabilize the GAN training, we follow heuristic rules based on the ratio of the discriminator loss over generator loss r = lossD/lossG and pause the training of the generator or discriminator if one of them is winning too much. In most cases, the heuristics are a) pause training D if r < 0.1; b) pause training G if r > 10. We did not Ô¨Ånd BatchNorm [20] helpful in further stabilizing the training as found in Radford et al. [43]. We have not experimented with all of the techniques discussed in Salimans et al. [47], some of which could further improve the results.
S9.2. Training PPGN-x
We train a DAE for images and incorporate it to the sampling procedure as a p(x) prior to avoid fooling examples [37]. The DAE is a 4-layer convolutional network that encodes an image to the layer conv1 of AlexNet [26] and decodes it back to images with 3 upconvolutional layers. We add an amount of Gaussian noise ‚àº N (0, œÉ2) with œÉ = 25.6 to images during training. The network is trained via the common training framework described in Sec. S9.1 for 25, 000 mini-batch iterations. We use L2 regularization of 0.0004.
S9.3. Training PPGN-h
For the PPGN-h variant, we train two separate networks: a generator G (that maps codes h to images x) and a prior p(h). G is trained via the same procedure described in Sec. S9.4. We model p(h) via a multi-layer perceptron DAE with 7 hidden layers of size: 4096 ‚àí 2048 ‚àí 1024 ‚àí 500 ‚àí 1024‚àí2048‚àí4096. We experimented with larger networks but found this to work the best. We sweep across different amounts of Gaussian noise N (0, œÉ2), and empirically chose œÉ = 1 (i.e. ‚àº10% of the mean fc6 feature activation). The network is trained via the common training framework described in Sec. S9.1 for 100, 000 mini-batch iterations. We use L2 regularization of 0.001.
S9.4. Training Noiseless Joint PPGN-h
Here we describe the training details of the generator network G used in the main experiments in Sections 3.3, 3.5, 3.4. The training procedure follows closely the framework by Dosovitskiy & Brox [9].
The purpose is to train a generator network G to reconstruct images from an abstract, high-level feature code space of an encoder network E‚Äîhere, the Ô¨Årst fully connected layer (fc6) of an AlexNet DNN [26] pre-trained to

perform image classiÔ¨Åcation on the ImageNet dataset [7] (Fig. S9a) We train G as a decoder for the encoder E, which is kept frozen. In other words, E + G form an image autoencoder (Fig. S9b).
Training losses. G is trained with 3 different losses as in Dosovitskiy & Brox [9], namely, an adversarial loss LGAN , an image reconstruction loss Limg, and a feature matching loss Lh1 measured in the spatial layer pool5 (Fig. S9b):

LG = Limg + Lh1 + LGAN

(18)

Limg and Lh1 are L2 reconstruction losses in their respective space of images x and h1 (pool5) codes :

Limg = ||xÀÜ ‚àí x||2

(19)

Lh1 = ||hÀÜ1 ‚àí h1||2

(20)

The adversarial loss for G (which intuitively maximizes the chance D makes mistakes) follows the original GAN paper [14]:

LGAN = ‚àí log(DœÅ(GŒ∏(hi)))

(21)

i

where xi is a training image, and hi = E(xi) is a code. As in Goodfellow et al. [14], D tries to tell apart real and fake images, and is trained with the adversarial loss as follows:

LD = ‚àí log(DœÅ(xi)) + log(1 ‚àí DœÅ(GŒ∏(hi))) (22)
i
Architecture. G, an upconvolutional (also ‚Äúdeconvolutional‚Äù) network [10] with 9 upconvolutional and 3 fully connected layers. D is a regular convolutional network for image classiÔ¨Åcation with a similar architecture to AlexNet [26] with 5 convolutional layers followed by 3 fully connected layers, and 2 outputs (for ‚Äúreal‚Äù and ‚Äúfake‚Äù classes).
The networks are trained via the common training framework described in Sec. S9.1 for 106 mini-batch iterations. We use L2 regularization of 0.0004.
SpeciÔ¨Åcs of DGN-AM reproduction. Note that while the original set of parameters in Nguyen et al. [37] (including a small number of iterations, an L2 decay on code h, and a step size decay) produces high-quality images, it does not allow for a long sampling chain, traveling from one mode to another. For comparisons with other models within our framework, we sample from DGN-AM with ( 1, 2, 3) = (0, 1, 10‚àí17), which is slightly different from (Œª, 1, 0) in Eq. 10, but produces qualitatively the same result.

16

S9.5. Training Joint PPGN-h
Via the same existing network structures from DGN-AM [37], we train the generator G differently by treating the entire model as being composed of 3 interleaved DAEs: one for h, h1, and x respectively (see Fig. S9c). SpeciÔ¨Åcally, we add Gaussian noise to these variables during training, and by incorporating three corresponding L2 reconstruction losses (see Fig. S9c). Adding noise to an AE can be considered as a form of regularization that encourages an autoencoder to extract more useful features [57]. Thus, here, we hypothesize that adding a small amount of noise to h1 and x might slightly improve the result. In addition, the beneÔ¨Åts of adding noise to h and training the pair G and E as a DAE for h are two fold: 1) it allows us to formally estimate the quantity ‚àÇlogp(h)/‚àÇh (see Eq. 6) following a previous mathematical justiÔ¨Åcation from Alain & Bengio [1]; 2) it allows us to sample with a larger noise level, which might improve the mixing speed.
We add noise to h during training, and train G with a L2 reconstruction loss for h:

Lh = ||hÀÜ ‚àí h||2

(23)

Thus, generator network G is trained with 4 losses in total:

LG = Limg + Lh + Lh1 + LGAN

(24)

Three losses Limg, Lh1 , and LGAN remain the same as in the training of Noiseless Joint PPGN-h (Sec. S9.4). Net-

work architectures and other common training details re-

main the same as described in Sec. S9.4. The amount of Gaussian noise N (0, œÉ2) added to the

3 different variables x, h1, and h are respectively œÉ = {1, 4, 1} which are ‚àº1% of the mean pixel values and

‚àº10% of the mean activations respectively in pool5 and

fc6 space computed from the training set. We experimented

with larger noise levels, but were not able to train the model

successfully as large amounts of noise resulted in training

instability. We also tried training without noise for x, i.e.

treating the model as being composed of 2 DAEs instead of

3, but did not obtain qualitatively better results.

Note that while we did not experiment in this paper,

jointly training both the generator G and the encoder E

via their respective maximum likelihood training algorithms

is possible. Also, Xie et al. [62] has proposed a training

regime that alternatively updates these two networks. That

cooperative training scheme indeeds yields a generator that

synthesizes impressive results for multiple image datasets

[62].

S10. Inpainting

We Ô¨Årst randomly mask out a 100 √ó 100 patch of a real 227 √ó 227 image xreal (Fig. 7a). The patch size is chosen

following Pathak et al. [42]. We perform the same update rule as in Eq. 11 (conditioning on a class, e.g. ‚Äúvolcano‚Äù), but with an additional step updating image x during the forward pass:

x = M x + (1 ‚àí M ) xreal

(25)

where M is the binary mask for the corrupted patch, (1 ‚àí M ) xreal is the uncorrupted area of the real image, and denotes the Hadamard (elementwise) product. Intuitively, we clamp the observed parts of the synthesized image and then sample only the unobserved portion in each pass. The DAE p(h) model and the image classiÔ¨Åcation network p(y|h) model see progressively reÔ¨Åned versions of the Ô¨Ånal, Ô¨Ålled in image. This approach tends to Ô¨Åll in semantically correct content, but it often fails to match the local details of the surrounding context (Fig. 7b, the predicted pixels often do not transition smoothly to the surrounding context). An explanation is that we are sampling in the fully-connected fc6 feature space, which mostly encodes information of the global structure of objects instead of local details [64].
To encourage the synthesized image to match the context of the real image, we can add an extra condition in pixel space in the form of an additional term to the update rule in Eq. 5 to update h in the direction of minimizing the cost: ||(1 ‚àí M ) xreal ‚àí (1 ‚àí M ) x||22. This helps the Ô¨Ålled-in pixels match the surrounding context better (Fig. 7 b vs. c). Compared to the Context-Aware Fill feature in Photoshop CS6, which is based on the PatchMatch technique [3], our method often performs worse in matching the local features of the surrounding context, but can Ô¨Åll in semantic objects better in many cases (Fig. 7, bird & bell pepper). More inpainting results are provided in the Fig. S24.

S11. PPGN-x: DAE model of p(x)
We investigate the effectiveness of using a DAE to model p(x) directly (Fig. 3a). This DAE is a 4-layer convolutional network trained on unlabeled images from ImageNet. We sweep across different noise amounts for training the DAE and empirically Ô¨Ånd that a noise level of 20% of the pixel value range, corresponding to 3 = 25.6, produces the best results. Full training and architecture details are provided in Sec. S9.2.
We sample from this chain following Eq. 7 with ( 1, 2, 3) = (1, 105, 25.6)5 and show samples in Figs. S13a & S14a. PPGN-x exhibits two expected problems: Ô¨Årst, it models the data distribution poorly, evidenced by the images becoming blurry over time. Second, the chain mixes slowly, changing only slightly in hundreds of steps.
5 The 1 and 3 correspond to the noise level used while training the DAE, and the 2 value is chosen manually to produce the best samples.

17

Pre-¬≠‚Äêtrained	 ¬†convnet for	 ¬†image	 ¬†classification

Ì†µÌ±• E1 ‚Ñé$ E2

‚Ñé

image

pool5

fc6

(a)	 ¬†Encoder	 ¬†network	 ¬†E

1000 labels

Denoising	 ¬†auto-¬≠‚Äêencoder for	 ¬†‚Ñé

Auto-¬≠‚Äêencoder for	 ¬†‚Ñé$

Ì†µÌ±• E1 ‚Ñé$ E2 Auto-¬≠‚Äêencoder for Ì†µÌ±•

‚ÑéG GAN for Ì†µÌ±•

‚Ñé%$ Ì†µÌ∞ø-. ‚Ñé$

E1

BE11

Ì†µÌ∞ø'()

Ì†µ"Ì±•

Ì†µÌ±•

D

D

Denoising	 ¬†auto-¬≠‚Äêencoder for	 ¬†‚Ñé$

Ì†µÌ±• +	 ¬†noise E1 ‚Ñé$+	 ¬†noise E2 Denoising	 ¬†auto-¬≠‚Äêencoder for Ì†µÌ±•

‚Ñé +	 ¬†noise G GAN for Ì†µÌ±•

Ì†µÌ∞ø-

‚Ñé/

‚Ñé

E2

EB12

‚Ñé%$ Ì†µÌ∞ø-. ‚Ñé$

E1

EB11

Ì†µÌ∞ø'()

Ì†µ"Ì±•

Ì†µÌ±•

D

D

‚Äúreal‚Äù

‚Äúfake‚Äù

Ì†µÌ∞ø*+,

(b)	 ¬†Noiseless	 ¬†joint	 ¬†PPGN-¬≠‚Äêh

‚Äúreal‚Äù

‚Äúreal‚Äù

‚Äúfake‚Äù

Ì†µÌ∞ø*+,

(c)	 ¬†Joint	 ¬†PPGN-¬≠‚Äêh

‚Äúreal‚Äù

Figure S9: In this paper, we propose a class of models called PPGNs that are composed of 1) a generator network G that is trained to draw a wide range of image types, and 2) a replaceable ‚Äúcondition‚Äù network C that tells G what to draw (Fig. 3). Panel (b) and (c) show the components involved in the training of the generator network G for two main PPGN variants experimented in this paper. Only shaded components (G and D) are being trained while others are kept frozen. b) For the Noiseless Joint PPGN-h variant (Sec. 3.5), we train a generator G to reconstruct images x from compressed features h produced by a pre-trained encoder network E. SpeciÔ¨Åcally, h and h1 are, respectively, features extracted at layer fc6 and pool5 of AlexNet [26] trained to classify ImageNet images (a). G is trained with 3 losses: an image reconstruction loss Limg, a feature matching loss [9] Lh1 and an adversarial loss [14] LGAN . As in Goodfellow et al. [14], D is trained to tell apart real and fake images. This PPGN variant produces the best image quality and thus used for the main experiments in this paper (Sec. 4). After G is trained, we sample from this model following an iterative sampling procedure described in Sec. 3.5. c) For the Joint PPGN-h variant (Sec. 3.4), we train the entire model as being composed of 3 interleaved DAEs respectively for x, h1 and h. In other words, we add noise to each of these variables and train the corresponding AE with a L2 reconstruction loss. The loss for D remains the same as in (a), while the loss for G is now composed of 4 components: L = Limg + Lh1 + Lh + LGAN . The sampling procedure for this PPGN variant is provided in Sec. 3.4. See Sec. S9 for more training and architecture details of the two PPGN variants.

Note that, instead of training the above DAE, one can also form an x-DAE by combining a pair of separately trained encoder E and a generator G into a composition E(G(.)). We also experiment with this model and call it Joint PPGN-x. The details of network E and G and how they can be combined are described in Sec. 3.4 (Joint PPGN-h). For sampling, we sample in the image space, similarly to the PPGN-x in this section. We found that Joint PPGN-x model performs better than PPGN-x, but worse than Joint PPGN-h (data not shown).

S12. Why PPGNs produce high-quality images
One practical question is why Joint PPGN-h produces high-quality images at a high resolution for 1000-class ImageNet more successfully than other existing latent variable models [41, 47, 43]. We can consider this question from two perspectives.
First, from the perspective of the training loss, G is trained with the combination of three losses (Fig. S9b), which may be a beneÔ¨Åcial approach to model p(x). The GAN [14] loss, which is the gradient of log(1 ‚àí D(x)), that is used to train G pushes each reconstruction G(h) toward a mode of real images pdata(x) and away from the current reconstruction distribution. This can be seen by noting

18

that the Bayes optimal D is pdata(x)/(pdata(x) + pmodel(x)) [14]. Since x ‚àº G(h) is already near a mode of pmodel(x), the net effect is to push G(h) towards one of the modes of pdata, thus making the reconstructions sharper and more plausible. If one uses only the GAN objective and no reconstruction objectives (L2 losses in the pixel or feature space), G may bring the sample far from the original x, possibly collapsing several modes of x into fewer modes. This is the typical, known ‚Äúmissing-mode‚Äù behavior of GANs [47, 14] that arises in part because GANs minimize the Jensen-Shannon divergence rather than Kullback-Leibler divergence between pdata and pmodel, leading to an overmemorization of modes [53]. The reconstruction losses are important to combat this missing mode problem and may also serve to enable better convergence of the feature space autoencoder to the distribution it models, which is necessary in order to make the h-space reconstruction properly estimate ‚àÇ log p(h)/‚àÇh [1].
Second, from the perspective of the learned h ‚Üí x mapping, we train the G parameters of the E + G pair of networks as an x-AE, mapping x ‚Üí h ‚Üí x (see Fig. S9b). In this conÔ¨Åguration, as in VAEs [24] and regular DAEs [57], the one-to-one mapping helps prevent the typical latent ‚Üí input missing mode collapse that occurs in GANs, where some input images are not representable using any code [14, 47]. However, unlike in VAEs and DAEs, where the latent distribution is learned in a purely unsupervised manner, we leverage the labeled ImageNet data to train E in a supervised manner that yields a distribution of features h that we hypothesize to be semantically meaningful and useful for building a generative image model. To further understand the effectiveness of using deep, supervised features, it might be interesting future work to train PPGNs with other feature distributions h such as random features or shallow features (e.g. produced by PCA).
19

Model Real ImageNet images AC-GAN [41] PPGN PPGN samples resized to 128 √ó 128

Image size 256 √ó 256 128 √ó 128 256 √ó 256 128 √ó 128

Inception accuracy 76.1% 10.1% 59.6% 54.8%

Inception score 210.4 ¬± 4.6 N/A 60.6 ¬± 1.6 47.7 ¬± 1.0

MS-SSIM 0.10 ¬± 0.06
N/A 0.23 ¬± 0.11 0.25 ¬± 0.11

Percent of classes 999 / 1000 847 / 1000 829 / 1000 770 / 1000

Table S3: A comparison between real ImageNet validation set images, AC-GAN [41] samples, PPGN samples and their resized 128√ó128 versions. Following the literature, we report Inception scores [47] (higher is better) and Inception accuracies [41] (higher is better) to evaluate sample quality, and MS-SSIM score [41] (lower is better), which measures sample diversity within each class. As in Odena et al. [41], the last column (‚ÄúPercent of classes‚Äù, higher is better) shows the number of classes that are more diverse (by MS-SSIM metric) than the least diverse class in ImageNet. Overall, PPGN samples are of substantially higher quality quality than AC-GAN samples (by Inception accuracy, i.e. PPGN samples are far more recognizable by the Google Inception network [50] than AC-GAN samples). Their diversity scores are similar (last column, 847/1000 vs. 829/1000). However, by all 4 metrics, PPGN samples have substantially lower diversity and quality than real images. This result aligns with our qualitative observations in Figs. S25 & S10. Row 2: Note that we chose to compare with AC-GAN [41] because, this model is also class-conditional and, to the best of our knowledge, it produces the previous highest resolution ImageNet images (128 √ó 128) in the literature. Row 3: For comparison with ImageNet 256 √ó 256 images, the spatial dimension of the samples from the generator G is 256 √ó 256 and we did not crop it to 227 √ó 227 as done in other experiments in the paper. Row 4: Although imperfect, we resized PPGN 256√ó256 samples down to 128√ó128 (last row) for comparison with AC-GAN.

20

(a) Real: top 9

(b) DGN-AM [37]

(c) Real: random 9

(d) PPGN (this)

Figure S10: (a) The 9 training set images that most highly activate a given class output neuron (e.g. Ô¨Åre engine). (b) DGNAM [37] synthesizes high-quality images, but they often converge to the mode of high-activating images (the top-9 mode). (c) 9 training set images randomly picked from the same class. (d) Our new method PPGN produces samples with better quality and substantially larger diversity than DGN-AM, thus better representing the diversity of images from the class.
21

(a) Samples produced by PPGN visualized in a grid t-SNE [56] .
(b) Samples hand-picked from (a) to showcase the diversity and quality of images produced by PPGN.
Figure S11: We qualitatively evaluate sample diversity by running 10 sampling chains (conditioned on class ‚Äúvolcano‚Äù), each for 200 steps, to produce 2000 samples, and Ô¨Åltering out samples with class probability of less than 0.97. From the remaining, we randomly pick 400 samples and plot them in a grid t-SNE [56] (top panel). From those, we chose a selection to highlight the quality and diversity of the samples (bottom panel). There is a tremendous amount of detail in each image and diversity across images. Samples include dormant volcanos and active2v2olcanoes with smoke plumes of different colors from white to black to Ô¨Åery orange. Some have two peaks and others one, and underneath are scree, green forests, or glaciers (complete with crevasses). The sky changes from different shades of mid-day blue through different sunsets to pitch black night.

(a) Samples produced by PPGN visualized in a grid t-SNE [56] .
(b) Samples hand-picked from (a) to showcase the diversity and quality of images produced by PPGN. Figure S12: The Ô¨Ågures are selected and plotted in the same way as Fig. S11, but here for the ‚Äúpool table‚Äù class. Once again, we observe a high degree of both image quality and diversity. Different felt colors (green, blue, and red), lighting conditions, camera angles, and interior designs are apparent.
23

(a) PPGN-x with a DAE model of p(x)
(b) DGN-AM [37] (which has a hand-designed Gaussian p(h) prior)
(c) PPGN-h: Generator and multi-layer perceptron DAE model of p(h)
(d) Joint PPGN-h: joint Generator and DAE
(e) Noiseless Joint PPGN-h: joint Generator and AE Figure S13: A comparison of samples generated from a single sampling chain (starting from a real image on the left) across different models. Each panel shows two sampling chains for that model: one conditioned on the ‚Äúplanetarium‚Äù class and the other conditioned on the ‚Äúkite‚Äù (a type of bird) class. The iteration number of the sampling chain is shown on top. (a) The sampling chain in the image space mixes poorly. (b) The sampling chain from DGN-AM [37] (in the h code space with a hand-designed Gaussian p(h) prior) produces better images, but still mixes poorly, as evidenced by similar samples over many iterations. (c) To improve sampling, we tried swapping in a p(h) model represented by a 7-layer DAE for h. However, the sampling chain does not mix faster or produce better samples. (d) We experimented with a better way to model p(h), i.e. modeling h via x. We treat the generator G and encoder E as an autoencoder for h and call this treatment ‚ÄúNoiseless Joint PPGN-h‚Äù (see Sec. 3.5). This is also our best model that we use for experiments in Sec. 4. This substantially improves the mixing speed and sample quality. (e) We train the entire mod2e4l as being composed of 3 DAEs and sample from it by adding noise to the image, fc6 and pool5 variables. The chain mixes slightly faster compared to (d), but generates slightly worse samples.

(a) PPGN-x with a DAE model of p(x)
(b) DGN-AM [37] (which has a hand-designed Gaussian p(h) prior)
(c) PPGN-h: Generator and a multi-layer perceptron DAE model of p(h)
(d) Joint PPGN-h: joint Generator and DAE
(e) Noiseless Joint PPGN-h: joint Generator and AE Figure S14: Same as Fig. S13, but starting from a random code h (which when pushed through generator network G produces the leftmost images) except for (a) which starts from random images as the sampling operates directly in the pixel space. All of our qualitative conclusions are the same as for Fig. S13. Note that the samples in (b) appear slightly worse than the images reported in Nguyen et al. [37]. The reason is that in the new framework introduced in this paper we perform an inÔ¨Ånitely long sampling chain at a constant learning rate to travel from one mode to another in the space. In contrast, the set of parameters (including the number of iterations, an L2 decay on code h, and a learning rate decay) in Nguyen et al. [37] is carefully tuned for the best image quality, but does not allow for a long sampling chain (Fig. 2).
25

(a) Very large noise ( 3 = 10‚àí1)
(b) Large noise ( 3 = 10‚àí5)
(c) Medium noise ( 3 = 10‚àí9)
(d) Small noise ( 3 = 10‚àí13)
(e) InÔ¨Ånitesimal noise ( 3 = 10‚àí17) Figure S15: Sampling chains with the noiseless PPGN model starting from the code of a real image (left) and conditioning on class ‚Äúplanetarium‚Äù i.e. ( 1, 2) = (1, 10‚àí5) for different noise levels 3. The sampling step numbers are shown on top. Samples are better with a tiny amount of noise (e) than with larger noise levels (a,b,c & d), so we chose that as our default noise level for all sampling experiments with the Noiseless Joint PPGN-h variant (Sec. 3.5). These results suggest that a certain amount of noise added to the DAE during training might help the chain mix faster, and thus partly motivated our experiment in Sec. 3.4.
26

Figure S16: The default generator network G in our experiments (used in Sections 3.3 & 3.5) was trained to reconstruct images from compressed fc6 features extracted from AlexNet classiÔ¨Åcation network [26] with three different losses: adversarial loss [14], feature matching loss [9], and image reconstruction loss (more training details are in Sec. S9.4). Here, we test how robust G is to Gaussian noise added to an input code h of a real image. We sweep across different levels of Gaussian noise N (0, œÉ2) with œÉ = {1%, 10%, 20%, 30%, 40%} of the mean fc6 activation computed by the activations of validation set images. We observed that G is robust to even a large amount of noise up to œÉ = 20% despite being trained without explicit regularizations (i.e. with noise [57] or a contractive penalty [44]).
27

(a) Samples produced by the DGN-AM method [37]
(b) Samples produced by PPGN (the new model proposed in this paper) Figure S17: A comparison of images produced by the DGN-AM method [37] (top) and the new PPGN method we introduce in this paper (bottom). Both methods synthesize images conditioned on classes of scene images that the generator was never trained on. SpeciÔ¨Åcally, the condition model p(y|x) is AlexNet trained to classify 205 categories of scene images from the MIT Places dataset [65], while the prior model p(x) is trained to generate ImageNet images. Despite having a strong, learned prior (represented by a DAE trained on ImageNet images), the PPGN (like DGN-AM) produces high-quality images for an unseen dataset.
28

Figure S18: The model can be given a text description of an image and asked to generate the described image. Technically, that involves the same PPGN model, but conditioning on a caption instead of a class. Here the condition network is the LRCN image captioning model from Donahue et al. [8], which can generate reasonable captions for images. For each caption, we show 4 images synthesized by starting from random initializations. Note that it reasonably draws ‚Äútarmac‚Äù, ‚Äúsilhouette‚Äù or ‚Äúwoman‚Äù although these are not categories in the ImageNet dataset [7].
29

Figure S19: PPGNs have the ability to perform ‚Äòmultifaceted feature visualization,‚Äô meaning they can generate the set of inputs that activate a given hidden neuron, which improves our ability to understand what type of features that neuron has learned to detect [40, 64]. To demonstrate that capability, instead of conditioning on a class from the dataset, here we generate images conditioned on a hidden neuron previously identiÔ¨Åed as detecting text [64]: neuron number 243 in layer conv5 of the AlexNet DNN. We run 10 sampling chains, each for 200 steps, to produce 2000 samples, and Ô¨Åltering out samples with a softmax probability (taken over all depth columns at the same spatial location) of less than 0.97. From the remaining, we randomly pick 400 samples and plot them in a grid t-SNE [56]. These images can be interpreted as the preferred stimuli for this text detector unit [37]. The diversity of samples is substantially improved vs. previous methods [40, 37, 64] uncovering different facets that the neuron has learned to detect. In other words, while previous methods produced one type of sample per neuron [64, 37], or lower quality samples with greater diversity [40], PPGNs produce a diversity of high-quality samples, and thus represent the state of the art in multifaceted feature visualization.
30

Figure S20: This Ô¨Ågure shows the same thing as Fig. S19, except in this case for a hidden neuron previously identiÔ¨Åed to be a face detector [64] neuron (number 196) in layer conv5 of the AlexNet DNN. One can see different types of faces that the neuron has learned to detect, including everything from dog faces (top row) to masks (left columns), and human faces from different angles, against different backgrounds, with and without hats, and with different shirt colors. Interestingly, we see that certain types of houses with windows that resemble eye sockets also activate this neuron (center left). This demonstrates the value of PPGNs as tools to identify unexpected facets, which aids in understanding the network, predicting some failure cases, and providing hints for how the network may be improved.
31

(a) Snail
(b) Studio couch
(c) Harvester
(d) Pomegranate
(e) Tape player Figure S21: To evaluate the diversity of images within a class, here we show randomly chosen images from 5 different classes (a class label shown below each panel). Each image is the last sample produced from a 200-iteration sampling chain starting from a random initialization. The PPGN model is described in Sec. 3.5. We chose this method because it is simple, intuitive and straightforward to interpret and compare to other image generative models that do not require MCMC sampling. Another method to produce samples is to run a long sampling chain and record images that are produced at every sampling step; however, as done in Fig. S11, that method would require additional processing (including heuristic Ô¨Åltering and clustering) to obtain a set of different images because a well-known issue with MCMC sampling is that mixing is slow i.e. subsequent samples are often correlated. Note that one could obtain a larger diversity by running each sampling chain with a different set of parameters ( multipliers in Eq. 5); however, here we use the same set of parameters as previously reported in Sec. 3.5 for simplicity and reproducibility.
32

Figure S22: For a fair evaluation of the image quality produced by PPGN, here we show one randomly chosen sample for each of 120 random ImageNet classes (neither cherry-picked). Each image shown is the last sample produced from a 200-iteration sampling chain starting from a random initialization. The PPGN model is described in Sec. 3.5.
33

(a) 1 = 10‚àí1
(b) 1 = 10‚àí3
(c) 1 = 10‚àí5 (Noiseless Joint PPGN-h)
(d) 1 = 10‚àí7
(e) 1 = 10‚àí11
(f) 1 = 0 (no contribution from the prior) Figure S23: To evaluate the effect of the prior term 1 in the sampling, here we sweep across different values of this multiplier. We sample from the Noiseless Joint PPGN-h model (Sec. 3.5) starting from the code of a real image (left) and conditioning on class ‚Äúplanetarium‚Äù with a Ô¨Åxed amount of noise i.e. ( 2, 3) = (1, 10‚àí17) for different 1 values. The sampling step numbers are shown on top. Without the learned prior p(h) (f), we arrive at the DGN-AM treatment results where the chain does not mix at all (the same result as in Fig. S13b). Increasing 1 up to a small value (c-e) results in a chain that mixes faster, from one planetarium to another. When the contribution of the prior is too high which overwrites the class gradients, yielding a chain that mixes from one mode of generic images to another (a). We empirically chose 1 = 10‚àí5 as the default value for the Noiseless Joint PPGN-h experiments in this paper as it produces the best image quality and diversity for many classes.
34

Figure S24: To test the ability of PPGNs to perform ‚Äúinpainting‚Äù, we randomly mask out a 100 √ó 100 patch in a real image, and perform class-conditional image sampling via PPGN-context (described in Sec. 4.2) to Ô¨Åll in missing pixels. In addition to conditioning on a speciÔ¨Åc class (here, ‚Äúvolcano‚Äù, ‚Äújunco‚Äù and ‚Äúbell pepper‚Äù respectively), we put an additional constraint on the code h that it has to produce an image that matches the context region. PPGN-context performs semantically well in many cases. However, sometimes it does not match the local features of the surrounding regions. The result shows that the class-conditional image model does understand the semantics of images.
35

(a) 60 training set images randomly taken from the ‚Äúvolcano‚Äù class. .

(b) 60 PPGN samples randomly selected from 2000 samples, which are produced from 10 200-step sampling chains.

Figure S25: To evaluate how well PPGN samples represent the training set images, we compare 60 real images (top) vs. 60

PPGN generated images (bottom). All images are randomly selected. While the set of generated images are high-quality,

they have a much lower diversity compared to the set of real images. This observation aligns with our quantitative measure

in Table S3.

36

