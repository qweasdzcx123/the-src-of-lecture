75 Languages, 1 Model: Parsing Universal Dependencies Universally
Dan Kondratyuk1,2 and Milan Straka1 1Charles University, Institute of Formal and Applied Linguistics 2Saarland University, Department of Computational Linguistics
dankondratyuk@gmail.com, straka@ufal.mff.cuni.cz

arXiv:1904.02099v3 [cs.CL] 25 Aug 2019

Abstract
We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that ﬁne-tuning it on all datasets concatenated together with simple softmax classiﬁers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-speciﬁc components. We evaluate UDify for multilingual learning, showing that low-resource languages beneﬁt the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https:// github.com/hyperparticle/udify.
1 Introduction
In the absence of annotated data for a given language, it can be considerably difﬁcult to create models that can parse the language’s text accurately. Multilingual modeling presents an attractive way to circumvent this low-resource limitation. In a similar way learning a new language can enhance the proﬁciency of a speaker’s previous languages (Abu-Rabia and Sanitsky, 2010), a model which has access to multilingual information can begin to learn generalizations across languages that would not have been possible through monolingual data alone. Works such as McDonald et al. (2011); Naseem et al. (2012); Duong et al. (2015); Ammar et al. (2016); de Lhoneux et al. (2018); Kitaev and Klein (2018); Mulcaire et al. (2019) consistently demonstrate how pairing the

nsubj 4 (is)

Dependency Tag Dependency Head

x→x (optimizer) Number=Sing NOUN Layer Attention

Lemma UFeats UPOS

...
BERT

The best op ##timi ##zer is grad student descent
“The best optimizer is grad student descent”
Figure 1: An illustration of the UDify network architecture with task-speciﬁc layer attention, inputting word tokens and outputting UD annotations for each token.
training data of similar languages can boost evaluation scores of models predicting syntactic information like part-of-speech and dependency trees. Multilinguality not only can improve a model’s evaluation performance, but can also reduce the cost of training multiple models for a collection of languages (Johnson et al., 2017; Smith et al., 2018).
However, scaling to a higher number of languages can often be problematic. Without an ample supply of training data for the considered languages, it can be difﬁcult to form appropriate generalizations and especially difﬁcult if those

languages are distant from each other. But recent techniques in language model pretraining can proﬁt from a drastically larger supply of unsupervised text, demonstrating the capability of transferring contextual sentence-level knowledge to boost the parsing accuracy of existing NLP models (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018).
One such model, BERT (Devlin et al., 2018), introduces a self-attention (Transformer) network that results in state-of-the-art parsing performance when ﬁne-tuning its contextual embeddings. And with the release of a multilingual version pretrained on the entirety of the top 104 resourced languages of Wikipedia, BERT is remarkably capable of capturing an enormous collection of cross-lingual syntactic information. Conveniently, these languages nearly completely overlap with languages supported by the Universal Dependencies treebanks, which we will use to demonstrate the ability to scale syntactic parsing up to 75 languages and beyond.
The Universal Dependencies (UD) framework provides syntactic annotations consistent across a large collection of languages (Nivre et al., 2018; Zeman et al., 2018). This makes it an excellent candidate for analyzing syntactic knowledge transfer across multiple languages. UD offers tokenized sentences with annotations ideal for multi-task learning, including lemmas (LEMMAS), treebank-speciﬁc part-of-speech tags (XPOS), universal part-of-speech tags (UPOS), morphological features (UFEATS), and dependency edges and labels (DEPS) for each sentence.
We propose UDify, a semi-supervised multitask self-attention model automatically producing UD annotations in any of the supported UD languages. To accomplish this, we perform the following:
1. We input all sentences into a pretrained multilingual BERT network to produce contextual embeddings, introduce task-speciﬁc layer-wise attention similar to ELMo (Peters et al., 2018), and decode each UD task simultaneously using softmax classiﬁers.
2. We apply a heavy amount of regularization to BERT, including input masking, increased dropout, weight freezing, discriminative ﬁnetuning, and layer dropout.
3. We train and ﬁne-tune the model on the en-

tirety of UD by concatenating all available training sets together.
We evaluate our model with respect to UDPipe Future, one of the winners of the CoNLL 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies (Straka, 2018; Zeman et al., 2018). In addition, we analyze languages that multilingual training beneﬁts prediction the most, and evaluate the model for zeroshot learning, i.e., treebanks which do not have a training set. Finally, we provide evidence from our experiments and other related work to help explain why pretrained self-attention networks excel in multilingual dependency parsing.
Our work uses the AllenNLP library built for the PyTorch framework. Code for UDify and a release of the ﬁne-tuned BERT weights are available at https://github. com/hyperparticle/udify.
2 Multilingual Multi-Task Learning
In this section, we detail the multilingual training setup and the UDify multi-task model architecture. See Figure 1 for an architecture diagram.
2.1 Multilingual Pretraining with BERT
We leverage the provided BERT base multilingual cased pretrained model1, with a self-attention network of 12 layers, 12 attention heads per layer, and hidden dimensions of 768 (Devlin et al., 2018). The model was trained by predicting randomly masked input words on the entirety of the top 104 languages with the largest Wikipedias. BERT uses a wordpiece tokenizer (Wu et al., 2016), which segments all text into (unnormalized) sub-word units.
2.2 Cross-Linguistic Training Issues
Table 1 displays a list of vocabulary sizes, indicating that UD treebanks possess nearly 1.6M unique tokens combined. To sidestep the problem of a ballooning vocabulary, we use BERT’s wordpiece tokenizer directly for all inputs. UD expects predictions to be along word boundaries, so we take the simple approach of applying the tokenizer to each word using UD’s provided segmentation. For prediction, we use the outputs of BERT corresponding to the ﬁrst wordpiece per word, ignoring
1https://github.com/google-research/ bert/blob/master/multilingual.md

TOKEN
Word Form BERT Wordpieces UPOS XPOS UFeats Lemmas (tags) Deps

VOCAB SIZE
1,588,655 119,547 17 19,126 23,974 109,639 251

Table 1: Vocabulary sizes of words and tags over all of UD v2.3, with a total of 12,032,309 word tokens and 668,939 sentences.

the rest2. In addition, the XPOS annotations are not uni-
versal across languages, or even across treebanks. Because each treebank can possess a different annotation scheme for XPOS which can slow down inference, we omit training and evaluation of XPOS from our experiments.
2.3 Multi-Task Learning with UD
For predicting UD annotations, we employ a multi-task network based on UDPipe Future (Straka, 2018), but with all embedding, encoder, and projection layers replaced with BERT. The remaining components include the prediction layers for each task detailed below, and layer attention (see Section 3.1). Then we compute softmax cross entropy loss on the output logits to train the network. For more details on reasons behind architecture choices, see Appendix A.
UPOS As is standard for neural sequence tagging, we apply a softmax layer along each word input, computing a probability distribution over the tag vocabulary to predict the annotation string.
UFeats Identical to UPOS prediction, we treat each UFeats string as a separate token in the vocabulary. We found this to produce higher evaluation accuracy than predicting each morphological feature separately. Only a small subset of the full Cartesian product of morphological features is valid, eliminating invalid combinations.
Lemmas Similar to Chrupała (2006); Mu¨ller et al. (2015), we reduce the problem of lemmatization to a sequence tagging problem by predicting a class representing an edit script, i.e., the sequence of character operations to transform the word form to the lemma. To precompute the tags, we ﬁrst ﬁnd
2We found last, max, or average pooling of the wordpieces were not any better or worse for evaluation. Kitaev and Klein (2018) report similar results.

the longest common substring between the form and the lemma, and then compute the shortest edit script converting the preﬁx and sufﬁx of the form into the preﬁx and sufﬁx of the lemma using the Wagner–Fischer algorithm (Wagner and Fischer, 1974). Upon predicting a lemma edit script, we apply the edit operations to the word form to produce the ﬁnal lemma. See also Straka (2018) for more details. We chose this approach over a sequence-to-sequence architecture like Bergmanis and Goldwater (2018) or Kondratyuk et al. (2018), as this signiﬁcantly reduces training efﬁciency.
Deps We use the graph-based biafﬁne attention parser developed by Dozat and Manning (2016); Dozat et al. (2017), replacing the bidirectional LSTM layers with BERT. The ﬁnal embeddings are projected through arc-head and arc-dep feedforward layers, which are combined using biafﬁne attention to produce a probability distribution of arc heads for each word. We then decode each tree with the Chu-Liu/Edmonds algorithm (Chu, 1965; Edmonds, 1967).
3 Fine-Tuning BERT on UD Annotations
We employ several strategies for ﬁne-tuning BERT for UD prediction, ﬁnding that regularization is absolutely crucial for producing a highscoring network.
3.1 Layer Attention
Empirical results suggest that when ﬁne-tuning BERT, combining the output of the last several layers is more beneﬁcial for the downstream tasks than just using the last layer (Devlin et al., 2018). Instead of restricting the model to any subset of layers, we devise a simple layer-wise dot-product attention where the network computes a weighted sum of all intermediate outputs of the 12 BERT layers using the same weights for each token. This is similar to how ELMo mixes the output of multiple recurrent layers (Peters et al., 2018).
More formally, let wi be a trainable scalar for BERT embeddings BERTij at layer i with a token at position j, and let c be a trainable scalar. We compute contextual embeddings e(task) such that
e(jtask) = c BERTij · softmax(w)i (1)
i
To prevent the UD classiﬁers from overﬁtting to the information in any single layer, we devise

layer dropout, where at each training step, we set each parameter wi to −∞ with probability 0.1. This effectively redistributes probability mass to all other layers, forcing the network to incorporate the information content of all BERT layers. We compute layer attention per task, using one set of c, w parameters for each of UPOS, UFeats, Lemmas, and Deps.
3.2 Transfer Learning with ULMFiT
The ULMFiT strategy deﬁnes several useful methods for ﬁne-tuning a network on a pretrained language model (Howard and Ruder, 2018). We apply the same methods, with a few minor modiﬁcations.
We split the network into two parameter groups, i.e., the parameters of BERT and all other parameters. We apply discriminative ﬁne-tuning, setting the base learning rate of BERT to be 5e−5 and 1e−3 everywhere else. We also freeze the BERT parameters for the ﬁrst epoch to increase training stability.
While ULMFiT recommends decaying the learning rate linearly after a linear warmup, we found that this is prone to training divergence in self-attention networks, introducing vanishing gradients and underﬁtting. Instead, we apply an inverse square root learning rate decay with linear warmup (Noam) seen in training Transformer networks for machine translation (Vaswani et al., 2017).
3.3 Input Masking
The authors of BERT recommend not to mask words randomly with [MASK] when ﬁne-tuning the network. However, we discovered that masking often reduces the tendency of the classiﬁers to overﬁt to BERT by forcing the network to rely on the context of surrounding words. This word dropout strategy has been observed in other works showing improved test performance on a variety of NLP tasks (Iyyer et al., 2015; Bowman et al., 2016; Clark et al., 2018; Straka, 2018).
4 Experiments
We evaluate UDify with respect to every test set in each treebank. As there are too many results to ﬁt within one page, we display a salient subset of scores and compare them with UDPipe Future. The full results are listed in Appendix A.
We do not directly reference metrics from other

models in the CoNLL 2018 Shared Task, as the tables of results do not assume gold word segmentation and may not provide a fair comparison. Instead, we retrained the open source UDPipe Future model using gold segmentation and report results here due to its architectural similarity to UDify and its strong performance.
Note that the UDPipe Future baseline does not itself use BERT. Evaluation of BERT utilization in UDPipe Future can be found in Straka et al. (2019).
4.1 Datasets
For all experiments, we use the full Universal Dependencies v2.3 corpus available on LINDAT (Nivre et al., 2018). We omit the evaluation of datasets that do not have their training annotations freely available, i.e., Arabic NYUAD (ar nyuad), English ESL (en esl), French FTB (fr ftb), Hindi English HEINCS (qhe heincs), and Japanese BCCWJ (ja bccwj).
To train the multilingual model, we concatenate all available training sets together, similar to McDonald et al. (2011). Before each epoch, we shufﬂe all sentences and feed mixed batches of sentences to the network, where each batch may contain sentences from any language or treebank, for a total of 80 epochs3.
4.2 Hyperparameters
A summary of hyperparameters can be found in Table 6 in Appendix A.1.
4.3 Probing for Syntax
Hewitt and Manning (2019) introduce a structural probe for identifying dependency structures in contextualized word embeddings. This probe evaluates whether syntax trees (i.e., unlabeled undirected dependency trees) can be easily extracted as a global property of the embedding space using a linear transformation of the network’s contextual word embeddings. The probe trains a weighted adjacency matrix on the layers of contextual embeddings produced by BERT, identifying a linear transformation where squared L2 distance between embedding vectors encodes the distance between words in the parse tree. Edges are decoded by computing the minimum spanning tree on the weight matrix (the lowest sum of edge distances).
3 We train on a GTX 1080 Ti for approximately 25 days. See Appendix A.1 for more details

TREEBANK

MODEL

UPOS FEATS LEM UAS LAS

Czech PDT (cs pdt)

UDPipe
Lang UDify UDify+Lang

99.18
99.18 99.18 99.24

97.23 99.02 93.33 91.31
96.87 98.72 94.35 92.41 96.85 98.56 94.73 92.88 97.44 98.93 95.07 93.38

German GSD (de gsd)

UDPipe Lang UDify UDify+Lang

94.48 94.77 94.55 95.29

90.68 96.80 85.53 81.07 91.73 96.34 87.54 83.39 90.65 94.82 87.81 83.59 91.94 96.74 88.11 84.13

English EWT (en ewt )

UDPipe
Lang UDify UDify+Lang

96.29
96.82 96.21 96.57

97.10 98.25 89.63 86.97
97.27 97.97 91.70 89.38 96.17 97.35 90.96 88.50 96.96 97.90 91.55 89.06

Spanish AnCora (es ancora)

UDPipe
Lang UDify UDify+Lang

98.91
98.60 98.53 98.68

98.49 99.17 92.34 90.26
98.14 98.52 92.82 90.52 97.84 98.09 92.99 90.50 98.25 98.68 93.35 91.28

French GSD (fr gsd)

UDPipe
Lang UDify UDify+Lang

97.63
98.05 97.83 97.96

97.13 98.35 90.65 88.06
96.26 97.96 92.77 90.61 96.59 97.48 93.60 91.45 96.73 98.17 93.56 91.45

Russian SynTagRus
(ru syntagrus)

UDPipe
Lang UDify UDify+Lang

99.12
98.90 98.97 99.08

97.57 98.53 93.80 92.32
96.58 95.16 94.40 92.72 96.35 94.43 94.83 93.13 97.22 96.58 95.13 93.70

Belarusian HSE (be hse)

UDPipe
Lang UDify UDify+Lang

93.63
95.88 97.54 97.25

73.30 87.34 78.58 72.72
76.12 84.52 83.94 79.02 89.36 85.46 91.82 87.19 85.02 88.71 90.67 86.98

Buryat BDT (bxr bdt)

UDPipe
Lang UDify UDify+Lang

40.34
52.54 61.73 61.73

32.40 58.17 32.60 18.83
37.03 54.64 29.63 15.82 47.86 61.06 48.43 26.28 42.79 58.20 33.06 18.65

Upper Sorbian
UFAL (hsb ufal)

UDPipe
Lang UDify UDify+Lang

62.93
73.70 84.87 87.58

41.10 68.68 45.58 34.54
46.28 58.02 39.02 28.70 48.63 72.73 71.55 62.82 53.19 71.88 71.40 60.65

Kazakh KTB (kk ktb)

UDPipe
Lang UDify UDify+Lang

55.84
73.52 85.59 81.32

40.40 63.96 53.30 33.38
46.60 57.84 50.38 32.61 65.14 77.40 74.77 63.66 60.50 67.30 69.16 53.14

Lithuanian HSE (lt hse)

UDPipe
Lang UDify UDify+Lang

81.70
83.40 90.47 84.53

60.47 76.89 51.98 42.17
54.34 58.77 51.23 38.96 68.96 67.83 79.06 69.34 56.98 58.21 58.40 39.91

Table 2: Test set scores for a subset of highresource (top) and low-resource (bottom) languages in comparison to UDPipe Future without BERT, with 3 UDify conﬁgurations: Lang, ﬁne-tune on the treebank. UDify, ﬁne-tune on all UD treebanks combined. UDify+Lang, ﬁne-tune on the treebank using BERT weights saved from ﬁne-tuning on all UD treebanks combined.

We train the structural probe on unmodiﬁed and ﬁne-tuned BERT using the default hyperparameters of Hewitt and Manning (2019) to evalu-

MODEL CONFIGURATION UPOS FEATS LEM UAS LAS

UDPipe w/o BERT

UDify UDify UDify

Task Layer Attn Global Layer Attn Sum Layers

93.76
93.40 93.12 93.02

91.04 94.63 84.37 79.76
88.72 90.41 85.69 80.43 87.53 89.03 85.07 79.49 87.20 88.70 84.97 79.33

Table 3: Ablation comparing the average of scores over all treebanks: task-speciﬁc layer attention (4 sets of c, w computed for the 4 UD tasks), global layer attention (one set of c, w for all tasks), and simple sum of layers (c = 1 and w = ).

TREEBANK
Breton KEB Tagalog TRG Faroese OFT Naija NSC Sanskrit UFAL

UPOS FEATS LEM UAS LAS

br keb tl trg fo oft
pcm nsc sa ufal

63.67 61.64 77.86 56.59 40.21

46.75 53.15 63.97 40.19 35.27 75.00 64.73 39.38 35.71 53.82 69.28 61.03 52.75 97.52 47.13 33.43 18.45 37.60 41.73 19.80

Table 4: Test set results for zero-shot learning, i.e., no UD training annotations available. Languages that are pretrained with BERT are bolded.

TREEBANK English EWT (en ewt)

MODEL
BERT BERT+ﬁnetune en ewt

UUAS
65.48 79.67

Table 5: UUAS test scores calculated on the predictions produced by the syntactic structural probe (Hewitt and Manning, 2019) using the English EWT treebank, on the unmodiﬁed multilingual cased BERT model and the same BERT model ﬁne-tuned on the treebank.

ate whether the representations affected by ﬁnetuning BERT on dependency trees would more closely match the structure of these trees.
5 Results
We show scores of UPOS, UFeats (FEATS), and Lemma (LEM) accuracies, along with unlabeled and labeled attachment scores (UAS, LAS) evaluated using the ofﬁcal CoNLL 2018 Shared Task evaluation script.4 Results for a salient subset of high-resource and low-resource languages are shown in Table 2, with a comparison between UDPipe Future and UDify ﬁne-tuning on all languages. In addition, the table compares UDify with ﬁne-tuning on either a single language or both languages (ﬁne-tuning multilingually, then ﬁne-tuning on the language with the saved multilingual weights) to provide a reference point for multilingual inﬂuences on UDify. We provide
4https://universaldependencies.org/ conll18/evaluation.html

1

0

Weight

#1

#2

UPOS

UFeat

#3

Lemma

Dep

#4

1 2 3 4 5 6 7 8 9 10 11 12 BERT Layer #

Figure 2: The unnormalized BERT layer attention

weights wi contributing to layer i for each task after training. A linear change in weight scales each BERT

layer exponentially due to the softmax in Equation 1

a full table of scores for all treebanks in Appendix A.4.
A more comprehensive overview is shown in Table 3, comparing different attention strategies applied to UDify. We display an average of scores over all (89) treebanks with a training set. For zero-shot learning evaluation, Table 4 displays a subset of test set evaluations of treebanks that do not have a training set, i.e., Breton, Tagalog, Faroese, Naija, and Sanskrit. We plot the layer attention weights w after ﬁne-tuning BERT in Figure 2, showing a set of weights per task. And Table 5 compares the unlabeled undirected attachment scores (UUAS) of dependency trees produced using a structural probe on both the unmodiﬁed multilingual cased BERT model and the extracted BERT model ﬁne-tuned on the English EWT treebank.
6 Discussion
In this section, we discuss the most notable features of the results.
6.1 Model Performance
On average, UDify reveals a strong set of results that are comparable in performance with the state-of-the-art in parsing UD annotations. UDify excels in dependency parsing, exceeding UDPipe Future by a large margin especially for lowresource languages. UDify slightly underperforms with respect to Lemmas and Universal Features, likely due to UDPipe Future additionally using character-level embeddings (Santos and Zadrozny, 2014; Ling et al., 2015; Ballesteros et al., 2015; Kim et al., 2016), while (for simplicity) UDify

does not. Additionally, UDify severely underperforms the baseline on a few low-resource languages, e.g., cop scriptorum. We surmise that this is due to using mixed batches on an unbalanced training set, which skews the model towards predicting larger treebanks more accurately. However, we ﬁnd that ﬁne-tuning on the treebank individually with BERT weights saved from UDify eliminates most of these gaps in performance.
Echoing results seen in Smith et al. (2018), UDify also shows strong improvement leveraging multilingual data from other UD treebanks. In low-resource cases, ﬁne-tuning BERT on all treebanks can be far superior to ﬁne-tuning monolingually. A second round of ﬁne-tuning on an individual treebank using UDify’s BERT weights can improve this further, especially for treebanks that underperform the baseline. However, for languages that are already display strong results, we typically notice worse evaluation performance across all the evaluation metrics. This indicates that multilingual ﬁne-tuning really is superior to single language ﬁne-tuning with respect to these high-performing languages, showing improvements of up to 20% reduction in error.
Interestingly, Slavic languages tend to perform the best with multilingual training. While languages like Czech and Russian possess the largest UD treebanks and do not differ as much in performance from monolingual ﬁne-tuning, evidenced by the improvements over single-language ﬁnetuning, we can see a large degree of morphological and syntactic structure has transferred to low-resource Slavic languages like Upper Sorbian, whose treebank contains only 646 sentences. But this is not only true of Slavic languages, as the Turkic language Kazakh (with less than 1,000 training sentences) has also improved signiﬁcantly.
The zero-shot results indicate that ﬁne-tuning on BERT can result in reasonably high scores on languages that do not have a training set. It can be seen that a combination of BERT pretraining and multilingual learning can improve predictions for Breton and Tagalog, which implies that the network has learned representations of syntax that cross lingual boundaries. Furthermore, despite the fact that neither BERT nor UDify have directly observed Faroese, Naija, or Sanskrit, we see unusually high performance in these languages. This can be partially attributed to each language closely resembling another: Faroese is very close to Ice-

.

.

.. .

..

Moving ﬁelds to the category and series areas

.

.

.

.. .

.

.

.

.

..

.

.

.. .

We land and spill out and go our separate ways .

.

.

.

.

.

.

.

.

..

BERT

.

.

. ..

. .

Moving ﬁelds to the category and series areas

.

. ..

..

.

.

.

.

..

.

.

.. .

We land and spill out and go our separate ways .

.

..

.

.

.

.. .

.

UDify

Figure 3: Examples of minimum spanning trees produced by the syntactic probe are shown below each sentence, evaluated on BERT (left) and on UDify (right). Gold dependency trees are shown above each sentence in black. Matched and unmatched spanning tree edges are shown in blue and red respectively.

landic, Naija (Nigerian Pidgin) is a variant of English, and Sanskrit is an ancient Indian language related to Greek, Latin, and Hindi.
Table 3 shows that layer attention on BERT for each task is beneﬁcial for test performance, much more than using a global weighted average. In fact, Figure 2 shows that each task prefers the layers of BERT differently, uniquely extracting the optimal information for a task. All tasks favor the information content in the last 3 layers, with a tendency to disprefer layers closer to the input. However, an interesting observation is that for Lemmas and UFeats, the classiﬁer prefers to also incorporate the information of the ﬁrst 3 layers. This meshes well with the linguistic intuition that morphological features are more closely related to the surface form of a word and rely less on context than other syntactic tasks. Curiously enough, the middle layers are highly dispreferred, meaning that the most useful processing for multilingual syntax (tagging, dependency parsing) occurs in the last 3-4 layers. The results released by Tenney et al. (2019) also agree with the intuition behind the weight distribution above, showing how the different layers of BERT generate hierarchical information like a traditional NLP pipeline, starting with low-level syntax (e.g., POS tagging) and building up to high-level syntactic and semantic dependency parsing.
6.2 Effect of Syntactic Fine-Tuning on BERT
Even without any supervised training, BERT encodes its syntax in the embedding’s distance close to human-annotated dependencies. But more notably, the results in Table 5 show that ﬁne-tuning BERT on Universal Dependencies signiﬁcantly boosts UUAS scores when compared to the gold dependency trees, an error reduction of 41%.

This indicates that the self-attention weights have learned a linearly-transformable representation of its vectors more closely resembling annotated dependency trees deﬁned by linguists. Even with just unsupervised pretraining, a global structural property of the vector space of the BERT weights already produces a decent representation of the dependency tree in the squared L2 distance. Following this, it should be no surprise that training with a non-linear graph-based dependency decoder would produce even higher quality dependency trees.
6.3 Attention Visualization
We performed a high-level visual analysis of the BERT attention weights to see if they have changed on any discernible level. Our observations reveal something notable: the attention weights tend to be more sparse, and are more often sensitive to constituent boundaries like clauses and prepositions. Figure 4 illustrates this point, showing the attention weights of a particular attention head on an example sentence. We ﬁnd similar behavior in 13 additional attention heads for the provided example sentence.
We see that some of the attention structure remains after ﬁne-tuning. Previously, the attention head was mostly sensitive to previous words and punctuation. But after ﬁne-tuning, it demonstrates more ﬁne-grained attention towards immediate wordpieces, prepositions, articles, and adjectives. We found similar evidence in other attention heads, which implies that ﬁne-tuning on UD produces attention that more closely resembles localized dependencies within constituents. We also ﬁnd that BERT base heavily preferred to attend to punctuation, while UDify BERT does to a much lesser degree.

Figure 4: Visualization of BERT attention head 4 at layer 11, comparing the attended words on an English sentence between BERT base and UDify BERT after ﬁne-tuning. The right column indicates the attended words (keys) with respect to the words in the left column (queries). Darker lines indicate stronger attention weights.

6.4 Factors that Enable BERT to Excel at Dependency Parsing and Multilinguality
Goldberg (2019) assesses the syntactic capabilities of BERT and concludes that BERT is remarkably capable of processing syntactic tasks despite not being trained on any supervised data. Conducting similar experiments, Vig (2019) and Sileo (2019) visualize the attention heads within each BERT layer, showing a number of distinct attention patterns, including attending to previous/next words, related words, punctuation, verbs/nouns, and coreference dependencies.
This neat delegation of certain low-level information processing tasks to the attention heads hints at why BERT might excel at processing syntax. We see that from the analysis on BERT ﬁnetuned with syntax using the syntactic probe and attention visualization, BERT produces a representation that keeps constituents close in its vector space, and improves this representation to more closely resemble human annotated dependency trees when ﬁne-tuned on UD as seen in Figure 3. Furthermore, Ahmad et al. (2018) provide results consistent with their claim that self-attention networks can be more robust than recurrent networks to the change of word order, observing that selfattention networks capture less word order information in their architecture, which is what allows them to generally perform better at cross-lingual

parsing. Wu and Dredze (2019) also analyze multilingual BERT and report that the model retains both language-independent as well as languagespeciﬁc information related to each input sentence, and that the shared embedding space with the input wordpieces correlates strongly with crosslingual generalization.
From the evidence above, we can see that the combination of strong regularization paired with the ability to capture long-range dependencies with self-attention and contextual pretraining on an enormous corpus of raw text are large contributors that enable robust multilingual modeling with respect to dependency parsing. Pretraining self-attention networks introduces a strong syntactic bias that is capable of generalizing across languages. The dependencies seen in the output dependency trees are highly correlated with the implicit dependencies learned by the self-attention, showing that self-attention is remarkably capable of modeling syntax by picking up on common syntactic patterns in text. The introduction of multilingual data also shows that these attention heads provide a surprising amount of capacity that do not degrade the performance considerably when compared to monolingual training. E.g., Devlin et al. (2018) report that the ﬁne-tuning on the multilingual BERT model results in a small degradation in English ﬁne-tune performance with 104

pretrained languages compared to an equivalent model pretrained only on English. This also hints that the BERT model can be compressed significantly without compromising heavily on evaluation performance.
7 Related Work
This work’s main contribution in combining treebanks for multilingual UD parsing is most similar to the Uppsala system for the CoNLL 2018 Shared Task (Smith et al., 2018). Uppsala combines treebanks of one language or closely related languages together over 82 treebanks and parses all UD annotations in a multi-task pipeline architecture for a total of 34 models. This approach reduces the number of models required to parse each language while also showing results that are no worse than training on each treebank individually, and in especially low-resource cases, significantly improved. Combining UD treebanks in a language-agnostic way was ﬁrst introduced in Vilares et al. (2016), which train bilingual parsers on pairs of UD treebanks, showing similar improvements.
Other efforts in training multilingual models include Johnson et al. (2017), which demonstrate a machine translation model capable of supporting translation between 12 languages. Recurrent models have also shown to be capable of scaling to a larger number of languages as seen in Artetxe and Schwenk (2018), which deﬁne a scalable approach to train massively multilingual embeddings using recurrent networks on an auxiliary task, e.g., natural language inference. Schuster et al. (2019) produce context-independent multilingual embeddings using a novel embedding alignment strategy to allow models to improve the use of crosslingual information, showing improved results in dependency parsing.
8 Conclusion
We have proposed and evaluated UDify, a multilingual multi-task self-attention network ﬁnetuned on BERT pretrained embeddings, capable of producing annotations for any UD treebank, and exceeding the state-of-the-art in UD dependency parsing in a large subset of languages while being comparable in tagging and lemmatization accuracy. Strong regularization and task-speciﬁc layer attention are highly beneﬁcial for ﬁne-tuning, and coupled with training multilingually, also reduce

the number of required models to train down to one. Multilingual learning is most beneﬁcial for low-resource languages, even ones that do not possess a training set, and can be further improved by ﬁne-tuning monolingually using BERT weights saved from UDify’s multilingual training. All these results indicate that self-attention networks are remarkably capable of capturing syntactic patterns, and coupled with unsupervised pretraining are able to scale to a large number of languages without degrading performance.
Acknowledgments
The work described herein has been supported by OP VVV VI LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project CZ.02.1.01/0.0/0.0/16 013/0001781) and it has been supported and has been using language resources developed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071).
Dan Kondratyuk has been supported by the Erasmus Mundus program in Language & Communication Technologies (LCT), and by the German Federal Ministry of Education and Research (BMBF) through the project DEEPLEE (01IW17001).
References
Salim Abu-Rabia and Ekaterina Sanitsky. 2010. Advantages of bilinguals over monolinguals in learning a third language. Bilingual Research Journal, 33(2):173–199.
Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei Chang, and Nanyun Peng. 2018. On difﬁculties of cross-lingual transfer with order differences: A case study on dependency parsing. arXiv preprint arXiv:1811.00570.
Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A Smith. 2016. Many languages, one parser. Transactions of the Association for Computational Linguistics, 4:431–444.
Mikel Artetxe and Holger Schwenk. 2018. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. arXiv preprint arXiv:1812.10464.
Miguel Ballesteros, Chris Dyer, and Noah A Smith. 2015. Improved transition-based parsing by modeling characters instead of words with lstms. In Proceedings of the 2015 Conference on Empirical Meth-

ods in Natural Language Processing, pages 349– 359.
Toms Bergmanis and Sharon Goldwater. 2018. Context sensitive neural lemmatization with lematus. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1391– 1400.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. 2016. Generating sentences from a continuous space. CoNLL 2016, page 10.
Grzegorz Chrupała. 2006. Simple data-driven context-sensitive lemmatization. Procesamiento del Lenguaje Natural, 37.
Yoeng-Jin Chu. 1965. On the shortest arborescence of a directed graph. Scientia Sinica, 14:1396–1400.
Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. 2018. Semi-supervised sequence modeling with cross-view training. arXiv preprint arXiv:1809.08370.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Timothy Dozat and Christopher D Manning. 2016. Deep biafﬁne attention for neural dependency parsing. arXiv preprint arXiv:1611.01734.
Timothy Dozat, Peng Qi, and Christopher D Manning. 2017. Stanford’s graph-based neural dependency parser at the conll 2017 shared task. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 20–30.
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pages 845–850.
Jack Edmonds. 1967. Optimum branchings. Journal of Research of the national Bureau of Standards B, 71(4):233–240.
Yoav Goldberg. 2019. Assessing bert’s syntactic abilities. arXiv preprint arXiv:1901.05287.
John Hewitt and Christopher D Manning. 2019. A structural probe for ﬁnding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics.

Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 328–339.
Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daume´ III. 2015. Deep unordered composition rivals syntactic methods for text classiﬁcation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 1681–1691.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vie´gas, Martin Wattenberg, Greg Corrado, et al. 2017. Googles multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339–351.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. Character-aware neural language models. In Thirtieth AAAI Conference on Artiﬁcial Intelligence.
Nikita Kitaev and Dan Klein. 2018. Multilingual constituency parsing with self-attention and pretraining. arXiv preprint arXiv:1812.11760.
Daniel Kondratyuk, Toma´sˇ Gavencˇiak, Milan Straka, and Jan Hajicˇ. 2018. Lemmatag: Jointly tagging and lemmatizing for morphologically rich languages with brnns. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4921–4928.
Miryam de Lhoneux, Johannes Bjerva, Isabelle Augenstein, and Anders Søgaard. 2018. Parameter sharing between dependency parsers for related languages. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4992–4997.
Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Luis Marujo, and Tiago Luis. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1520–1530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the conference on empirical methods in natural language processing, pages 62–72. Association for Computational Linguistics.
Phoebe Mulcaire, Jungo Kasai, and Noah Smith. 2019. Polyglot contextual representations improve crosslingual transfer. arXiv preprint arXiv:1902.09697.

Thomas Mu¨ller, Ryan Cotterell, Alexander Fraser, and Hinrich Schu¨tze. 2015. Joint lemmatization and morphological tagging with lemming. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2268–2274.
Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629–637. Association for Computational Linguistics.
Joakim Nivre, Mitchell Abrams, Zˇ eljko Agic´, and Ahrenberg. 2018. Universal dependencies 2.3. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (U´ FAL), Faculty of Mathematics and Physics, Charles University.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.
Cicero D Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818–1826.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. 2019. Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing. arXiv preprint arXiv:1902.09492.
Damien Sileo. 2019. Understanding bert transformer: Attention isnt all you need. Towards Data Science.
Aaron Smith, Bernd Bohnet, Miryam de Lhoneux, Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82 treebanks, 34 models: Universal dependency parsing with multi-treebank models. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 113–123, Brussels, Belgium. Association for Computational Linguistics.
Milan Straka. 2018. UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197–207, Brussels, Belgium. Association for Computational Linguistics.
Milan Straka, Jana Strakova´, and Jan Hajicˇ. 2019. Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing. arXiv preprint arXiv:1908.07448.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008.

Jesse Vig. 2019. Visualizing attention in transformer-

based language models.

arXiv preprint

arXiv:1904.02679.

David Vilares, Carlos Go´mez-Rodr´ıguez, and Miguel A Alonso. 2016. One model, two languages: training bilingual parsers with harmonized treebanks. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 425–431.

Robert A Wagner and Michael J Fischer. 1974. The string-to-string correction problem. Journal of the ACM (JACM), 21(1):168–173.

Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert. arXiv preprint arXiv:1904.09077.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.

Daniel Zeman, Jan Hajicˇ, Martin Popel, Martin Potthast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov. 2018. CoNLL 2018 shared task: Multilingual parsing from raw text to universal dependencies. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–21, Brussels, Belgium. Association for Computational Linguistics.

A Appendix

In this section, we detail and explain hyperparameter choices and miscellaneous details related to model training and display the full tables of evaluation results of UDify across all UD languages.

A.1 Hyperparameters
Upon concatenating all training sets, we shufﬂe all the sentences, bundle them into batches of 32 sentences each, and train UDify for a total of 80 epochs before stopping. We hold the learning rate constant until we unfreeze BERT in the second epoch, where we and linearly warm up the learning rate for the next 8,000 batches and then apply inverse square root learning rate decay for the remaining epochs. For the dependency parser, we use feedforward tag and arc dimensions of 300

HYPERPARAMETER
Dependency tag dimension Dependency arc dimension Optimizer β1, β2 Weight decay Label Smoothing Dropout BERT dropout Mask probability Layer dropout Batch size Epochs Base learning rate BERT learning rate Learning rate warmup steps Gradient clipping

VALUE
256 768 Adam 0.9, 0.99 0.01 0.03 0.5 0.2 0.2 0.1 32 80 1e−3 5e−5 8000 5.0

Table 6: A summary of model hyperparameters.

and 800 respectively. We apply a small weight decay penalty of 0.01 to ensure that the weights remain small after each update. For optimization we use the Adam optimizer and we compute softmax cross entropy loss to train the network. We use a default β1 value of 0.9 and lower the β2 value from the typical 0.999 to 0.99. The reasoning is to increase the decay rate of the second moment in the Adam optimizer to reduce the chance of the optimizer being too optimistic with respect to the gradient history. We clip the gradient updates to a maximum L2 magnitude of 5.0. A summary of hyperparameters can be found in Table 6.
To speed up training, we employ bucketed batching, sorting all sentences by their length and grouping similar length sentences into each batch. However, to ensure that most sentences do not get grouped within the same batch, we fuzz the lengths of each sentence by a maximum of 10% of its true length when grouping sentences together.
Despite using all the regularization strategies shown previously, we still observe overﬁtting and must apply more aggressive techniques. To further regularize the network, we also increase the attention and hidden dropout rates of BERT from 0.1 to 0.2, and we also apply a dropout rate of 0.5 to all BERT layers before computing layer attention for each of the four tasks and applying a layer dropout with probability 0.1. We increase the masking probability of each wordpiece from 0.15 to 0.2.
With all these regularization strategies and hyperparameter choices combined, we are able to ﬁne-tune BERT for far more epochs before the network starts to overﬁt, i.e., 80 as opposed to around

LAS Difference

0.3

0.2

0.1

0.0

0.1

0.2

0.3

102

103

104

105

Train Sentences

Figure 5: A plot of the difference in LAS between UDify and UDPipe Future with respect to the number of training sentences in each treebank.

10. Even so, we believe even more regularization can improve test performance.
The ﬁnal multilingual UDify model was trained over approximately 25 days on an NVIDIA GTX 1080 Ti taking an average of 8 hours per epoch. We use half-precision (fp16) training to be able to keep the BERT model in memory. One notable aspect of training is that while we observed the model start to level out in validation performance at around epoch 30, the model continually made small, incremental improvements over each subsequent epoch, resulting in far higher scores than if the model training was terminated early. This can be partially attributed to the decaying inverse square root learning rate.
Due to the high training times, we are only able to report on a small number of training experiments for the most relevant and useful results. Prior to developing the ﬁnal model, we conducted ﬁne-tuning experiments on pairs of languages to ﬁnd a set of hyperparameters that worked best for multilingual learning. After this, we gradually scaled up training to 3 languages, 5 languages, 15 languages, and then ﬁnally the model presented above. We had high doubts, and wanted to see where the limit was in multilingual training. We were pleasantly surprised to ﬁnd that this simple training scheme was able to scale up so well to all UD treebanks.
A.2 Training Size Effect on Performance
To gain a better understanding of where the largest score improvements in UDify occur, we plot the LAS improvement UDify provides over UDPipe Future for each treebank, ordered by the size (number of sentences) of the training set, see Fig-

1.0

0.8

0.6

LAS

0.4

0.2

UDPipe Future

UDify

0.0

102

103

104

105

Train Sentences

Figure 6: A plot of LAS between with respect to the number of training sentences in each treebank.

tual embeddings of each word in the original sentence.
A.4 Full Results of UD Scores
We show in Tables 7, 8, 9, and 10 UDify scores evaluated on all 124 treebanks with the ofﬁcial CoNLL 2018 Shared Task evaluation script. For comparison, we also include the full test evaluation of UDPipe Future on the subset of 89 treebanks with a training set. We also add a column indicating the size of each treebank, i.e., the number of sentences in the training set.

ure 5. The results show that the largest improvements tend to occur on small treebanks with less than 3,000 training examples. For absolute LAS values, see Figure 6, which indicates that more training resources tend to improve evaluation performance overall.
A.3 Miscellaneous Details
Our results show that modeling language-speciﬁc properties is not strictly necessary to achieve highperforming cross-lingual representations for dependency parsing, though we caution that the model can also likely be improved by these techniques.
Fine-tuning BERT on UD introduces a syntactic bias in the network, and we are interested in observing any differences in transfer learning by ﬁne-tuning this new “UD-BERT” on other tasks. We leave a comprehensive evaluation of injecting syntactic bias into language models with respect to knowledge transfer for future work.
We note that saving the weights of BERT and ﬁne-tuning a second round can improve performance as demonstrated in Stickland et al. (2019). The improvements of UDify+Lang over just UDify can be partially attributed to this, but we can see that even these improvements can be inferior to ﬁne-tuning on all UD treebanks.
BERT limits its positional encoding to 512 wordpieces, causing some sentences in UD to be too long to ﬁt into the model. We use a sliding window approach to break up long sentences into windows of 512 wordpieces, overlapping each window by 256 wordpieces. After feeding the windows into BERT, we select the ﬁrst 256 wordpieces of each window and any remaining wordpieces in the last window to represent the contex-

TREEBANK Afrikaans AfriBooms
Akkadian PISANDUB Amharic ATT Ancient Greek PROIEL
Ancient Greek Perseus
Arabic PADT
Arabic PUD Armenian ArmTDP
Bambara CRB Basque BDT
Belarusian HSE
Breton KEB Bulgarian BTB
Buryat BDT
Cantonese HK Catalan AnCora
Chinese CFL Chinese GSD
Chinese HK Chinese PUD Coptic Scriptorium
Croatian SET
Czech CAC
Czech CLTT
Czech FicTree
Czech PDT
Czech PUD Danish DDT
Dutch Alpino
Dutch LassySmall

MODEL
UDPipe UDify UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify

UPOS
98.25 97.48 19.92 15.25 97.86 91.20 93.27 85.67 96.83 96.58 79.98
93.49 94.42 30.86 96.11 95.45
93.63 97.54 62.78 98.98 98.89
40.34 61.73 67.11
98.88 98.89 83.75
94.88 95.35 82.86 92.68 94.70 27.17 98.13 98.02 99.37 99.14
98.88 99.17 98.55 98.34 99.18 99.18 97.93 97.78 97.50
96.83 97.67
96.50 96.70

UFEATS
97.66 96.63 99.51 43.95 92.44 82.29 91.39 81.67 94.11 91.77 40.32 82.85 76.90 57.96 92.48 86.80
73.30 89.36 47.12 97.82 96.18
32.40 47.45 91.01 98.37 98.34 82.72
99.22 99.35 86.47 98.40 96.35 52.85 92.25 89.67 96.34 95.42
91.59 93.66 95.87 91.82 97.23 96.69 93.98 97.33 95.41
96.33 97.66
96.42 96.57

LEMMAS
97.46 95.23 2.32 58.04 93.51 76.16 85.02 70.51 95.28 73.55 0.00 92.86 85.63 20.42 96.29 90.53 87.34 85.46 51.31 97.94 93.49
58.17 61.03 96.01 99.07 98.14 98.75 99.99 99.97 100.00 100.00 95.49 55.71 97.27 95.34 98.57 98.32
98.25 98.86 98.63 98.13 99.02 98.52 96.94 97.52 94.60 97.09 95.44 97.41 95.10

UAS
89.38 86.97 27.65 17.38 85.93 78.91 78.85 70.51
87.54 87.72 76.17
78.62 85.63 30.28 86.11 84.94
78.58 91.82 63.52
93.38 95.54
32.60 48.43 46.82
93.22 94.25 62.46
84.64 87.93 65.53 79.08 85.58 27.58
91.10 94.08
92.99 94.33
86.90 91.69
92.91 95.19
93.33 94.73 92.59
86.88 87.76
91.37 94.23
90.20 94.34

LAS
86.58 83.48 4.54 3.49 82.11 72.66 73.54 62.64 82.94 82.88 67.07
71.27 78.61
8.60 82.86 80.97
72.72 87.19 39.84
90.35 92.40
18.83 26.28 32.01
91.06 92.33 42.48
80.50 83.75 49.32 56.51 80.97 10.82
86.78 89.79
90.71 92.41
84.03 89.96
89.75 92.77
91.31 92.88 87.95
84.31 84.50
88.38 91.21
86.39 91.22

CLAS
81.44 77.42 3.27 4.88 77.70 66.07 67.60 55.60 79.77 79.47 65.10
65.77 73.72
6.56 81.79 79.52
69.14 85.05 35.14
87.01 89.59
12.36 20.61 33.35
87.18 89.27 43.46
76.79 80.33 47.84 55.22 72.24
6.50
84.11 87.70
88.84 91.03
80.55 87.59
86.97 90.99
89.64 91.64 84.85
81.20 81.60
83.51 87.32
81.88 88.03

MLAS
77.66 70.57 1.04 0.23 67.16 50.79 53.87 39.15 73.92 70.52 10.67 48.11 46.80 1.04 72.33 63.60
46.20 71.54
4.64 83.63 83.43
1.26 5.51 14.29
84.48 86.21 21.07
71.04 74.36 22.85 40.92 64.45
0.19 73.61 72.72
84.30 84.68
71.63 79.50 81.04 77.77
86.15 87.13 77.39 76.29 73.76
77.28 82.81
77.19 82.06

BLEX
77.82 70.93 0.30 2.53 71.22 47.27 53.19 35.05 75.87 50.26 0.00 60.11 59.14 0.76 78.54 71.56
58.28 68.66 16.34 84.42 80.44
6.49 11.68 31.26
86.18 86.61 42.22
76.78 80.28 47.84 55.22 68.48
1.44
81.19 82.00
87.18 89.21
79.20 86.79
85.49 88.39
88.60 89.95 82.81 78.51 75.15
79.82 80.76
78.83 81.40

SIZE
1.3k 1.3k
0 0 15.0k 15.0k 11.5k 11.5k 6.1k 6.1k 0 561 561 0 5.4k 5.4k 261 261 0 8.9k 8.9k 20 20 0 13.1k 13.1k 0 4.0k 4.0k 0 0 371 371 7.0k 7.0k 23.5k 23.5k 861 861 10.2k 10.2k 68.5k 68.5k 0 4.4k 4.4k 12.3k 12.3k 5.8k 5.8k

Table 7: The full test results of UDify on 124 treebanks (part 1 of 4). The SIZE column indicates the number of training sentences.

TREEBANK English EWT English GUM English LinES English PUD English ParTUT Erzya JR Estonian EDT Faroese OFT Finnish FTB Finnish PUD Finnish TDT French GSD French PUD French ParTUT French Sequoia French Spoken Galician CTG Galician TreeGal German GSD German PUD Gothic PROIEL Greek GDT Hebrew HTB Hindi HDTB Hindi PUD Hungarian Szeged Indonesian GSD Indonesian PUD Irish IDT Italian ISDT Italian PUD Italian ParTUT

MODEL
UDPipe UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify

UPOS
96.29 96.21 96.02 95.44 96.91 95.31 96.18
96.10 96.16 46.66 97.64 97.44 77.46 96.65 93.80 96.48 97.45 94.43
97.63 97.83 91.67 96.93 96.12 98.79 97.89
95.91 96.23 97.84 96.51 95.82 94.59
94.48 94.55 89.49 96.61 95.55 97.98 97.72 97.02 96.94 97.52 97.12 87.54
95.76 96.36 93.69 93.36 76.10 92.72 90.49
98.39 98.51 94.73 98.38 98.21

UFEATS
97.10 96.02 96.82 94.12 96.31 91.34 93.50 95.51 92.61 31.82 96.23 95.13 35.20 96.62 90.38 93.84 95.43 90.48 97.13 96.17 59.65 94.43 88.36 98.09 88.97
100.00 98.67 99.83 97.10 93.96 80.67 90.68 90.43 30.66 90.73 85.97 94.96 93.29 95.87 93.41 94.15 92.59 22.81 91.75 86.16 95.58 93.32 44.23 82.43 71.84 98.11 98.01 58.16
97.77 98.38

LEMMAS
98.25 97.28 96.85 93.15 96.45 94.50 94.20 97.74 96.45 45.73 95.30 86.56 51.09 95.49 88.80 84.64 91.45 82.89 98.35 97.34 100.00 95.70 93.97 98.57 97.15 96.92 96.59 98.58 97.08 97.06 94.93 96.80 94.42 94.77 94.75 80.57 95.82 89.43 97.12 94.15 98.67 98.23 100.00 95.05 90.19 99.64 98.37 100.00 90.48 81.27 98.66 97.72 96.08 98.16 97.55

UAS
89.63 90.96
87.27 89.14
84.15 87.33 91.52
90.29 92.84 31.90
88.00 89.53 67.24 90.68 86.37 89.76 89.88 86.42
90.65 93.60 88.36 92.17 90.55
92.37 92.53
82.90 85.24 86.44 84.75
82.72 84.08
85.53 87.81 89.86
85.28 85.61
92.10 94.33
89.70 91.63
94.85 95.13 71.64
84.04 89.68
85.31 86.45 77.47 80.39 80.05
93.49 95.54 94.18
92.64 95.96

LAS
86.97 88.50
84.12 85.73
79.71 83.71 88.66
87.27 90.14 16.38
85.18 86.67 59.26 87.89 81.40 86.58 87.46 82.03
88.06 91.45 82.76 89.63 88.06 90.73 90.05
77.53 80.01 83.82 80.89 77.69 76.77
81.07 83.59 84.46 79.60 79.37
89.79 92.15
86.86 88.11 91.83 91.46 58.42
79.73 84.88
78.99 80.10 56.90 72.34 69.28
91.54 93.69 91.76
90.47 93.68

CLAS
84.02 86.25
78.55 83.03
77.44 82.95 87.83
82.58 86.28 10.83
83.62 85.17 51.17 85.11 81.01 86.64 85.87 82.62
84.35 88.54 81.74 84.62 83.19 87.55 86.67
71.82 75.40 78.58 74.62
71.69 73.06
76.26 80.03 80.50 76.92 76.26
85.71 88.67
81.45 83.04 88.21 87.80 53.03
78.65 83.93
76.76 78.05 54.88 63.48 60.02
87.34 90.40 90.05
85.05 89.83

MLAS
79.00 79.80 73.51 72.55 71.38 68.62 75.61 76.44 74.59
0.58
78.72 79.20
2.39 80.58 68.16 77.83 80.43 70.89
79.76 81.61 25.24 75.22 63.03 84.51 67.98
68.24 69.74 72.46 65.86 63.73 49.76
58.82 61.27
2.10 66.70 63.09 78.60 77.89 75.52 72.55 78.49 75.54
3.32 67.63 64.27 67.74 66.93 7.41 46.49 34.39
84.28 86.54 25.55
81.87 86.83

BLEX
82.36 83.39 74.68 74.30
73.22 76.23 80.57
80.33 82.01
2.83 78.51 69.31 21.92 81.18 70.15 69.12 76.64 63.66
82.39 84.51 81.74 78.07 74.03 85.93 82.52
69.47 72.77 77.21 72.17 68.89 66.99
72.13 72.48 72.95 72.93 58.65 79.72 71.83 78.14 74.87 86.83 86.10 53.03 73.63 72.21 76.38 76.31 54.88 55.32 43.07
85.49 86.70 83.74
82.99 86.44

SIZE
12.5k 12.5k 2.9k 2.9k 2.7k 2.7k
0 1.8k 1.8k
0 24.4k 24.4k
0 15.0k 15.0k
0 12.2k 12.2k 14.5k 14.5k
0 804 804 2.2k 2.2k 1.2k 1.2k 2.3k 2.3k 601 601 13.8k 13.8k
0 3.4k 3.4k 1.7k 1.7k 5.2k 5.2k 13.3k 13.3k
0 911 911 4.5k 4.5k
0 567 567 13.1k 13.1k
0 1.8k 1.8k

Table 8: The full test results of UDify on 124 treebanks (part 2 of 4).

TREEBANK Japanese GSD Japanese Modern Japanese PUD Kazakh KTB Komi Zyrian IKDP Komi Zyrian Lattice Korean GSD Korean Kaist Korean PUD Kurmanji MG Latin ITTB Latin PROIEL Latin Perseus Latvian LVTB Lithuanian HSE Maltese MUDT Marathi UFAL Naija NSC North Sami Giella Norwegian Bokmaal Norwegian Nynorsk Norwegian NynorskLIA Old Church Slavonic PROIEL Old French SRCMF Persian Seraji Polish LFG Polish SZ Portuguese Bosque Portuguese GSD Portuguese PUD Romanian Nonstandard

MODEL
UDPipe UDify
UDify
UDify
UDPipe UDify
UDify
UDify
UDPipe UDify
UDPipe UDify
UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDPipe UDify
UDify
UDPipe UDify

UPOS
98.13 97.08 74.94
97.89
55.84 85.59
59.92
38.57
96.29 90.56 95.59 94.67 64.43
53.36 60.23
98.34 98.48
97.01 96.79
88.40 90.96
96.11 96.02
81.70 90.47
95.99 91.98
80.10 88.59
55.44
92.54 90.21 98.31 98.18 98.14 98.14
89.59 95.01
96.91 84.23 96.09 95.73 97.75 96.22 98.80 98.80
98.34 98.36
97.07 97.10
98.31 98.04 90.14
96.68 96.83

UFEATS
99.98 99.97 96.14 99.98
40.40 65.49 39.32 29.45 99.77 99.63
100.00 99.98 60.47 41.54 37.78 96.97 95.81 91.53 89.49
79.10 82.09 93.01 89.78
60.47 70.00
100.00 99.89 67.23 59.22 51.32 90.03 83.55 97.14 96.36 97.02 96.55
86.13 93.36 90.66 71.30 97.81 96.98 97.78 94.73 95.49 87.71 93.04 67.11 96.40 89.70 99.92 95.75 51.16 90.88 88.89

LEMMAS
99.52 98.80 79.70 99.31
63.96 77.18
57.56 55.33
93.40 82.84 94.30 85.89 70.47 69.58 58.08 98.99 98.08 96.32 91.79 81.45 81.08 95.46 91.00 76.89 67.17 100.00 100.00
81.31 72.82 97.03 88.31 71.50 98.64 97.33 98.18 97.18
93.93 96.13 93.11 65.70 100.00 100.00
97.44 92.55 97.54 94.04 97.16 93.92 98.46 91.60 99.30 98.95 99.79 94.78 89.33

UAS
95.06 94.37 74.99 94.89
53.30 74.77 36.01 28.85 87.70 82.74 88.42 87.57 63.57 45.23 35.86
91.06 92.43
83.34 84.85
71.20 78.33
87.20 89.33
51.98 79.06 84.65 83.07
70.63 79.37 45.75 78.30 74.30
92.39 93.97
92.09 94.34
68.08 75.40 89.66 76.71 91.74 91.74 90.05 89.59
96.58 96.67
93.39 93.67
91.36 91.37
93.01 94.22 87.02
89.12 90.36

LAS
93.73 92.08 55.62 93.62
33.38 63.66 22.12 12.99 84.24 74.26 86.48 84.52 46.89 34.32 20.40
88.80 90.12
78.66 80.52
61.28 69.60
83.35 85.09
42.17 69.34 79.71 75.56
61.41 67.72 32.16 73.49 67.13
90.49 92.18
90.01 92.37
60.07 69.60 85.04 66.67 86.83 86.65 86.66 85.84 94.76 94.58 91.24 89.20 89.04 87.84
91.63 92.54 80.17
84.20 85.26

CLAS
88.35 86.19 42.67 87.92
27.06 61.84 17.45 10.79 82.05 71.72 84.12 82.05 45.29 29.41 14.75
86.40 87.93
76.20 77.96
56.32 65.95
80.90 82.34
38.93 66.00 71.49 65.08
57.44 60.13 31.62 70.94 64.41
88.18 90.40
87.68 90.39
54.89 65.33 83.41 64.10 83.85 83.49 83.26 81.98
93.01 93.03 89.39 87.31 85.19 84.13
87.67 89.37 74.10
78.91 80.41

MLAS
86.37 82.99 30.89 84.86 4.82 34.23 1.54 0.72 79.74 65.94 80.72 78.27 16.26 2.74 1.42 82.35 82.24 67.40 67.18
41.58 50.26 71.92 69.51
18.17 36.21 66.75 58.14 29.34 21.71
4.73 62.40 51.20
84.06 85.02
82.97 85.01
44.47 56.90 73.63 46.25 79.91 78.85 81.23 76.65 87.04 76.50 81.06 48.47 76.67 69.09 85.96 82.32 17.51 65.93 64.68

BLEX
88.04 85.12 35.47 87.15
15.10 45.51 6.80 3.28 76.35 57.58 79.22 68.99 30.94
19.39 7.28
85.71 85.97 73.65 71.00
45.09 51.33 76.64 72.58
28.70 36.35 71.49 65.08 45.87 39.25 29.33 61.45 40.63
86.53 87.13
85.47 86.71
50.98 62.27 77.81 43.88 83.85 83.49 80.93 74.74 90.26 85.15 85.99 80.24 83.06 78.64
86.94 87.90 74.10 73.44 68.11

SIZE
7.1k 7.1k
0
0
32 32
0
0
4.4k 4.4k
23.0k 23.0k
0
21 21
16.8k 16.8k
15.9k 15.9k
1.3k 1.3k
7.2k 7.2k
154 154
1.1k 1.1k
374 374
0
2.3k 2.3k
15.7k 15.7k
14.2k 14.2k
340 340
4.1k 4.1k
13.9k 13.9k
4.8k 4.8k
13.8k 13.8k
6.1k 6.1k
8.3k 8.3k
9.7k 9.7k
0
8.0k 8.0k

Table 9: The full test results of UDify on 124 treebanks (part 3 of 4).

TREEBANK Romanian RRT Russian GSD Russian PUD Russian SynTagRus Russian Taiga Sanskrit UFAL Serbian SET Slovak SNK Slovenian SSJ Slovenian SST Spanish AnCora Spanish GSD Spanish PUD Swedish LinES Swedish PUD Swedish Sign Language SSLC Swedish Talbanken Tagalog TRG Tamil TTB Telugu MTG Thai PUD Turkish IMST Turkish PUD Ukrainian IU Upper Sorbian UFAL Urdu UDTB Uyghur UDT Vietnamese VTB Warlpiri UFAL Yoruba YTB

MODEL
UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDify UDPipe UDify UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDPipe UDify UDify UDify

UPOS
97.96 97.73 97.10 96.91 93.06 99.12 98.97
93.18 95.39 37.33 98.33 98.30
96.83 97.46
98.61 98.73
93.79 95.40 98.91 98.53 96.85 95.91 88.98
96.78 96.85 96.36 68.09 63.48
97.94 98.11 60.62
91.05 91.50
93.07 93.48 56.78 96.01 94.29 77.34
97.59 97.71
62.93 84.87
93.66 94.37 89.87 75.88
89.68 91.29 33.44 50.86

UFEATS
97.53 96.12 92.66 87.45 63.60 97.57 96.29
82.87 88.47 17.63 94.35 92.22 90.82 89.30 95.92 93.44
86.28 89.81 98.49 97.89 97.09 95.08 54.58 89.43 87.24 80.04
100.00 96.10 96.86 95.92 35.62 87.28 83.21
99.03 99.31 62.48 92.55 84.49 24.59 92.66 88.63
41.10 48.84
81.92 82.80 88.30 70.80 99.72 99.58 18.15 78.32

LEMMAS
98.41 95.84 97.37 77.73 77.93 98.53 94.47
89.99 90.19 37.38 97.36 95.86 96.40 93.80 98.25 96.50 95.17 95.15 99.17 98.07 98.97 96.52 100.00 97.03 92.70 88.81 100.00 100.00 98.01 95.50 73.63 93.92 80.84 100.00 100.00 100.00 96.01 87.71 84.31 97.23 94.00
68.68 72.68 97.40 96.68 95.31 79.70 99.55 99.21 39.17 85.56

UAS
91.31 93.16
88.15 90.71 93.51
93.80 94.83
75.45 84.02 40.21
92.70 95.68
89.82 95.92
92.96 94.74
73.51 80.37
92.34 92.99
90.71 90.82 90.45
86.07 88.77 89.17 50.35 40.43
89.63 91.91 64.04
74.11 79.34
91.26 92.23 49.05
74.19 74.56 67.68
88.29 92.83
45.58 71.55
87.50 88.43 78.46 65.89
70.38 74.11 21.66 37.62

LAS
86.74 88.56
84.37 86.03 87.14
92.32 93.13
69.11 77.80 18.56
89.27 91.95
86.90 93.87
91.16 93.07
67.51 75.03
90.26 90.50 88.03 87.23 83.08
81.86 85.49 86.10 37.94 26.95
86.61 89.03 40.07
66.37 71.29 85.02 83.91 26.06 67.56 67.44 46.07
85.25 90.30
34.54 62.82
81.62 82.84 67.09 48.80
62.56 66.00
7.96 19.09

CLAS
82.57 84.87
82.66 84.51 83.96
90.85 91.87
65.31 75.12 15.38
87.08 90.30
84.81 92.86
88.76 90.94
63.46 71.19
86.39 87.26 82.85 82.83 77.42
80.32 85.61 85.25 39.51 30.12
84.45 87.26 36.84
63.71 69.10 81.76 79.92 18.42
63.83 63.87 39.95
81.90 88.15
27.18 56.04
75.20 77.00 60.85 38.95
60.03 63.34
7.49 16.56

MLAS
79.02 79.20 74.07 67.24 37.25 87.91 86.91
48.81 59.71 0.85 79.14 78.45
74.00 77.33 83.85 81.55
52.67 61.32 83.97 83.43 75.98 72.47 18.06
66.48 66.99 57.12 30.96 23.29
79.67 80.72
0.00 55.31 53.62 77.75 76.10 3.77 56.96 49.42 2.61 73.81 72.93 3.37 16.19
55.02 56.70 47.84 21.75
55.56 58.71
0.00 6.30

BLEX
81.09 79.92 80.03 62.08 61.86 89.17 85.44
57.21 65.15 4.12
84.18 84.93
81.37 85.12 86.89 86.38
60.32 67.24 85.51 84.85 81.47 78.08 77.42
77.38 77.62 72.92 39.51 30.12 82.26 81.31 13.16 59.58 54.84 81.76 79.92 18.42 61.37 54.10 32.50
79.10 81.04
16.65 37.89
73.07 73.97 57.08 31.31
59.54 62.61
0.88 12.15

SIZE
8.0k 8.0k 3.9k 3.9k
0 48.8k 48.8k
881 881
0 2.9k 2.9k 8.5k 8.5k 6.5k 6.5k 2.1k 2.1k 14.3k 14.3k 14.2k 14.2k
0 2.7k 2.7k
0 88 88 4.3k 4.3k 0 401 401 1.1k 1.1k 0 3.7k 3.7k 0 5.3k 5.3k 24 24 4.0k 4.0k 1.7k 1.7k 1.4k 1.4k 0 0

Table 10: The full test results of UDify on 124 treebanks (part 4 of 4).

