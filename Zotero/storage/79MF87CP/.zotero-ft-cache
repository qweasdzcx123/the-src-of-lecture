Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition
Jun Liu1, Amir Shahroudy1, Dong Xu2, and Gang Wang1(B)
1 School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore
{jliu029,amir3,wanggang}@ntu.edu.sg 2 School of Electrical and Information Engineering, University of Sydney,
Sydney, Australia dong.xu@sydney.edu.au
Abstract. 3D action recognition – analysis of human actions based on 3D skeleton data – becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its eﬀect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.
Keywords: 3D action recognition · Recurrent neural networks · Long short-term memory · Trust gate · Spatio-temporal analysis
1 Introduction
In recent years, action recognition based on the locations of major joints of the body in 3D space has attracted a lot of attention. Diﬀerent feature extraction and classiﬁer learning approaches are studied for 3D action recognition [1–3]. For example, Yang and Tian [4] represented the static postures and the dynamics of the motion patterns via eigenjoints and utilized a Na¨ıve-Bayes-Nearest-Neighbor classiﬁer learning. A HMM was applied by [5] for modeling the temporal dynamics of the actions over a histogram-based representation of 3D joint locations. Evangelidis et al. [6] learned a GMM over the Fisher kernel representation of a succinct skeletal feature, called skeletal quads. Vemulapalli et al. [7] represented the skeleton conﬁgurations and actions as points and curves in a Lie group
c Springer International Publishing AG 2016 B. Leibe et al. (Eds.): ECCV 2016, Part III, LNCS 9907, pp. 816–833, 2016. DOI: 10.1007/978-3-319-46487-9 50

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 817
respectively, and utilized a SVM classiﬁer to classify the actions. A skeletonbased dictionary learning utilizing group sparsity and geometry constraint was also proposed by [8]. An angular skeletal representation over the tree-structured set of joints was introduced in [9], which calculated the similarity of these features over temporal dimension to build the global representation of the action samples and fed them to SVM for ﬁnal classiﬁcation.
Recurrent neural networks (RNNs) which are a variant of neural nets for handling sequential data with variable length, have been successfully applied to language modeling [10–12], image captioning [13,14], video analysis [15–24], human re-identiﬁcation [25,26], and RGB-based action recognition [27–29]. They also have achieved promising performance in 3D action recognition [30–32].
Existing RNN-based 3D action recognition methods mainly model the longterm contextual information in the temporal domain to represent motion-based dynamics. However, there is also strong dependency between joints in the spatial domain. And the spatial conﬁguration of joints in video frames can be highly discriminative for 3D action recognition task.
In this paper, we propose a spatio-temporal long short-term memory (STLSTM) network which extends the traditional LSTM-based learning to two concurrent domains (temporal and spatial domains). Each joint receives contextual information from neighboring joints and also from previous frames to encode the spatio-temporal context. Human body joints are not naturally arranged in a chain, therefore feeding a simple chain of joints to a sequence learner cannot perform well. Instead, a tree-like graph can better represent the adjacency properties between the joints in the skeletal data. Hence, we also propose a tree structure based skeleton traversal method to explore the kinematic relationship between the joints for better spatial dependency modeling.
In addition, since the acquisition of depth sensors is not always accurate, we further improve the design of the ST-LSTM by adding a new gating function, so called “trust gate”, to analyze the reliability of the input data at each spatiotemporal step and give better insight to the network about when to update, forget, or remember the contents of the internal memory cell as the representation of long-term context information.
The contributions of this paper are: (1) spatio-temporal design of LSTM networks for 3D action recognition, (2) a skeleton-based tree traversal technique to feed the structure of the skeleton data into a sequential LSTM, (3) improving the design of the ST-LSTM by adding the trust gate, and (4) achieving stateof-the-art performance on all the evaluated datasets.
2 Related Work
Human action recognition using 3D skeleton information is explored in diﬀerent aspects during recent years [33–50]. In this section, we limit our review to more recent RNN-based and LSTM-based approaches.
HBRNN [30] applied bidirectional RNNs in a novel hierarchical fashion. They divided the entire skeleton to ﬁve major groups of joints and each group was fed

818 J. Liu et al.
into a separated bidirectional RNN. The output of these RNNs were concatenated to represent upper-body and lower-body, then each was fed into another set of RNNs. The global body representation was obtained by concatenating the output of these two RNNs and it was fed to the next layer of RNN. The hidden representation of the ﬁnal RNN was fed to a softmax classiﬁer layer for action classiﬁcation.
Zhu et al. [51] added a mixed-norm regularization term to a deep LSTM network’s cost function in order to push the network towards learning co-occurrence of discriminative joints for action classiﬁcation. They further introduced an internal dropout [52] technique within the LSTM unit, which was applied on all the gate activations.
Diﬀerential LSTM [31] added a new gating inside LSTM to keep track of the derivatives of the memory states in order to discover patterns within salient motion patterns. All the input features for each frame were concatenated and fed to the diﬀerential LSTM.
Part-aware LSTM [32] separated the memory cell to part-based sub-cells and pushed the network towards learning the long-term context representations individually for each part. The output of the network was learned over the concatenated part-based memory cells followed by the common output gate.
Unlike the above mentioned works, the framework proposed in this paper does not concatenate the joint-based input features, instead it explicitly models the dependencies between the joints and applies recurrent analysis over spatial and temporal domains concurrently. Besides, a novel trust gate is developed to make LSTM robust to noisy input data.
3 Spatio-Temporal Recurrent Networks
Human actions can be characterized by the motion of body parts over time. In 3D human action recognition, we have three dimensional locations of the major body joints in each frame. Recently, recurrent neural networks have been successfully employed for skeleton-based 3D action recognition [30,32,51].
Long Short-Term Memory (LSTM) networks [53] are very successful extensions of the recurrent neural networks (RNNs). They utilize the gating mechanism over an internal memory cell to learn and represent a better and more complex representation of the long-term dependencies among the input sequential data, thus they are suitable for feature learning over a sequence of temporal data.
In this section, ﬁrst we will brieﬂy review the standard LSTM networks, then describe the proposed spatio-temporal LSTM model and the skeleton-based tree traversal. Next we will introduce an eﬀective gating scheme for LSTM to deal with the measurement noise in the input data (body joint locations) for the task of 3D human action recognition.
3.1 Temporal Modeling with LSTM
A typical LSTM unit contains an input gate it, a forget gate ft, an output gate ot, and an output state ht, together with an internal memory cell state ct. The LSTM transition equations are formulated as:

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 819

⎛⎞ ⎛ ⎞

it

σ

⎜⎜⎝

ft ot

⎟⎟⎠

=

⎜⎜⎝

σ σ

⎟⎟⎠ M

xt ht−1

(1)

ut

tanh

ct = it ut + ft ct−1

(2)

ht = ot tanh(ct)

(3)

where indicates element-wise product, xt denotes the input to the network at time step t, and ut denotes the modulated input. σ is the sigmoid activation function. M : D+d → 4d is an aﬃne transformation consisting of model
parameters, where D is the dimensionality of input xt and d is the number of LSTM cell state units.
Intuitively, the input gate it determines the extent to which the modulated input information (ut) is supposed to update the memory cell at time t. The forget gate ft determines the eﬀectiveness of the previous state of the memory cell (ct−1) on its current state (ct). Finally, the output gate ot governs the amount of information output from the memory cell. Readers are referred to [54] for more
details about the mechanism of LSTM.

3.2 Spatio-Temporal LSTM
Very recent attempts on applying RNNs for 3D human action recognition [30–32, 51] show outstanding performance and prove the strengths of RNNs in modeling the complex dynamics of the human actions in temporal space.
The main focus of these existing methods was on utilizing RNNs over temporal domain for discovering the discriminative dynamics and body motion patterns for 3D action recognition. However, there is also discriminative information in static postures encoded within the joints’ 3D locations in each individual frame and the sequential nature of skeleton data makes it possible to adopt RNN-based learning in spatial domain as well. Unlike other existing methods, which concatenated the joints information, we extend the recurrent analysis towards spatial domain to discover the spatial dependency patterns between diﬀerent joints at each frame.
In this fashion, we propose a spatio-temporal LSTM (ST-LSTM) model which simultaneously models the spatial dependencies of the joints and the temporal dependencies among the frames. As shown in Fig. 1, every ST-LSTM unit corresponds to one of the skeletal joints. Each of the units receives the hidden representation of the previous joint and also the hidden representation of its own joint from the previous frame. In this section we assume joints are arranged in a chain-like sequence with the order shown in Fig. 2(a). In Sect. 3.3, we will show a more advanced method to take advantage of the adjacency information of the body joints as a tree structure.
We use j ∈ {1, ..., J} and t ∈ {1, ..., T } to denote the indices of joints and frames respectively. Each ST-LSTM unit is fed with its input (xj,t, location of the corresponding joint at current frame), its own hidden representation at the

820 J. Liu et al.

hj-1,t

hj,t hj,t

(j,t)

hj,t-1

Time

Joint
Fig. 1. The illustration of the proposed spatio-temporal LSTM network. In the spatial direction, body joints in a frame are fed in a sequence. In the temporal direction, the locations of the corresponding joints are fed over time. Each unit receives the hidden representation of previous joints and previous frames of the same joint as contextual information.

previous time step (hj,t−1), and the hidden representation of the previous joint
at current frame (hj−1,t). Each unit is also equipped with two diﬀerent forget gates corresponding to the two incoming channels of context information: fjS,t for the spatial domain, and fjT,t for the temporal domain. The proposed ST-LSTM is formulated as:

⎛ ⎞⎛ ⎞

ij,t

σ

⎜⎜⎜⎜⎝

fjS,t fjT,t oj,t

⎟⎟⎟⎟⎠

=

⎜⎜⎜⎜⎝

σ σ σ

⎟⎟⎟⎟⎠

⎛ ⎝M

⎛ ⎝

xj,t hj−1,t hj,t−1

⎞⎞ ⎠⎠

(4)

uj,t

tanh

cj,t = ij,t uj,t + fjS,t cj−1,t + fjT,t cj,t−1

(5)

hj,t = oj,t tanh(cj,t)

(6)

3.3 Tree-Structure Based Traversal
Arranging joints in a simple chain ignores the kinematic dependency relations between the joints and adds false connections between body joints which are not strongly related. In human parsing, skeletal joints are popularly modeled as a tree-based pictorial structure [55,56], as illustrated in Fig. 2(b). In our ST-LSTM framework, it is also beneﬁcial to model the spatial dependency of the joints based on their adjacency tree structure. For example, hidden representation of the neck joint (number 2 in Fig. 2(a)) is expected to be more informative for the right hand joints (7, 8, 9) than the joint number 6.
However, trees cannot be directly fed into the ST-LSTM framework. To mitigate this issue, we propose a bidirectional tree traversal method to visit joints in a sequence which maintains the adjacency information of the skeletal tree structure.
As illustrated in Fig. 2(c), at the ﬁrst spatial step, the root node (central spine joint) is fed to the network, then the network follows a depth-ﬁrst traversal in the

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 821

3

4

2

7

5

1

8

6

10

9

11

14

12

15

13

16

(a)

1

2

10

3

4

7 11

14

5

8

12

15

6

9

13

16

(b)

1

1

30

1617

2

10

3

24 9 10
34

18 15 23
24 7 11

29 14

5

11

8

14 19

22 25

28

5 6
7

8
12 13

12 21
20

15 27
26

6

9

13

16

(c)

Fig. 2. (a) Skeletal joints of a human body. In the simple joint chain model, the joint visiting order is 1-2-3-...-16. (b) Skeleton is transformed to a tree structure. (c) Tree traversal over the spatial steps. The tree can be unfolded to a chain with the traversal, and the joint visiting order is 1-2-3-2-4-5-6-5-4-2-7-8-9-8-7-2-1-10-11-12-13-12-11-10-1415-16-15-14-10-1.

spatial domain. When it reaches a leaf node, it goes back. In this fashion, each connection of the tree structure will be passed twice and the context information is fed along both directions. Upon the end of the traversal, it gets back to the root node.
This traversal strategy guarantees the transmission of the data in both directions (top-down and bottom-up) inside the adjacency tree structure. Therefore each node will have the contextual information from both its descendants and ancestors. Compared to the simple chain model described in Sect. 3.2, this tree traversal technique can discover stronger long-term spatial dependency patterns based on the joints’ adjacency structure.
In addition, the input to the ST-LSTM network at each step is limited to a single joint in a speciﬁc frame, which is much smaller in size compared to the concatenated input features of other existing methods. As a result, we have much fewer model parameters and this can be considered as a weight sharing regularization inside our learning framework, which leads to better generalization in the scenarios with limited training samples. This is an advantage in 3D action recognition, because most of the current datasets have a small number of training samples.
Similar to other LSTM implementations [57,58], the representation capacity of our network can be improved by stacking multiple layers of the tree structured ST-LSTMs and constructing a deep yet completely tractable network, as illustrated in Fig. 3.
3.4 Spatio-Temporal LSTM with Trust Gates
The inputs of the proposed tree-structured ST-LSTM are the 3D positions of skeletal joints collected by sensors like Microsoft Kinect, which are not always reliable due to noise and occlusion. This limits the performance of the network. To address this issue, we propose to add a new gate to the LSTM unit which

822 J. Liu et al.
t3 t2 t1

Joints

ST-LSTM units (Layer 1) ST-LSTM units (Layer 2) Softmax classifier

Fig. 3. A graphical model of the deep tree-structured ST-LSTM network. For clarity, some arrows are omitted in the stacked network (better viewed in color). In this ﬁgure, the output of the ﬁrst ST-LSTM layer is fed to the second ST-LSTM layer as its input. The second ST-LSTM layer’s output is fed to softmax layer. (Color ﬁgure online)

analyzes the reliability of the input at each spatio-temporal step, based on the estimation of the input from the available contextual information.
Our novel gating method is inspired by the works in natural language processing [58] which predict next word based on LSTM representation of previous words. This idea worked well because of the high dependency among the words in a sentence. Similarly, since the skeletal joints often move together and this articulated motion follows common yet complex patterns at each spatio-temporal step, the input data xj,t is supposed to be predictable from the contextual representations hj,t−1 and hj−1,t.
This predictability inspired us to add new mechanism to ST-LSTM to predict the input and compare it with the actual incoming input. The amount of the estimation error is used as input to a new “trust gate”. The derived trust value provides information to the long-term memory mechanism to learn better decisions about when and how to remember and forget the contents of the memory cell. For example, when the trust gate ﬁnds out the current joint has wrong 3D measurements, it can block the input gate and prevent the memory cell from updating based on current unreliable input.
Mathematically, for an input at step (j, t), we develop a function to generate its prediction, based on the available contextual information:

pj,t = tanh Mp

hj−1,t hj,t−1

(7)

where the aﬃne transformation Mp maps the data from 2d to d, so the dimensionality of pj,t is d. It is worth noting that the contextual information at each step is not limited to the hidden states of the previous spatial step but it also includes the previous temporal step, i.e., the long-term memory information of the same joint in previous frames and the contextual information of other visited joints in the same frame are seamlessly incorporated. Therefore, we can expect this function to be able to produce good predictions.

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition

h j-1,t

h j,t-1

p j,t

-

x j,t

x’j,t

x j,t

h j-1,t

uj,t

h j,t-1

x j,t

h j-1,t

ij,t

h j,t-1

x x h h h h j,t j-1,t j,t-1 j,t j-1,t j,t-1

fjT,t

oj,t

cj,t-1 cj,t

hj,t

j,t

+ cj,t

c

cj-1,t

fjS,t x h h j,t j-1,t j,t-1

Fig. 4. Schema of the proposed ST-LSTM with trust gate.

823

The activation of the proposed trust gate τ is a vector in d, which is similar to the activation of the input gate and the forget gate, and it will be calculated as:

xj,t = tanh Mx xj,t

(8)

τj,t = G(xj,t − pj,t)

(9)

where Mx : D → d is an aﬃne transformation, and the new activation function G(·) is an element-wise operation formulated as:

G(z) = exp(−λz2)

(10)

In this equation, λ > 0 is a parameter to control the spread of the Gaussian function. G(z) produces a large response if z is close to origin, and small response when z has a large absolute value.
Utilizing the proposed trust gate, the cell state of the ST-LSTM neuron can be updated as:
cj,t = τj,t ij,t uj,t + (1 − τj,t) fjS,t cj−1,t + (1 − τj,t) fjT,t cj,t−1 (11)

If the new input xj,t cannot be trusted (because of noise or occlusion), then we need to take advantage of more history information and try to block the new input. In contrast, if the input is reliable, we can let the learning algorithm update the memory cell by importing input information.
Figure 4 depicts the scheme of the new ST-LSTM unit empowered with the trust gate. This can be learned similar to other gates by back-propagation. The proposed trust gate technique is theoretically general and can be applied to other applications to deal with unreliable input data.

3.5 Learning the Classiﬁer
Since the action labels are always given at the video level, we feed them as the training outputs of the ST-LSTM at each spatio-temporal step. The network learns to predict the action class yˆ among a discrete set of classes Y using a

824 J. Liu et al.

softmax layer. The overall prediction of a video is computed by averaging the predictions of all the steps. Empirically, this method provides better performance compared to the minimization of the loss at the last step only.
The objective function of our model is formulated as:

JT

L=

l(yˆj,t, y)

j=1 t=1

(12)

where l(yˆj,t, y) is the negative log-likelihood loss [54] measuring the diﬀerence between the true label y and the predicted result yˆj,t at step (j, t). The objective function can be minimized using back-propagation through time (BPTT)
algorithm [54].

4 Experiments
The proposed model is evaluated on four datasets: NTU RGB+D dataset, SBU Interaction dataset, UT-Kinect dataset, and Berkeley MHAD dataset. We conduct extensive experiments with diﬀerent conﬁgurations as follows:
(1) “ST-LSTM (Joint Chain)”: In this conﬁguration, the joints are visited one by one in a simple chain order (see Fig. 2(a)).
(2) “ST-LSTM (Tree Traversal)”: The proposed tree traversal strategy (Fig. 2(c)) is adopted in this conﬁguration to fully exploit the tree-based spatial structure of human joints.
(3) “ST-LSTM (Tree Traversal) + Trust Gate”: This conﬁguration involves the trust gate to deal with noisy input.

4.1 Evaluation Datasets
NTU RGB+D Dataset [32]. To the best of our knowledge, this dataset is currently the largest depth-based action recognition dataset. It is collected by Kincet v2 and contains more than 56 thousand sequences and 4 million frames. A total of 60 diﬀerent action classes including daily actions, pair actions, and medical conditions are performed by 40 subjects aged between 10 and 35. The 3D coordinates of 25 joints are provided in this dataset. The large intra-class and view point variations make this dataset very challenging. Due to the large amount of samples, this dataset is highly suitable for deep learning based action recognition.
SBU Interaction Dataset [59]. This dataset is captured with Kinect and contains 8 classes of two-person interactions. It includes 282 skeleton sequences in 6822 frames. Each skeleton has 15 joints. The challenges of this dataset include: (1) in most interactions, one person is acting and the other one is reacting; and (2) the joint coordinates in many sequences are of low accuracy.

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 825
UT-Kinect Dataset [5]. This dataset contains 10 action classes performed by 10 subjects, captured with a stationary Kinect. Each action was performed twice by every subject. The locations of 20 joints are provided in this dataset. The high intra-class variation and viewpoint diversity makes it challenging.
Berkeley MHAD [60]. The MHAD dataset is captured by a motion capture system. It consists of 659 sequences and about 82 min of recording. Eleven different action classes were performed by 7 male and 5 female subjects. The 3D locations of 35 joints are provided in this dataset.
4.2 Implementation Details
In our experiments, each video sequence is divided to T sub-sequences with the same length, and one frame was randomly selected from each sub-sequence. Such a method adds randomness into the process of data generation and improves the generalization capability. We observe this strategy achieves better performance in contrast to uniformly sampled frames. We cross-validated the performance based on leave-one-subject-out protocol on NTU RGB+D dataset, and found T = 20 as the optimum value.
We use Torch toolbox as the deep learning platform and an NVIDIA Tesla K40 GPU to run our experiments. We train the network using stochastic gradient descent, and set learning rate, momentum and decay rate as 2×10−3, 0.9 and 0.95, respectively. For our network, we set the neuron size d to 128, and the parameter λ used in G(·) to 0.5. We use two ST-LSTM layers in the stacked network, and the applied probability of dropout is 0.5. Though there are variations in terms of sequence length, joint number, and data acquisition equipment for diﬀerent datasets, we use the same parameter settings mentioned above. This indicates the insensitiveness of our method to the parameter settings, as it achieves promising results on all the datasets with the same conﬁguration.
4.3 Experimental Results
NTU RGB+D Dataset. This dataset has two standard evaluation protocols [32]. One is cross-subject evaluation, for which half of the subjects are used for training and the remaining are for testing. The second is cross-view evaluation, for which two viewpoints are used for training and one is left out for testing.
The results are shown in Table 1. Deep RNN and deep LSTM models concatenate the joints features at each frame and then feed them to the network to model the temporal dynamics and ignore the spatial dynamics. As can be seen, both “ST-LSTM (Joint Chain)” and “ST-LSTM (Tree Traversal)” models outperform these methods by a notable margin.
It can also be observed that the trust gate brings signiﬁcant performance improvement, because the data acquired by Kinect is noisy and some joints are frequently occluded in this dataset.
A notable portion of samples of this dataset are captured from side view, as shown in Fig. 5. Based on the design of Kinect’s body tracking mechanism,

826 J. Liu et al.

Fig. 5. Example images with noisy skeletons from NTU RGB+D dataset.

side view skeletal data is less accurate than the front view. To further show the eﬀectiveness of trust gate, we analyze the performance using only the samples in side views. When using “ST-LSTM (Tree Traversal)”, the accuracy is 76.5 %, while “ST-LSTM (Tree Traversal) + Trust Gate” achieves 81.6 %. This indicates the proposed trust gate can eﬀectively handle severely noisy data.
To verify the eﬀectiveness of layer stacking, we decrease the network size by using only one ST-LSTM layer, and the accuracies drop to 65.5 % (crosssubject) and 77.0 % (cross-view). It indicates our two-layer stacked model has better representation strengths than a single-layer model.
The sensitivity of the proposed model to neural unit sizes and λ values are also evaluated and the results are depicted in Fig. 6. When trust gate is used, our model achieves better performance for all the λ values tested compared to the model without trust gate.
Finally, we evaluate the classiﬁcation performance on early stopping conditions by feeding the ﬁrst p (0 < p < 1) portion of the testing video to the trained network on cross-subject protocol. When setting p as 0.1, 0.2, ..., 1.0, the corresponding accuracies are 13.4 %, 21.6 %, 33.9 %, 46.6 %, 55.5 %, 61.1 %, 64.6 %, 66.7 %, 68.2 %, 69.2 %, respectively. We can ﬁnd that the results improve when a larger portion of video is fed.

Table 1. Experimental results (accuracies) on NTU RGB+D Dataset

Method

Cross subject Cross view

Lie group [7]

50.1 %

52.8 %

Skeletal quads [6]

38.6 %

41.4 %

Dynamic skeletons [61]

60.2 %

65.2 %

HBRNN [30]

59.1 %

64.0 %

Part-aware LSTM [32]

62.9 %

70.3 %

Deep RNN [32]

56.3 %

64.1 %

Deep LSTM [32]

60.7 %

67.3 %

ST-LSTM (Joint Chain)

61.7 %

75.5 %

ST-LSTM (Tree Traversal)

65.2 %

76.1 %

ST-LSTM (Tree Traversal) + Trust Gate 69.2 %

77.7 %

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 827
Fig. 6. (a) Comparison of the performance for diﬀerent neuron size (d) values on NTU RGB+D dataset (cross-subject). (b) Comparison of diﬀerent λ values on NTU RGB+D dataset (cross-subject). The blue line indicates the results when diﬀerent λ values are used for trust gate, and the red dashed line indicates the performance when trust gate is not added. (Color ﬁgure online)
SBU Interaction Dataset. We follow the standard experimental protocol of [59] and perform 5-fold cross validation on SBU Interaction Dataset. In this dataset, two human skeletons are provided in each frame, so our traversal visits the joints throughout the two skeletons over the spatial steps. We summarize the results in terms of average classiﬁcation accuracy in Table 2. In the table, [51] and [30] are both LSTM-based methods, which are more relevant to our model.
As can be seen, the proposed “ST-LSTM (Tree Traversal) + Trust Gate” model outperforms all other skeleton-based methods. “ST-LSTM (Tree Traversal)” yields higher accuracy than “ST-LSTM (Joint Chain)”, as the latter adds some unreasonable links between the less related joints.
It is worth noting that deep LSTM [51], Co-occurrence LSTM [51], and HBRNN [30] all use the Svaitzky-Golay ﬁlter in temporal domain to smooth the skeleton joint positions to reduce the inﬂuence of the noise in the data captured by Kinect. However, even without trust gate (which aims at handling noisy input), the “ST-LSTM (Tree Traversal)” model outperforms HBRNN and deep LSTM, and achieves comparable result (88.6 %) to Co-occurrence LSTM. Once the trust gate is utilized, the accuracy jumps to 93.3 %. We do not adopt any skeleton normalization operation, such as translation or rotation of the skeleton [7], and achieve state-of-the-art performance. We notice that [62] obtained very similar result (93.4 %) on SBU dataset. However, their method utilized both RGB and depth images, while our method just uses the skeleton data.
UT-Kinect Dataset. There are two popular protocols on this dataset. First is the leave-one-out-cross-validation protocol [5]. Second is proposed in [63], for which half of the subjects are used for training and the remaining are used for testing. We use both protocols to evaluate the proposed method more extensively.
On the ﬁrst protocol, our model achieves superior performance over other skeleton-based methods by a large margin, as shown in Table 3. On the second evaluation protocol (Table 4), our model achieves competitive result (95.0 %) to Elastic functional coding [68] (94.9 %).
Berkeley MHAD. We follow the protocol in [30] on MHAD dataset, in which 384 sequences corresponding to the ﬁrst 7 subjects are used for training and the 275 sequences of the remaining 5 subjects are used for testing. The results are

828 J. Liu et al.

Table 2. Experimental results on SBU interaction dataset

Method

Acc

Yun et al., [59]

80.3 %

Ji et al., [64]

86.9 %

CHARM [65]

83.9 %

HBRNN [30] (reported by [51]) 80.4 %

Co-occurrence LSTM [51]

90.4 %

Deep LSTM (reported by [51]) 86.0 %

ST-LSTM (Joint Chain)

84.7 %

ST-LSTM (Tree)

88.6 %

ST-LSTM (Tree) + Trust Gate 93.3 %

Table 3. Results on UT-kinect dataset (leave-one-out-cross-validation protocol [5])

Method

Acc

Histogram of 3D Joints [5]

90.9 %

Grassmann Manifold [66]

88.5 %

Riemannian Manifold [67]

91.5 %

ST-LSTM (Joint Chain)

91.0 %

ST-LSTM (Tree)

92.4 %

ST-LSTM (Tree) + Trust Gate 97.0 %

shown in Table 5. Our method achieves the accuracy of 100 % without preliminary smoothing operations, which are adopted in [30].
Besides, we have tested our model on MSR Action3D dataset [69] following the protocol in [30], and achieved an accuracy of 94.8 %, which is slightly superior to 94.5 % achieved by HBRNN [30].
4.4 Eﬀectiveness of Trust Gate
To better study the eﬀectiveness of the trust gate in the proposed network model, we speciﬁcally evaluate noisy samples from MSR Action3D dataset. We manually rectify some noisy joints of these samples by referring to the corresponding depth maps, and compared the activations of the trust gates on noisy and rectiﬁed inputs. As shown in Fig. 7(a), the activation of the trust gate is smaller when a noisy joint is fed, compared to the corresponding rectiﬁed joint. This shows how the network reduces the impact of the noisy input data.

Fig. 7. Behavior of trust gate when inputting noisy data. (a) j3 is a noisy joint location, and j3 is the corresponding rectiﬁed joint position. In the histogram, the blue bar is the magnitude of the trust gate when inputting the noisy joint j3 . The red bar is the magnitude of the corresponding trust gate when j3 is rectiﬁed to j3. (b) The diﬀerence between the trust gate calculated when inputting the original data and that calculated
when the noise is imposed at the jN -th spatial step and tN -th time step. (Color ﬁgure online)

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 829

Table 4. Experimental results on UTKinect Dataset (half-vs-half protocol [63])

Method

Acc

Skeleton Joint Features [63] 87.9 %

Lie Group [7] (reported by [68]) 93.6 %

Elastic functional coding [68] 94.9 %

ST-LSTM (Tree) + Trust Gate 95.0 %

Table 5. Experimental results on MHAD Dataset

Method

Acc

Vantigodi et al. [70]

96.1 %

Oﬂi et al. [71]

95.4 %

Vantigodi et al. [72]

97.6 %

Kapsouras et al. [73]

98.2 %

HBRNN [30]

100 %

ST-LSTM (Tree) + Trust Gate 100 %

To comprehensively evaluate the trust gate, we also manually add noise to one joint for all testing samples on MHAD dataset. Note that MHAD dataset was captured with motion capture system, thus the skeletal joints are much more accurate than those collected by Kinect. We add noise to the right foot joint by moving the joint away from the original position. The direction of the translation vector is randomly chosen and the norm is also a random value around 30 cm (this is a signiﬁcant noise in the scale of human bodies). For each video, we add noise to the same joint at the same time step, and then analyze the eﬀect in average.
We measure the diﬀerence in the magnitude of the trust gate activations between the original data and the noisy ones. For all the testing samples, we perform the same procedure, then calculate the average diﬀerence. The result is depicted in Fig. 7(b). We can see when the noisy data is fed to the network, the magnitude of the trust gate is reduced. This shows how the network ignores the noisy input, and tries to prevent it from aﬀecting the network. In this experiment, we observe the overall accuracy does not drop after adding the noise.
5 Conclusion
In this paper we propose to extend the RNN-based 3D action recognition to spatio-temporal domain. A new ST-LSTM network is introduced which analyses the 3D location of each individual joint in each video frame, at each processing step. For better representation of the structured input to the network, a skeleton tree traversal algorithm is proposed which takes the adjacency graph of body joints into account and improves the performance of the network by arranging the most related joints together in the input sequence. Due to the unreliability of the 3D input data, a new gating mechanism is also proposed to improve the robustness of the network against noise and occlusion. The provided experimental results validate the proposed contributions and prove the eﬀectiveness of our method by achieving superior performance over the existing state-of-the-art methods on four evaluated datasets.

830 J. Liu et al.
Acknowledgement. The research is supported by Singapore Ministry of Education (MOE) Tier 2 ARC28/14, and Singapore A*STAR Science and Engineering Research Council PSF1321202099. This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at Nanyang Technological University. The ROSE Lab is supported by the National Research Foundation, Singapore, under its Interactive Digital Media (IDM) Strategic Research Programme. We also would like to thank NVIDIA for the GPU donation.
References
1. Presti, L.L., La Cascia, M.: 3d skeleton-based human action classiﬁcation: a survey. PR 53, 130–147 (2016)
2. Han, F., Reily, B., Hoﬀ, W., Zhang, H.: Space-time representation of people based on 3d skeletal data: a review. arXiv (2016)
3. Zhu, F., Shao, L., Xie, J., Fang, Y.: From handcrafted to learned representations for human action recognition: a survey. IVC (2016, in press)
4. Yang, X., Tian, Y.: Eﬀective 3d action recognition using eigenjoints. JVCIR 25, 2–11 (2014)
5. Xia, L., Chen, C., Aggarwal, J.: View invariant human action recognition using histograms of 3d joints. In: CVPRW (2012)
6. Evangelidis, G., Singh, G., Horaud, R.: Skeletal quads: Human action recognition using joint quadruples. In: ICPR (2014)
7. Vemulapalli, R., Arrate, F., Chellappa, R.: Human action recognition by representing 3d skeletons as points in a lie group. In: CVPR (2014)
8. Luo, J., Wang, W., Qi, H.: Group sparsity and geometry constrained dictionary learning for action recognition from depth maps. In: ICCV (2013)
9. Ohn-Bar, E., Trivedi, M.: Joint angles similarities and hog2 for action recognition. In: CVPRW (2013)
10. Mikolov, T., Kombrink, S., Burget, L., Cˇ ernocky`, J.H., Khudanpur, S.: Extensions of recurrent neural network language model. In: ICASSP (2011)
11. Sundermeyer, M., Schlu¨ter, R., Ney, H.: LSTM neural networks for language modeling. In: INTERSPEECH (2012)
12. Mesnil, G., He, X., Deng, L., Bengio, Y.: Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding. In: INTERSPEECH (2013)
13. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: a neural image caption generator. In: CVPR (2015)
14. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: neural image caption generation with visual attention. In: ICML (2015)
15. Yue-Hei Ng, J., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O., Monga, R., Toderici, G.: Beyond short snippets: deep networks for video classiﬁcation. In: CVPR (2015)
16. Srivastava, N., Mansimov, E., Salakhudinov, R.: Unsupervised learning of video representations using LSTMS. In: ICML (2015)
17. Singh, B., Marks, T.K., Jones, M., Tuzel, O., Shao, M.: A multi-stream bidirectional recurrent neural network for ﬁne-grained action detection. In: CVPR (2016)
18. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: deep learning on spatio-temporal graphs. In: CVPR (2016)

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 831
19. Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., Savarese, S.: Social LSTM: Human trajectory prediction in crowded spaces. In: CVPR (2016)
20. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition. In: CVPR (2016)
21. Ibrahim, M.S., Muralidharan, S., Deng, Z., Vahdat, A., Mori, G.: A hierarchical deep temporal model for group activity recognition. In: CVPR (2016)
22. Ma, S., Sigal, L., Sclaroﬀ, S.: Learning activity progression in LSTMS for activity detection and early detection. In: CVPR (2016)
23. Ni, B., Yang, X., Gao, S.: Progressively parsing interactional objects for ﬁne grained action detection. In: CVPR (2016)
24. Li, Y., Lan, C., Xing, J., Zeng, W., Yuan, C., Liu, J.: Online human action detection using joint classiﬁcation-regression recurrent neural networks. arXiv (2016)
25. Varior, R.R., Shuai, B., Lu, J., Xu, D., Wang, G.: A siamese long short-term memory architecture for human re-identiﬁcation. In: ECCV (2016)
26. Varior, R.R., Haloi, M., Wang, G.: Gated siamese convolutional neural network architecture for human re-identiﬁcation. In: ECCV (2016)
27. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: CVPR (2015)
28. Li, Q., Qiu, Z., Yao, T., Mei, T., Rui, Y., Luo, J.: Action recognition by learning deep multi-granular spatio-temporal video representation. In: ICMR (2016)
29. Wu, Z., Wang, X., Jiang, Y.G., Ye, H., Xue, X.: Modeling spatial-temporal clues in a hybrid deep learning framework for video classiﬁcation. In: ACM MM (2015)
30. Du, Y., Wang, W., Wang, L.: Hierarchical recurrent neural network for skeleton based action recognition. In: CVPR (2015)
31. Veeriah, V., Zhuang, N., Qi, G.J.: Diﬀerential recurrent neural networks for action recognition. In: ICCV (2015)
32. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: A large scale dataset for 3d human activity analysis. In: CVPR (2016)
33. Wang, J., Liu, Z., Wu, Y., Yuan, J.: Learning actionlet ensemble for 3d human action recognition. In: TPAMI (2014)
34. Meng, M., Drira, H., Daoudi, M., Boonaert, J.: Human-object interaction recognition by learning the distances between the object and the skeleton joints. In: FG (2015)
35. Shahroudy, A., Ng, T.T., Yang, Q., Wang, G.: Multimodal multipart learning for action recognition in depth videos. In: TPAMI (2016)
36. Wang, J., Wu, Y.: Learning maximum margin temporal warping for action recognition. In: ICCV (2013)
37. Rahmani, H., Mahmood, A., Huynh, D.Q., Mian, A.: Real time action recognition using histograms of depth gradients and random decision forests. In: WACV (2014)
38. Shahroudy, A., Wang, G., Ng, T.T.: Multi-modal feature fusion for action recognition in RGB-D sequences. In: ISCCSP (2014)
39. Wang, C., Wang, Y., Yuille, A.L.: Mining 3d key-pose-motifs for action recognition. In: CVPR (2016)
40. Rahmani, H., Mian, A.: Learning a non-linear knowledge transfer model for crossview action recognition. In: CVPR (2015)
41. Lillo, I., Carlos Niebles, J., Soto, A.: A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets. In: CVPR (2016)

832 J. Liu et al.
42. Hu, J.F., Zheng, W.S., Ma, L., Wang, G., Lai, J.: Real-time RGB-D activity prediction by soft regression. In: ECCV (2016)
43. Chen, C., Jafari, R., Kehtarnavaz, N.: Fusion of depth, skeleton, and inertial data for human action recognition. In: ICASSP (2016)
44. Rahmani, H., Mian, A.: 3d action recognition from novel viewpoints. In: CVPR (2016)
45. Liu, Z., Zhang, C., Tian, Y.: 3d-based deep convolutional neural network for action recognition with depth sequences. IVC (2016, in press)
46. Cai, X., Zhou, W., Wu, L., Luo, J., Li, H.: Eﬀective active skeleton representation for low latency human action recognition. TMM 18, 141–154 (2016)
47. Al Alwani, A.S., Chahir, Y.: Spatiotemporal representation of 3d skeleton jointsbased action recognition using modiﬁed spherical harmonics. PR Lett. (2016, in press)
48. Tao, L., Vidal, R.: Moving poselets: A discriminative and interpretable skeletal motion representation for action recognition. In: ICCVW (2015)
49. Shahroudy, A., Ng, T.T., Gong, Y., Wang, G.: Deep multimodal feature analysis for action recognition in RGB+D videos. arXiv (2016)
50. Du, Y., Fu, Y., Wang, L.: Representation learning of temporal dynamics for skeleton-based action recognition. TIP 25, 3010–3022 (2016)
51. Zhu, W., Lan, C., Xing, J., Zeng, W., Li, Y., Shen, L., Xie, X.: Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks. In: AAAI (2016)
52. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overﬁtting. JMLR 15, 1929–1958 (2014)
53. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9, 1735–1780 (1997)
54. Graves, A.: Supervised sequence labelling. In: Graves, A. (ed.) Supervised Sequence Labelling with Recurrent Neural Networks. SCI, vol. 385, pp. 5–13. Springer, Heidelberg (2012)
55. Zou, B., Chen, S., Shi, C., Providence, U.M.: Automatic reconstruction of 3d human motion pose from uncalibrated monocular video sequences based on markerless human motion tracking. PR 42, 1559–1571 (2009)
56. Yang, Y., Ramanan, D.: Articulated pose estimation with ﬂexible mixtures-ofparts. In: CVPR (2011)
57. Graves, A., Mohamed, A.r., Hinton, G.: Speech recognition with deep recurrent neural networks. In: ICASSP (2013)
58. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: NIPS (2014)
59. Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras, D.: Two-person interaction detection using body-pose features and multiple instance learning. In: CVPRW (2012)
60. Oﬂi, F., Chaudhry, R., Kurillo, G., Vidal, R., Bajcsy, R.: Berkeley MHAD: a comprehensive multimodal human action database. In: WACV (2013)
61. Hu, J.F., Zheng, W.S., Lai, J., Zhang, J.: Jointly learning heterogeneous features for RGB-D activity recognition. In: CVPR (2015)
62. Lin, L., Wang, K., Zuo, W., Wang, M., Luo, J., Zhang, L.: A deep structured model with radius-margin bound for 3d human activity recognition. IJCV 118, 256–273 (2015)
63. Zhu, Y., Chen, W., Guo, G.: Fusing spatiotemporal features and joints for 3d action recognition. In: CVPRW (2013)

Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition 833
64. Ji, Y., Ye, G., Cheng, H.: Interactive body part contrast mining for human interaction recognition. In: ICMEW (2014)
65. Li, W., Wen, L., Choo Chuah, M., Lyu, S.: Category-blind human action recognition: a practical recognition system. In: ICCV (2015)
66. Slama, R., Wannous, H., Daoudi, M., Srivastava, A.: Accurate 3d action recognition using learning on the grassmann manifold. PR 48, 556–567 (2015)
67. Devanne, M., Wannous, H., Berretti, S., Pala, P., Daoudi, M., Del Bimbo, A.: 3-d human action recognition by shape analysis of motion trajectories on riemannian manifold. IEEE Trans. Cybern. 45, 1340–1352 (2015)
68. Anirudh, R., Turaga, P., Su, J., Srivastava, A.: Elastic functional coding of human actions: from vector-ﬁelds to latent variables. In: CVPR (2015)
69. Li, W., Zhang, Z., Liu, Z.: Action recognition based on a bag of 3d points. In: CVPRW (2010)
70. Vantigodi, S., Babu, R.V.: Real-time human action recognition from motion capture data. In: NCVPRIPG (2013)
71. Oﬂi, F., Chaudhry, R., Kurillo, G., Vidal, R., Bajcsy, R.: Sequence of the most informative joints (SMIJ): a new representation for human skeletal action recognition. JVCIR 25, 24–38 (2014)
72. Vantigodi, S., Radhakrishnan, V.B.: Action recognition from motion capture data using meta-cognitive RBF network classiﬁer. In: ISSNIP (2014)
73. Kapsouras, I., Nikolaidis, N.: Action recognition on motion capture data using a dynemes and forward diﬀerences representation. JVCIR 25, 1432–1445 (2014)

