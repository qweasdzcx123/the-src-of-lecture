1
One Pixel Attack for Fooling Deep Neural Networks
Jiawei Su*, Danilo Vasconcellos Vargas* and Kouichi Sakurai

arXiv:1710.08864v7 [cs.LG] 17 Oct 2019

Abstract—Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modiﬁed. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a blackbox attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% conﬁdence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate lowcost adversarial attacks against neural networks for evaluating robustness.
Index Terms—Differential Evolution, Convolutional Neural Network, Information Security, Image Recognition.

I. INTRODUCTION
I N the domain of image recognition, DNN-based approach has outperform traditional image processing techniques, achieving even human-competitive results [25]. However, several studies have revealed that artiﬁcial perturbations on natural images can easily make DNN misclassify and accordingly proposed effective algorithms for generating such samples called “adversarial images” [7][11][18][24]. A common idea for creating adversarial images is adding a tiny amount of well-tuned additive perturbation, which is expected to be imperceptible to human eyes, to a correctly classiﬁed natural image. Such modiﬁcation can cause the classiﬁer to label the modiﬁed image as a completely different class. Unfortunately, most of the previous attacks did not consider extremely limited scenarios for adversarial attacks, namely the modiﬁcations might be excessive (i.e., the amount of modiﬁed pixels is fairly large) such that it may be perceptible to human eyes (see Figure 3 for an example). Additionally, investigating adversarial images created under extremely limited scenarios might give
Authors are with the Graduate School/Faculty of Information Science and Electrical Engineering, Kyushu University, Japan. The third author is also afﬁliated to Advanced Telecommunications Research Institute International (ATR).
The ofﬁcial version of this article has been published in IEEE Transactions on Evolutionary Computation [65], which can be accessed through the following link: https://ieeexplore.ieee.org/abstract/document/8601309
*Both authors have equal contribution.

Fig. 1. One-pixel attacks created with the proposed algorithm that successfully fooled three types of DNNs trained on CIFAR-10 dataset: The All convolutional network (AllConv), Network in network (NiN) and VGG. The original class labels are in black color while the target class labels and the corresponding conﬁdence are given below.
new insights about the geometrical characteristics and overall behavior of DNN’s model in high dimensional space [9]. For example, the characteristics of adversarial images close to the decision boundaries can help describing the boundaries’ shape.
In this paper, by perturbing only one pixel with differential evolution, we propose a black-box DNN attack in a scenario where the only information available is the probability labels (Figure 1 and 2) Our proposal has mainly the following advantages compared to previous works:

2

Fig. 2. One-pixel attacks on ImageNet dataset where the modiﬁed pixels are highlighted with red circles. The original class labels are in black color while the target class labels and their corresponding conﬁdence are given below.

Fig. 3. An illustration of the adversarial images generated by using Jacobian saliency-map approach [18]. The perturbation is conducted on about 4% of the total pixels and can be obvious to human eyes. Since the adversarial pixel perturbation has become a common way of generating adversarial images, such abnormal “noise” might be recognized with expertise.

• Effectiveness - On Kaggle CIFAR-10 dataset, being able to launch non-targeted attacks by only modifying one pixel on three common deep neural network structures with 68.71%, 71.66% and 63.53% success rates. We additionally ﬁnd that each natural image can be perturbed to 1.8, 2.1 and 1.5 other classes. On the original CIFAR10 dataset with a more limited attack scenario, we show 22.60%, 35.20% and 31.40% success rates. On ImageNet dataset, non-targeted attacking the BVLC AlexNet model also by changing one pixel shows that 16.04% of the test images can be attacked.
• Semi-Black-Box Attack - Requires only black-box feedback (probability labels) but no inner information of target DNNs such as gradients and network structures. Our method is also simpler than existing approaches since it does not abstract the problem of searching perturbation to any explicit target functions but directly focus on increasing the probability label values of the target classes.
• Flexibility - Can attack more types of DNNs (e.g., networks that are not differentiable or when the gradient calculation is difﬁcult).
Regarding the extremely limited one-pixel attack scenario, there are several main reasons why we consider it:
• Analyze the Vicinity of Natural Images - Geometrically, several previous works have analyzed the vicinity of natural images by limiting the length of perturbation vector. For example, the universal perturbation adds small value to each pixel such that it searches the adversarial images in a sphere region around the natural image [14]. On the other side, the proposed few-pixel perturbations can be regarded as cutting the input space using very lowdimensional slices, which is a different way of exploring the features of high dimensional DNN input space.

Among them, one-pixel attack is an extreme case of several-pixel attack. Theoretically, it can give geometrical insight to the understanding of CNN input space, in contrast to another extreme case: universal adversarial perturbation [14] that modiﬁes every pixel. • A Measure of Perceptiveness - The attack can be effective for hiding adversarial modiﬁcation in practice. To the best of our knowledge, none of the previous works can guarantee that the perturbation made can be completely imperceptible. A direct way of mitigating this problem is to limit the amount of modiﬁcations to as few as possible. Speciﬁcally, instead of theoretically proposing additional constraints or considering more complex cost functions for conducting perturbation, we propose an empirical solution by limiting the number of pixels that can be modiﬁed. In other words, we use the number of pixels as units instead of length of perturbation vector to measure the perturbation strength and consider the worst case which is one-pixel modiﬁcation, as well as two other scenarios (i.e. 3 and 5 pixels) for comparison.
II. RELATED WORKS
The security problem of DNN has become a critical topic [1][2]. C. Szegedy et al. ﬁrst revealed the sensitivity to well-tuned artiﬁcial perturbation [24] which can be crafted by several gradient-based algorithms using back-propagation for obtaining gradient information [11][24]. Speciﬁcally, I.J.Goodfellow et al. proposed “fast gradient sign” algorithm for calculating effective perturbation based on a hypothesis in which the linearity and high-dimensions of inputs are the main reason that a broad class of networks are sensitive to small perturbation [11]. S.M. Moosavi-Dezfooli et al. proposed

3

a greedy perturbation searching method by assuming the linearity of DNN decision boundaries [7]. In addition, N. Papernot et al. utilize Jacobian matrix to build “Adversarial Saliency Map” which indicates the effectiveness of conducting a ﬁxed length perturbation through the direction of each axis [18][20]. Except adversarial perturbation, there are other ways of creating adversarial images to make the DNN misclassify, such as artiﬁcial image [16] and rotation [36]. Besides, adversarial perturbation can be also possible in other domains such as speech recognition [33], natural language processing [34] and malware classiﬁcation [35].
A number of detection and defense methods have been also proposed to mitigate the vulnerability induced by adversarial perturbation [39]. For instance, network distillation which was originally proposed for squeezing information of an network to a smaller one is found to be able to reduce the network sensitivity enhancing the robustness of the neural network [40]. Adversarial training [41] is proposed for adding adversarial images to the training data such that the robustness against known adversarial images can be improved. On the other side, some image processing methods are proved to be effective for detecting adversarial images. For example, B.Liang et al. show that noise reduction methods such as scalar quantization and spatial smoothing ﬁlter can be selectively utilized for mitigating the inﬂuence of adversarial perrturbation. By comparing the label of an image before and after the transformation the perturbation can be detected [43]. The method works well on detecting adversarial images with both low and high entropy. Similarly, W. Xu et al. show that squeezing color bits and local/non-local spatial smoothing can have high success rate on distinguishing adversarial images [42]. However, recent studies show that many of these defense and detection methods can be effectively evaded by conducting little modiﬁcation on the original attacks [45], [46], [60].
Several black-box attacks that require no internal knowledge about the target systems such as gradients, have also been proposed [5][17][15]. In particular, to the best of our knowledge, the only work before ours that ever mentioned using one-pixel modiﬁcation to change class labels is carried out by N. Narodytska et al[15]. However, differently from our work, they only utilized it as a starting point to derive a further semi black-box attack which needs to modify more pixels (e.g., about 30 pixels out of 1024) without considering the scenario of one-pixel attack. In addition, they have neither measured systematically the effectiveness of the attack nor obtained quantitative results for evaluation. An analysis of the one-pixel attack’s geometrical features as well as further discussion about its implications are also lacking.
There have been many efforts to understand DNN by visualizing the activation of network nodes [28] [29][30]while the geometrical characteristics of DNN boundary have gained less attraction due to the difﬁculty of understanding highdimensional space. However, the robustness evaluation of DNN with respect to adversarial perturbation might shed light in this complex problem [9]. For example, both natural and random images are found to be vulnerable to adversarial perturbation. Assuming these images are evenly distributed, it suggests that most data points in the input space are gathered

near to the boundaries [9]. In addition, A. Fawzi et al. revealed more clues by conducting a curvature analysis. Their conclusion is that the region along most directions around natural images are ﬂat with only few directions where the space is curved and the images are sensitive to perturbation[10]. Interestingly, universal perturbations (i.e. a perturbation that when added to any natural image can generate adversarial images with high effectiveness) were shown possible and to achieve a high effectiveness when compared to random perturbation. This indicates that the diversity of boundaries might be low while the boundaries’ shapes near different data points are similar [14].
III. METHODOLOGY
A. Problem Description
Generating adversarial images can be formalized as an optimization problem with constraints. We assume an input image can be represented by a vector in which each scalar element represents one pixel. Let f be the target image classiﬁer which receives n-dimensional inputs, x = (x1, .., xn) be the original natural image correctly classiﬁed as class t. The probability of x belonging to the class t is therefore ft(x). The vector e(x) = (e1, .., en) is an additive adversarial perturbation according to x, the target class adv and the limitation of maximum modiﬁcation L. Note that L is always measured by the length of vector e(x). The goal of adversaries in the case of targeted attacks is to ﬁnd the optimized solution e(x)∗ for the following question:
maximize fadv(x + e(x))
e(x)∗
subject to e(x) ≤ L
The problem involves ﬁnding two values: (a) which dimensions that need to be perturbed and (b) the corresponding strength of the modiﬁcation for each dimension. In our approach, the equation is slightly different:
maximize fadv(x + e(x))
e(x)∗
subject to e(x) 0 ≤ d,
where d is a small number. In the case of one-pixel attack d = 1. Previous works commonly modify a part of all dimensions while in our approach only d dimensions are modiﬁed with the other dimensions of e(x) left to zeros.
The one-pixel modiﬁcation can be seen as perturbing the data point along a direction parallel to the axis of one of the n dimensions. Similarly, the 3 (5)-pixel modiﬁcation moves the data points within 3 (5)-dimensional cubes. Overall, fewpixel attack conducts perturbations on the low-dimensional slices of input space. In fact, one-pixel perturbation allows the modiﬁcation of an image towards a chosen direction out of n possible directions with arbitrary strength. This is illustrated in Figure 4 for the case when n = 3.
Thus, usual adversarial images are constructed by perturbating all pixels with an overall constraint on the strength of accumulated modiﬁcation[8][14] while the few-pixel attack considered in this paper is the opposite which speciﬁcally

4

Fig. 4. An illustration of using one and two-pixel perturbation attack in a 3-dimensional input space (i.e. the image has three pixels). The green point (sphere) denotes a natural image. In the case of one-pixel perturbation, the search space is the three perpendicular lines that intersect at point of natural image, which are denoted by red and black stripes. For two-pixel perturbation, the search space is the three blue (shaded) two-dimensional planes. In summary, one and two-pixel attacks search the perturbation on respectively one and two dimensional slices of the original three dimensional input space.
focus on few pixels but does not limit the strength of modiﬁcation.
B. Differential Evolution
Differential evolution (DE) is a population based optimization algorithm for solving complex multi-modal optimization problems [23], [6]. DE belongs to the general class of evolutionary algorithms (EA). Moreover, it has mechanisms in the population selection phase that keep the diversity such that in practice it is expected to efﬁciently ﬁnd higher quality solutions than gradient-based solutions or even other kinds of EAs [4]. In speciﬁc, during each iteration another set of candidate solutions (children) is generated according to the current population (parents). Then the children are compared with their corresponding parents, surviving if they are more ﬁtted (possess higher ﬁtness value) than their parents. In such a way, only comparing the parent and his child, the goal of keeping diversity and improving ﬁtness values can be simultaneously achieved.
DE does not use the gradient information for optimizing and therefore does not require the objective function to be differentiable or previously known. Thus, it can be utilized on a wider range of optimization problems compared to gradient based methods (e.g., non-differentiable, dynamic, noisy, among others). The use of DE for generating adversarial images have the following main advantages:
• Higher probability of Finding Global Optima - DE is a meta-heuristic which is relatively less subject to local minima than gradient descent or greedy search algorithms (this is in part due to diversity keeping mechanisms and the use of a set of candidate solutions). Moreover, the problem considered in this article has a strict constraint (only one pixel can be modiﬁed) making it relatively harder.

• Require Less Information from Target System - DE does not require the optimization problem to be differentiable as is required by classical optimization methods such as gradient descent and quasi-newton methods. This is critical in the case of generating adversarial images since 1) There are networks that are not differentiable, for instance [26]. 2) Calculating gradient requires much more information about the target system which can be hardly realistic in many cases.
• Simplicity - The approach proposed here is independent of the classiﬁer used. For the attack to take place it is sufﬁcient to know the probability labels.
There are many DE variations/improvements such as selfadaptive [3], multi-objective [27], among others. The current work can be further improved by taking these variations/improvements into account.
C. Method and Settings
We encode the perturbation into an array (candidate solution) which is optimized (evolved) by differential evolution. One candidate solution contains a ﬁxed number of perturbations and each perturbation is a tuple holding ﬁve elements: x-y coordinates and RGB value of the perturbation. One perturbation modiﬁes one pixel. The initial number of candidate solutions (population) is 400 and at each iteration another 400 candidate solutions (children) will be produced by using the usual DE formula:
xi(g + 1) = xr1(g) + F (xr2(g) − xr3(g)),
r1 = r2 = r3,
where xi is an element of the candidate solution, r1, r2, r3 are random numbers, F is the scale parameter set to be 0.5, g is the current index of generation. Once generated, each candidate solution compete with their corresponding parents according to the index of the population and the winner survive for next iteration. The maximum number of iteration is set to 100 and early-stop criterion will be triggered when the probability label of target class exceeds 90% in the case of targeted attacks on Kaggle CIFAR-10, and when the label of true class is lower than 5% in the case of non-targeted attacks on ImageNet. Then the label of true class is compared with the highest non-true class to evaluate if the attack succeeded. The initial population is initialized by using uniform distributions U (1, 32) for CIFAR-10 images and U (1, 227) for ImageNet images, for generating x-y coordinate (e.g., the image has a size of 32X32 in CIFAR-10 and for ImageNet we unify the original images with various resolutions to 227X227) and Gaussian distributions N (µ=128, σ=127) for RGB values. The ﬁtness function is simply the probabilistic label of the target class in the case of CIFAR-10 and the label of true class in the case of ImageNet. The crossover is not included in our scheme.
IV. EVALUATION AND RESULTS
The evaluation of the proposed attack method is based on CIFAR-10 and ImageNet datasets. We introduce several metrics to measure the effectiveness of the attacks:

5

• Success Rate - In the case of non-targeted attacks, it is deﬁned as the percentage of adversarial images that were successfully classiﬁed by the target system as an arbitrary target class. In the case of targeted attack, it is deﬁned as the probability of perturbing a natural image to a speciﬁc target class.
• Adversarial Probability Labels (Conﬁdence) - Accumulates the values of probability label of the target class for each successful perturbation, then divided by the total number of successful perturbations. The measure indicates the average conﬁdence given by the target system when mis-classifying adversarial images.
• Number of Target Classes - Counts the number of natural images that successfully perturb to a certain number (i.e. from 0 to 9) of target classes. In particular, by counting the number of images that can not be perturbed to any other classes, the effectiveness of non-targeted attack can be evaluated.
• Number of Original-Target Class Pairs - Counts the number of times each original-destination class pair was attacked.
A. Kaggle CIFAR-10
We train 3 types of common networks: All convolution network [22], Network in Network[13] and VGG16 network[21] as target image classiﬁers on CIFAR-10 dataset [12], [64]. The structures of the networks are described in Table 1, 2 and 3. The network setting were kept as similar as possible to the original with a few modiﬁcations in order to get the highest classiﬁcation accuracy. Both the scenarios of targeted and non-targeted attacks are considered. For each of the attacks on the three types of neural networks 500 natural images are randomly selected from the Kaggle CIFAR-10 test dataset to conduct the attack.
Note that we use the Kaggle CIFAR-10 test dataset [64] instead of the original one for this experiments. The dataset contains 300,000 cifar-10 images which can be visually inspected to have the following modiﬁcations: duplication, rotation, clipping, blurring, adding few random bad pixels and so on. However, the exact employed modiﬁcation algorithm is not released. This makes it a more practical dataset which simulates common scenarios that images can contain unknown random noise. We also show the results on the original CIFAR10 test dataset in Section 5 for comparison.
In addition, an experiment is conducted on the all convolution network [22] by generating 500 adversarial images with three and ﬁve pixel-modiﬁcation. The objective is to compare one-pixel attack with three and ﬁve pixel attacks. For each natural image, nine target attacks are launched trying to perturb it to the other 9 target classes. Note that we actually only launch targeted attacks and the effectiveness of non-targeted attack is evaluated based on targeted attack results. That is, if an image can be perturbed to at least one target class out of total 9 classes, the non-targeted attack on this image succeeds. Overall, it leads to the total of 36000 adversarial images created. To evaluate the effectiveness of the attacks, some established measures from the literature are used as well as some new kinds of measures are introduced:

conv2d layer(kernel=3, stride = 1, depth=96) conv2d layer(kernel=3, stride = 1, depth=96) conv2d layer(kernel=3, stride = 2, depth=96) conv2d layer(kernel=3, stride = 1, depth=192) conv2d layer(kernel=3, stride = 1, depth=192)
dropout(0.3) conv2d layer(kernel=3, stride = 2, depth=192) conv2d layer(kernel=3, stride = 2, depth=192) conv2d layer(kernel=1, stride = 1, depth=192) conv2d layer(kernel=1, stride = 1, depth=10)
average pooling layer(kernel=6, stride=1) ﬂatten layer
softmax classiﬁer
TABLE I ALL CONVOLUTION NETWORK
conv2d layer(kernel=5, stride = 1, depth=192) conv2d layer(kernel=1, stride = 1, depth=160) conv2d layer(kernel=1, stride = 1, depth=96)
max pooling layer(kernel=3, stride=2) dropout(0.5)
conv2d layer(kernel=5, stride = 1, depth=192) conv2d layer(kernel=5, stride = 1, depth=192) conv2d layer(kernel=5, stride = 1, depth=192)
average pooling layer(kernel=3, stride=2) dropout(0.5)
conv2d layer(kernel=3, stride = 1, depth=192) conv2d layer(kernel=1, stride = 1, depth=192) conv2d layer(kernel=1, stride = 1, depth=10)
average pooling layer(kernel=8, stride=1) ﬂatten layer
softmax classiﬁer
TABLE II NETWORK IN NETWORK
conv2d layer(kernel=3, stride = 1, depth=64) conv2d layer(kernel=3, stride = 1, depth=64)
max pooling layer(kernel=2, stride=2) conv2d layer(kernel=3, stride = 1, depth=128) conv2d layer(kernel=3, stride = 1, depth=128)
max pooling layer(kernel=2, stride=2) conv2d layer(kernel=3, stride = 1, depth=256) conv2d layer(kernel=3, stride = 1, depth=256) conv2d layer(kernel=3, stride = 1, depth=256)
max pooling layer(kernel=2, stride=2) conv2d layer(kernel=3, stride = 1, depth=512) conv2d layer(kernel=3, stride = 1, depth=512) conv2d layer(kernel=3, stride = 1, depth=512)
max pooling layer(kernel=2, stride=2) conv2d layer(kernel=3, stride = 1, depth=512) conv2d layer(kernel=3, stride = 1, depth=512) conv2d layer(kernel=3, stride = 1, depth=512)
max pooling layer(kernel=2, stride=2) ﬂatten layer
fully connected(size=2048) fully connected(size=2048)
softmax classiﬁer
TABLE III VGG16 NETWORK
B. ImageNet
For ImageNet we applied a non-targeted attack with the same DE parameter settings used on the CIFAR-10 dataset, although ImageNet has a search space 50 times larger than CIFAR-10. Note that we actually launch the non-targeted attack for ImageNet by using a ﬁtness function that aims to decrease the probability label of the true class. Different from CIFAR-10, whose effectiveness of non-targeted attack is calculated based on the targeted attack results carried out by using a ﬁtness function for increasing the probability of

6

target classes. Given the time constraints, we conduct the experiment without proportionally increasing the number of evaluations, i.e. we keep the same number of evaluations. Our tests are run over the BVLC AlexNet using 105 images from ILSVRC 2012 test set selected randomly for the attack. For ImageNet we only conduct one pixel attack because we want to verify if such a tiny modiﬁcation can fool images with larger size and if it is computationally tractable to conduct such attacks. The ILSVRC 2012 images are in lossy jpeg format with non-uniﬁed sizes. In order to reduce the practical interference to the evaluation as much as possible, we ﬁrst convert all target images from jpeg to png therefore during later processing it will be lossless. The images are further resized to 227X227 resolution for inputting to AlexNet (using nearest ﬁlter). Then we follow the same procedure to attacking CIFAR-10. Note that the discrepancy on pre-processing raw images (e.g., using center cropping instead of simple resizing) can inﬂuence the classiﬁcation performance of AlexNet and attack rate. Here we only show the result on one setting and leave the comprehensive evaluation of attacking AlexNet using difference pre-processing methods for future work.
C. Results
The success rates and adversarial probability labels for onepixel perturbations on three CIFAR-10 networks and BVLC network are shown in Table 4 and the three and ﬁve-pixel perturbations on Kaggle CIFAR-10 is shown in Table 5. The number of target classes is shown by Figure 5. The number of original-target class pairs is shown by the heat-maps of Figure 6 and 7. In addition to the number of original-target class pairs, the total number of times each class had an attack which either originated or targeted it is shown in Figure 8. Since only non-targeted attacks are launched on ImageNet, the “Number of target classes” and “Number of original-target class pairs” metrics are not included in the ImageNet results.
1) Success Rate and Adversarial Probability Labels (Targeted Attack Results): On Kaggle CIFAR-10, the success rates of one-pixel attacks on three types of networks show the generalized effectiveness of the proposed attack through different network structures. On average, each image can be perturbed to about two target classes for each network. In addition, by increasing the number of pixels that can be modiﬁed to three and ﬁve, the number of target classes that can be reached increases signiﬁcantly. By dividing the adversarial probability labels by the success rates, the conﬁdence values (i.e. probability labels of target classes) are obtained which are 79.39%, 79.17% and 77.09% respectively to one, three and ﬁve-pixel attacks.
On ImageNet, the results show that the one pixel attack generalizes well to large size images and fool the corresponding neural networks. In particular, there is 16.04% chance that an arbitrary ImageNet test image can be perturbed to a target class with 22.91% conﬁdence. Note that the ImageNet results are done with the same settings as CIFAR-10 while the resolution of images we use for the ImageNet test is 227x227, which is 50 times larger than CIFAR-10 (32x32). Notice that in each successful attack the probability label of the target class is the

OriginAcc Targeted Non-targeted Conﬁdence

AllConv 85.6% 19.82% 68.71% 79.40%

NiN 87.2% 23.15% 71.66% 75.02%

VGG16 83.3% 16.48% 63.53% 67.67%

BVLC 57.3%
– 16.04% 22.91%

TABLE IV RESULTS OF CONDUCTING ONE-PIXEL ATTACK ON FOUR DIFFERENT TYPES OF NETWORKS: ALL CONVOLUTIONAL NETWORK (ALLCONV), NETWORK IN NETWORK (NIN), VGG16 AND BVLC ALEXNET. THE ORIGINALACC IS THE ACCURACY ON THE NATURAL TEST DATASETS. TARGETED/NON-TARGETED INDICATE THE ACCURACY OF CONDUCTING TARGETED/NON-TARGETED ATTACKS. CONFIDENCE IS THE AVERAGE
PROBABILITY OF TARGET CLASSES.

Success rate(tar) Success rate(non-tar)
Rate/Labels

3 pixels 40.57% 86.53% 79.17%

5 pixels 44.00% 86.34% 77.09%

TABLE V RESULTS OF CONDUCTING THREE-PIXEL ATTACK ON ALLCONV NETWORKS AND FIVE-PIXEL ATTACK ON NETWORK IN NETWORK.

highest. Therefore, the conﬁdence of 22.91% is relatively low but tell us that the other remaining 999 classes are even lower to an almost uniform soft label distribution. Thus, the onepixel attack can break the conﬁdence of BVLC AlexNet to a nearly uniform soft label distribution. The low conﬁdence is caused by the fact that we utilized a non-targeted evaluation that only focuses on decreasing the probability of the true class. Other ﬁtness functions should give different results.
2) Number of Target Classes (Non-targeted Attack Results): Regarding the results shown in Figure 5, we ﬁnd that with only one-pixel modiﬁcation a fair amount of natural images can be perturbed to two, three and four target classes. By increasing the number of pixels modiﬁed, perturbation to more target classes becomes highly probable. In the case of non-targeted one-pixel attack, the VGG16 network got a slightly higher robustness against the proposed attack. This suggests that all three types of networks (AllConv network, NiN and VGG16) are vulnerable to this type of attack.
The results of attacks are competitive with previous nontargeted attack methods which need much more distortions (Table 6). It shows that using one dimensional perturbation vectors is enough to ﬁnd the corresponding adversarial images for most of the natural images. In fact, by increasing the number of pixels up to ﬁve, a considerable number of images can be simultaneously perturbed to eight target classes. In some rare cases, an image can go to all other target classes with one-pixel modiﬁcation, which is illustrated in Figure 9.
3) Original-Target Class Pairs: Some speciﬁc originaltarget class pairs are much more vulnerable than others (Figure 6 and 7). For example, images of cat (class 3) can be much more easily perturbed to dog (class 5) but can hardly reach the automobile (class 1). This indicates that the vulnerable target classes (directions) are shared by different data points that belong to the same class. Moreover, in the case of one-pixel attack, some classes are more robust than others since their data points can be relatively hard to perturb to other classes. Among these data points, there are points that can not be perturbed to any other classes. This indicates that the labels of

7

Fig. 5. The graphs shows the percentage of natural images that were successfully perturbed to a certain number (from 0 to 9) of target classes by using one, three or ﬁve-pixel perturbation. The vertical axis shows the percentage of images that can be perturbed while the horizontal axis indicates the number of target classes.

these points rarely change when going across the input space through n directions perpendicular to the axes. Therefore, the corresponding original classes are kept robust along these directions. However, it can be seen that such robustness can rather easily be broken by merely increasing the dimensions of perturbation from one to three and ﬁve because both success rates and number of target classes that can be reached increase when conducting higher-dimensional perturbations.
Additionally, it can also be seen that each heat-map matrix is approximately symmetric, indicating that each class has similar number of adversarial images which were crafted from these classes as well as to these classes (Figure 8). Having said that, there are some exceptions for example the class 8 (ship) when attacking NiN, the class 4 (deer) when attacking AllConv networks with one pixel, among others. In the ship class when attacking NiN networks, for example, it is relatively easy to craft adversarial images from them while it is relatively hard to craft adversarial images to them. Such unbalance is intriguing since it indicates the ship class is similar to most of the other

Fig. 6. Heat-maps of the number of times a successful attack is present with the corresponding original-target class pair in one, three and ﬁve-pixel attack cases. Red (vertical) and (horizontal) blue indices indicate respectively the original and target classes. The number from 0 to 9 indicates respectively the following classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.
classes like truck and airplane but not vice-versa. This might be due to (a) boundary shape and (b) how close are natural images to the boundary. In other words, if the boundary shape is wide enough it is possible to have natural images far away from the boundary such that it is hard to craft adversarial images from it. On the contrary, if the boundary shape is mostly long and thin with natural images close to the border, it is easy to craft adversarial images from them but hard to craft adversarial images to them.
In practice, such classes which are easy to craft adversarial images from may be exploited by malicious users which may make the whole system vulnerable. In the case here, however, the exceptions are not shared between the networks,

8

Fig. 7. Heat-maps for one-pixel attack on Network in network and VGG.

AvgEvaluation AvgDistortion

AllConv 16000 123

NiN 12400 133

VGG16 20000 145

BVLC 25600 158

TABLE VI COST OF CONDUCTING ONE-PIXEL ATTACK ON FOUR DIFFERENT TYPES
OF NETWORKS. AVGEVALUATION IS THE AVERAGE NUMBER OF EVALUATIONS TO PRODUCE ADVERSARIAL IMAGES. AVGDISTORTION IS THE REQUIRED AVERAGE DISTORTION IN ONE-CHANNEL OF A SINGLE
PIXEL TO PRODUCE ADVERSARIAL IMAGES.

revealing that whatever is causing the phenomenon is not shared. Therefore, for the current systems under the given attacks, such a vulnerability seems hard to be exploited.
4) Time complexity and average distortion: To evaluate the time complexity we use the number of evaluations which is a common metric in optimization. In the DE case the number of evaluations is equal to the population size multiplied by the number of generations. We also calculate the average distortion on the single pixel attacked by taking the average modiﬁcation on the three color channels, which is a more straight forward and explicit measure of modiﬁcation strength. We did not use the Lp norm due to its limited effectiveness of measuring perceptiveness [38]. The results of two metrics are shown in Table 7.
5) Comparing with Random One-Pixel Attack: We compare the proposed method with the random attack to evaluate if DE is truly helpful for conducting one-pixel non-targeted attack on Kaggle CIFAR-10 dataset, which is shown in Table 8.
Speciﬁcally, for each natural image, the random search repeats 100 times, each time randomly modiﬁes one random

Fig. 8. Number of successful attacks (vertical axis) for a speciﬁc class acting as the original (black) and target (gray) class. The horizontal axis indicates the index of each class which is the same as Figure 7.
pixel of the image with random RGB value to attempt to change its label. The conﬁdence of the attack with respect to one image is set to be the highest probability target class label of 100 attacks.
In this experiment, we use the same number of evaluations (80000) for both DE and random search. According to the comparison, the DE is superior to the random attack regarding attack accuracy, especially in the case of VGG16 network. Speciﬁcally, DE is 19.01%, 29.94% and 47.96% more efﬁ-

9
true class for each image. The goal of the attack is to minimize this ﬁtness value. According to the results, it can be seen that the ﬁtness values can occasionally drop abruptly between two generations while in other cases they decrease smoothly. Moreover, the average ﬁtness value decreases monotonically with the number of generations, showing that the evolution works as expected. We also ﬁnd that BVLC network is harder to fool due to the smaller decrease in ﬁtness values.

Fig. 9. A natural image of the dog class that can be perturbed to all other nine classes. The attack is conducted over the AllConv network using the proposed one pixel attack. The table in the bottom shows the class labels output by the target DNN, all with approximately 100% conﬁdence. This curious result further emphasize the difference and limitations of current methods when compared to human recognition.

DE success rate Conﬁdence
Random Search success rate Conﬁdence

AllConv 68.71% 79.40% 49.70% 87.73%

NiN 71.66% 75.02% 41.72% 75.83%

VGG16 63.53% 67.67% 15.57% 59.90%

TABLE VII A COMPARISON OF ATTACK RATE AND CONFIDENCE BETWEEN DE ONE-PIXEL ATTACK AND RANDOM ONE-PIXEL ATTACK (NON-TARGETED)
ON KAGGLE CIFAR-10 DATASET.

cient than random search respectively for All Convolutional Network, Network in Network and VGG16. Even with a less efﬁcient result, random search is shown to ﬁnd 49.70% and 41.72% of the time for respectively All Convolutional Network and Network in Network, therefore the vulnerable pixels that can change the image label signiﬁcantly are quite common. That seems not to be the case for VGG though in which random search achieves only 15.57%. DE has a similar accuracy in all of them showing also a better robustness.
6) Change in ﬁtness values: We run an experiment over different networks to examine how the ﬁtness changes during evolution. The 30 (15) curves come from 30 (15) random Kaggle CIFAR-10 (ImageNet) images successfully attacked by the proposed one-pixel attack (Figure 10). The ﬁtness values are, as previously described, set to be the probability label of the

Fig. 10. The change of ﬁtness values during 100 generations of evolution of images (non-targeted) attacked by the proposed method among different network structures. The average values are highlighted by red dotted lines.

10

Targeted Non-targeted1
Conﬁdence Non-targeted2
Conﬁdence

AllConv 3.41% 22.67% 54.58% 22.60% 56.57%

NiN 4.78% 32.00% 55.18% 35.20% 60.08%

VGG16 5.63% 30.33% 51.19% 31.40% 53.58%

TABLE VIII RESULTS OF CONDUCTING ONE-PIXEL ATTACK ON ORIGINAL CIFAR-10
TEST SET. NON-TARGETED1 INDICATES THE NON-TARGETED ATTACK
ACCURACY CALCULATED FROM TARGETED ATTACK RESULTS AND
NON-TARGETED2 INDICATES THE TRUE NON-TARGETED ATTACK ACCURACY. OTHER METRICS ARE THE SAME TO TABLE 4.

V. RESULTS ON ORIGINAL CIFAR-10 TEST DATA
We present another evaluation of one pixel attack which is on original CIFAR-10 test dataset [12]. Comparing to the results on Kaggle CIFAR-10 aforementioned, the scenario is more limited since the images contain much less practical noise. Therefore, the target CNNs can have higher classiﬁcation accuracy and conﬁdence which deﬁnitely makes the attack harder. Additionally, we only use images correctly classiﬁed by the target CNNs while in the experiment on Kaggle CIFAR10 set we use all images (i.e., which contain wrongly classiﬁed images) with their true labels predicted by the target CNNs.
We use 500 random images for non-targeted attack and 300 for targeted attack. We also make small modiﬁcation on network structure for better implementation. Speciﬁcally, for the Network in Network, we remove the second average pooling layers. For All convolutional network, we remove the batch normalization on the ﬁrst layer. Three CIFAR-10 networks are re-trained to have similar natural accuracy to Table 4. An early-stop criterion will be triggered when the probability label of the target class exceeds the original class. All other settings are kept the same. The attack results are shown by Table 8. The number of target classes is shown by Figure 11. The number of original-target class pairs is shown by the heat-maps of Figure 12 and Figure 13. In addition to the number of original-target class pairs, the total number of times each class had an attack which either originated or targeted it is shown in Figure 14 and Figure 15.
According to the attack results shown, we ﬁnd the following features of one-pixel attack on original CIFAR-10.
1. Attack rate: The three networks have higher robustness to one-pixel attack according to the lower attack rate and conﬁdence (Table 8). This might due to the higher classiﬁcation accuracy and conﬁdence of three networks on original CIFAR10 test-set. Similar to the results on Kaggle set, the network in network still gets the lowest overall robustness considering both attack rate and conﬁdence. This might be related to the proximity to the decision boundary. However, VGG network becomes much more vulnerable in this case. The discrepancy indicates that the robustness among different networks can be varied when handling images with low (e.g. Kaggle CIFAR10) and high (e.g., original CIFAR-10) conﬁdence.
2. Number of targeted classes: According to Figure 11, it can be seen that in the case of targeted attack, it is still quite common that a vulnerable image can be perturbed to more than one class. In other words, the image might be locate near to the boundaries to multiple classes, especially in the case of

Fig. 11. The percentage of natural images that were successfully perturbed to a certain number (from 0 to 9) of target classes by one pixel targeted attack.
VGG. This is similar to the Kaggle CIFAR-10 results shown by Fig 5.
Note that one image can be perturbed to a ﬁnal target class A through the original target class B (i.e. semi-successful targeted attack). For some images, the number of B can be more than one. We do not count it as a successful targeted attack unless A = B.
3. Original-target class pairs: In both cases of targeted and non-targeted attack we again found the existence of vulnerable original-target classes pairs such as dog (5th)-cat (3rd) (Figure 12 and Figure 13 ). In most cases, for a class pair between class A and B, the number of successful perturbation from A to B is similar to the number of B to A, which makes the heat-maps almost symmetric. However, there are exceptions such as ship (8th)-airplane (0th) pair, which the perturbation from ship to airplane class is very frequent but not vice versa.
Additionally, it also can be seen from Figure 14 and Figure 15, some vulnerable classes exist which have higher number of times being both original and target class of the attack. A vulnerable original class is probably also vulnerable being a target class to a similar extend.
Most of these features, together with the speciﬁc vulnerable class-pairs shown by Figure 12 and Figure 13 and vulnerable classes shown by Figure 14 and Figure 15, are similar or even exactly the same to the ﬁnding on attacking Kaggle CIFAR-10 dataset.
VI. DISCUSSION
A. Adversarial Perturbation
Previous results have shown that many data points might be located near to the decision boundaries [9]. For the analysis the data points were moved small steps in the input space

11

Success rate Conﬁdence Number of pixels Network

Method Our method Our method
LSA[15] LSA[15] FGSM[11] FGSM[11]

35.20% 31.40% 97.89% 97.98% 93.67% 90.93%

60.08% 53.58%
72% 77% 93% 90%

1 (0.098%) 1 (0.098%) 33 (3.24%) 30 (2.99%) 1024 (100%) 1024 (100%)

NiN VGG NiN VGG NiN VGG

TABLE IX COMPASSION OF NON-TARGETED ATTACK EFFECTIVENESS BETWEEN THE PROPOSED METHOD AND TWO PREVIOUS WORKS. THIS SUGGESTS THAT
ONE PIXEL IS ENOUGH TO CREATE ADVERSARIAL IMAGES FROM MOST OF
THE NATURAL IMAGES.

while quantitatively analyzing the frequency of change in the class labels. In this paper, we showed that it is also possible to move the data points along few dimension to ﬁnd points where the class labels change. Our results also suggest that the assumption made by I. J. Goodfellow et al. that small addictive perturbation on the values of many dimensions will accumulate and cause huge change to the output [11], might not be necessary for explaining why natural images are sensitive to small perturbation. Since we only changed one pixel to successfully perturb a considerable number of images.
According to the experimental results, the vulnerability of CNN exploited by the proposed one pixel attack is generalized through different network structures as well as different image sizes. In addition, the results shown here mimics an attacker and therefore uses a low number of DE iterations with a relatively small set of initial candidate solutions. Therefore, the perturbation success rates should improve further by having either more iterations or a bigger set of initial candidate solutions. Implementing more advanced algorithms such as Co-variance Matrix Adaptation Evolution Strategy [32] instead of DE might also achieve the same improvement. Additionally, the proposed algorithm and the widely vulnerable images (i.e. natural images that can be used to craft adversarial images to most of the other classes) collected might be useful for generating better artiﬁcial adversarial images in order to augment the training dataset. This aids the development of more robust models[19] which is left as future work.
B. Robustness of One-pixel Attack
Some recently proposed detection methods have shown high accuracy of detecting adversarial perturbation. For example, B.Liang et al. utilize noise reduction to effectively detect both high and low-entropy images (e.g., bigger images give high entropy values) [43]. In addition, W. Xu et al. show that squeezing color bits and local/non-local spatial smoothing can simultanously detect L0, L2 and L∞ attacks [42]. As the trade-off of being a low-cost, easy-implemented L0 attack, we do not expect one pixel attack can achieve signﬁcantly better robustness against such detection methods compared to other L0 attacks such as [32].

Fig. 12. Heat-maps of the number of times a successful attack is present with the corresponding original-target class pair, for targeted attacks.
However, such detection schemes add another layer of preprocessing which increases the response time of the system. For example, both [42] and [43] require image processing and re-classiﬁcation of the resulting images. Therefore they can be inefﬁcient when dealing with adversarial scenarios such as novelty detection on security camera and image recognition systems on autonomous driving applications which run in real time with high frame rate. Besides, the impact of preprocessing on the classiﬁcation accuracy is still not fully understood.
Detecting adversarial perturbation indeed can be helpful in practice. However, the fundamental problem is still left unsolved: the neural networks are still not able to recognize similar images as such, ignoring small adversarial perturbation. By proposing novel attack methods, we aim to emphasize the existence of different types of vulnerabilities and the corresponding understanding.

12

Fig. 13. Heat-maps of the number of times a successful attack is present with the corresponding original-target class pair, for non-targeted attacks.
VII. FUTURE WORK
The DE utilized in this research belongs to a big class of algorithms called evolutionary strategies [48] which includes other variants such as Adaptive DE [49] and Covariance matrix adaptation evolution strategy (CMA-ES) [50], [51], [52]. In fact, there are a couple of recent developments [61], [62], [63] in evolutionary strategies and related areas that could further improve the current method, allowing for more efﬁcient and accurate attacks.
Furthermore, evolutionary computation also provides some promising approaches to solve adversarial machine learning related vulnerabilities. In fact, evolutionary-based machine learning allows for a great ﬂexibility of models and may be an answer to the same problems it is revealing. First, in an area of evolutionary machine learning called neuroevolution, it was shown to be possible to learn not just the weights but also the topology of the network with evolutionary computation [26], [44], [53]. In fact, SUNA [26] goes beyond current neural models to propose a uniﬁed neuron model (e.g., time-scales, neuromodulation, feedback, long-term memory) that can adapt its structure and models to learn completely different problems

Fig. 14. Number of successful attacks (vertical axis) for a speciﬁc class acting as the original (black) and target (gray) class, for targeted attacks.
(including non-markov problems) without changing any of its hyper-parameters. This generality is currently surpassing most if not all deep learning algorithms. Last but not least, self-organizing and novelty-organizing classiﬁers can adapt to changes in the environment by using ﬂexible representations [54], [55], [56]. For example, they can adapt to mazes that change in shape and to problems where the scope of variables change throughout the experiment [57]: a very challenging scenario in which most if not all deep learning algorithms fail. These among other achievements [58], [59] show a promising path that may solve current problems in deep neural networks in the years to come.
Besides, it can be seen that the one-pixel attack can be potentially extended to other domains such as natural language processing, speech recognition, which will be also left for future work.
VIII. ACKNOWLEDGMENT
This research was partially supported by Collaboration Hubs for International Program (CHIRP) of SICORP, Japan Science and Technology Agency (JST), and Kyushu University Education and Research Center for Mathematical and Data Science Grant.

13
Fig. 15. Number of successful attacks (vertical axis) for a speciﬁc class acting as the original (black) and target (gray) class, for non-targeted attacks.

14

REFERENCES
[1] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar. The security of machine learning. Machine Learning, 81(2): pp.121–148, 2010.
[2] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pp.16–25. ACM, 2006.
[3] J. Brest, S. Greiner, B. Boskovic, M. Mernik, and V. Zumer. Selfadapting control parameters in differential evolution: A comparative study on numerical benchmark problems. IEEE transactions on evolutionary computation, 10(6): pp.646–657, 2006.
[4] P. Civicioglu and E. Besdok. A conceptual comparison of the cuckoosearch, particle swarm optimization, differential evolution and artiﬁcial bee colony algorithms. Artiﬁcial intelligence review, pp.1–32, 2013.
[5] H. Dang, Y. Huang, and E.-C. Chang. Evading classiﬁers by morphing in the dark. 2017.
[6] S. Das and P. N. Suganthan. Differential evolution: A survey of the state-of-the-art. IEEE transactions on evolutionary computation, 15(1): pp.4–31, 2011.
[7] S. M. Moosavi Dezfooli, F. Alhussein and F. Pascal. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.2574–2582, 2016.
[8] S. M. Moosavi Dezfooli, F. Alhussein, F. Omar, F. Pascal, and S. Stefano. Analysis of universal adversarial perturbations. arXiv preprint arXiv:1705.09554, 2017.
[9] A. Fawzi, S. M. Moosavi Dezfooli, and P. Frossard. The robustness of deep networks: A geometrical perspective. IEEE Signal Processing Magazine, 34(6): pp.50-62.
[10] A. Fawzi, S.-M. Moosavi-Dezfooli, P. Frossard, and S. Soatto. Classiﬁcation regions of deep neural networks. arXiv preprint arXiv:1705.09552, 2017.
[11] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
[12] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, (1)4: pp. 7, University of Toronto.2009.
[13] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
[14] S. M. Moosavi Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), number EPFLCONF-226156, 2017.
[15] N. Narodytska and S. Kasiviswanathan. Simple black-box adversarial attacks on deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp.1310–1318. IEEE, 2017.
[16] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.427–436, 2015.
[17] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp.506–519. ACM, 2017.
[18] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp.372–387. IEEE, 2016.
[19] A. Rozsa, E. M. Rudd, and T. E. Boult. Adversarial diversity and hard positive generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp.25–32, 2016.
[20] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
[21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[22] J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. In ICLR (workshop track).
[23] R. Storn and K. Price. Differential evolution–a simple and efﬁcient heuristic for global optimization over continuous spaces. Journal of global optimization, 11(4): pp.341–359, 1997.
[24] S. Christian, Z. Wojciech, S. Ilya, b. Joan, E. Dumitru, G. Ian, F. Rob. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
[25] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.1701–1708, 2014.

[26] D. V. Vargas and J. Murata. Spectrum-diverse neuroevolution with uniﬁed neural models. IEEE transactions on neural networks and learning systems, 28(8):pp.1759–1773, 2017.
[27] D. V. Vargas, J. Murata, H. Takano, and A. C. B. Delbem. General
subpopulation framework and taming the conﬂict inside populations. Evolutionary computation, 23(1):pp.1–36, 2015. [28] D. Wei, B. Zhou, A. Torrabla, and W. Freeman. Understanding intraclass knowledge inside cnn. arXiv preprint arXiv:1507.02379, 2015. [29] J. Yosinski, J. Clune, T. Fuchs, and H. Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579,
2015. [30] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional
networks. In European conference on computer vision, pp.818–833.
Springer, 2014. [31] J. Su, D. Vargas, and K. Sakurai. One pixel attack for fooling deep
neural networks. arXiv preprint arXiv:1710.08864, 2017. [32] N. Hansen. The CMA evolution strategy: a comparing review In Towards
a new evolutionary computation, pp.75–102. Springer, 2006. [33] A. Moustafa, B. Bharathan, S. Mani. Did you hear that? Adversarial
Examples Against Automatic Speech Recognition. arXiv preprint arXiv:1801.00554, 2018. [34] P. Nicolas, M. Patrick, S. Ananthram, H. Richard. Crafting Adversarial Input Sequences for Recurrent Neural Networks. arXiv preprint arXiv:1604.08275, 2016. [35] G. Kathrin, P. Nicolas, M. Praveen, B. Michael, M. Patrick. Adversarial
Perturbations Against Deep Neural Networks for Malware Classiﬁcation. arXiv preprint arXiv:1606.04435, 2016. [36] E. Logan, T. Brandon, T. Dimitris, S. Ludwig, M. Aleksander. A
Rotation and a Translation Sufﬁce: Fooling CNNs with Simple Transformations. arXiv preprint arXiv:1712.02779, 2017. [37] G. Kathrin, P. Nicolas, M. Praveen, B. Michael, M. Patrick. Adversarial
Perturbations Against Deep Neural Networks for Malware Classiﬁcation. arXiv preprint arXiv:1606.04435, 2016. [38] M. Sharif, L. Bauer, MK. Reiter On the Suitability of Lp-norms for Creating and Preventing Adversarial Examples. arXiv preprint arXiv:1802.09653, 2018. [39] X. Yuan, P. He, Q. Zhu, R. R. Bhat Adversarial Examples: Attacks and Defenses for Deep Learning. arXiv preprint arXiv:1712.07107, 2017. [40] N. Papernot, P. McDaniel, X. Wu, S. Jha, A. Swami Distillation as
a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE Symposium on Security and Privacy (SP), pp.1701–
1708. [41] R. Huang, B. Xu, D. Schuurmans, C. Szepesvri Learning with a strong
adversary. arXiv preprint arXiv:1511.03034, 2015. [42] W. Xu, D. Evans, Y. Qi Feature squeezing: Detecting adversarial
examples in deep neural networks arXiv preprint arXiv:1704.01155,
2017. [43] B. Liang et al. Detecting Adversarial Examples in Deep Networks with
Adaptive Noise Reduction. arXiv preprint arXiv:1705.08378, 2017. [44] K. O. Stanley, R. Miikkulainen. Evolving neural networks through
augmenting topologies. In Evolutionary Computation, pp.99–127. [45] N. Carlini, D. Wagner. Adversarial examples are not easily detected:
Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, pp.3–14. [46] N. Carlini, D. Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. [47] N. Carlini, D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP),
pp.39–57. [48] H.G. Beyer and H.P. Schwefel. Evolution strategiesA comprehensive
introduction. In Natural computing, pp.3–52. [49] A.K. Qin and P.N. Suganthan. Self-adaptive differential evolution
algorithm for numerical optimization. In The 2005 IEEE Congress on Evolutionary Computation, pp.1785–1791. [50] N. Hansen, S. D. Mller, and P. Koumoutsakos. Reducing the time
complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). In Evol. Comput, pp.1–18. [51] N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation
distributions in evolution strategies: The covariance matrix adaptation. In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, pp.312–317. [52] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. In Evol. Comput, pp.159–195. [53] D. Whitley, S. Dominic, R. Das and C.W. Anderson. Genetic reinforcement learning for neurocontrol problems. In Machine Learning,
pp.259–284. [54] D.V. Vargas, H. Takano and J. Murata. Self organizing classiﬁers:
ﬁrst steps in structured evolutionary machine learning. In Evolutionary Intelligence, pp.57–72. [55] D.V. Vargas, H. Takano and J. Murata. Self organizing classiﬁers and

15
niched ﬁtness. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pp.1109–1116. [56] D.V. Vargas, H. Takano and J. Murata. Novelty-organizing team of classiﬁers-a team-individual multi-objective approach to reinforcement learning. In Proceedings of the SICE Annual Conference (SICE), pp.1785–1792. [57] D.V. Vargas, H. Takano and J. Murata. Novelty-organizing team of classiﬁers in noisy and dynamic environments. In 2015 IEEE Congress on Evolutionary Computation (CEC), pp.2937–2944. [58] R.J. Urbanowicz and J.H. Moore. ExSTraCS 2.0: description and evaluation of a scalable learning classiﬁer system. In Evolutionary intelligence, pp.89–116. [59] I.M. Alvarez, W.N. Browne and M. Zhang. Compaction for code fragment based learning classiﬁer systems. In Australasian Conference on Artiﬁcial Life and Computational Intelligence, pp.41–53. [60] N. Carlini and D. Wagner. Magnet and efﬁcient defenses against adversarial attacks are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017. [61] A. Abdolmaleki, B. Price, N. Lau, L.P. Reis and G. Neumann. Deriving and improving CMA-ES with information geometric trust regions. In Proceedings of the Genetic and Evolutionary Computation Conference, pp.657–664. [62] K. Nishida and Y. Akimoto. PSA-CMA-ES: CMA-ES with population size adaptation. In Proceedings of the Genetic and Evolutionary Computation Conference, pp.865–872. [63] M. Groves and J. Branke. Sequential sampling for noisy optimisation with CMA-ES. In Proceedings of the Genetic and Evolutionary Computation Conference, pp.1023–1030. [64] CIFAR-10 - Object Recognition in Images@Kaggle. At https://www.kaggle.com/c/cifar-10/data. Accessed date: 1 Feb, 2018. [65] Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai One Pixel Attack for Fooling Deep Neural Networks. In IEEE Transactions on Evolutionary Computation, Vol.23 , Issue.5 , pp. 828–841. Publisher: IEEE. DOI: 10.1109/TEVC.2019.2890858.

